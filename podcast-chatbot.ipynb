{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\users\\armin\\anaconda3\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\armin\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\armin\\anaconda3\\lib\\site-packages (0.2.82)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\armin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\armin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\armin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\armin\\anaconda3\\lib\\site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\armin\\anaconda3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install feedparser\n",
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.legacy import Document\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "podcast_atom_link = \"https://api.substack.com/feed/podcast/1084089.rss\" # latent space podcast\n",
    "parsed = feedparser.parse(podcast_atom_link)\n",
    "episode = [ep for ep in parsed.entries if ep['title'] == \"RAG Is A Hack - with Jerry Liu from LlamaIndex\"][0]\n",
    "episode_summary = episode['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><em>Want to help define </em><em>the AI Engineer stack</em><em>? >800 folks have weighed in on the top tools, communities and builders for the first </em><em>State of AI Engineering</em><em> survey, which we will present for the first time at next week‚Äôs </em><em>AI Engineer Summit</em><em>. Join us </em><em>online</em><em>!</em></p><p><em>This post had robust discussion on </em><em>HN</em><em> and </em><em>Twitter</em><em>.</em></p><p>In October 2022, Robust Intelligence hosted an internal hackathon to play around with LLMs which led to the creation of two of the most important AI Engineering tools: LangChain ü¶ú‚õìÔ∏è (<a href=\"https://www.latent.space/p/langchain#details\" target=\"_blank\">our interview with Harrison here</a>) and LlamaIndex ü¶ô by Jerry Liu, which we‚Äôll cover today. In less than a year, LlamaIndex has crossed 600,000 monthly downloads, raised $8.5M from Greylock, has a fast growing open source community that contributes to LlamaHub, and it doesn‚Äôt seem to be slowing down.</p><p><strong>LlamaIndex‚Äôs Origin (aka GPT Tree Index)</strong></p><p>Jerry struggled to make large amounts of data work with GPT-3 (which had a 4,096 tokens context window). Today LlamaIndex is at the forefront of the RAG wave (Retrieval Augmented Generation), but in the beginning Jerry wasn‚Äôt focused on embeddings and search, but rather on understanding how models could summarize, link, and reason about data. </p><p>On November 5th, Jerry pushed the <a href=\"https://github.com/run-llama/llama_index/commit/2e62c6987808797611e9bb7c1ae8c86e72a88727\" target=\"_blank\">first version</a> to Github under the name ‚ÄúGPT Tree Index‚Äù: </p><p><em>The GPT Tree Index first takes in a large dataset of unprocessed text data as input. It then builds up a tree-index in a bottom-up fashion; each parent node is able to summarize the children nodes using a general </em><em>summarization prompt</em><em>; each intermediate node containing summary text summarizing the components below. Once the index is built, it can be saved to disk and loaded for future use.</em></p><p><em>Then, say the user wants to use GPT-3 to answer a question. Using a </em><em>query prompt template</em><em>, GPT-3 will be able to recursively perform tree traversal in a top-down fashion in order to answer a question. For example, in the very beginning GPT-3 is tasked with selecting between *n* top-level nodes which best answers a provided query, by outputting a number as a multiple-choice problem. The GPT Tree Index then uses the number to select the corresponding node, and the process repeats recursively among the children nodes until a leaf node is reached.</em></p><p>[‚Ä¶]</p><p><em>How is this better than an embeddings-based approach / other state-of-the-art QA and retrieval methods?</em></p><p><em>The intent is not to compete against existing methods. A simpler embedding-based technique could be to just encode each chunk as an embedding and do a simple question-document embedding look-up to retrieve the result. This project is a simple exercise to test how GPT can organize and lookup information.</em></p><p>The project attracted a lot of attention early on (the announcement tweet has ~330 likes), but it wasn‚Äôt until ~February 2023 that the open source community really started to explode, which was around the same time that LlamaHub was released. LlamaHub made it easy for developers to import data from Google Drive, Discord, Slack, databases, and more into their LlamaIndex projects.  </p><p>What is LlamaIndex? </p><p>As we mentioned, LlamaIndex is leading the charge in the development of the RAG stack. RAG boils down to two parts:</p><p>* <strong>Indexing</strong> (i.e. how do you load and index the data in your knowledge base)</p><p>* <strong>Querying</strong> (i.e. how do you surface the data and fit it in the model context) </p><p>Indexing</p><p>To get your data from all your sources to your RAG knowledge base, you can leverage a few tools: </p><p>* <strong>Documents / Nodes</strong>: A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a ‚Äúchunk‚Äù of a source Document (i.e. one Document has many Node) as well as its relationship to other Node objects.</p><p>* <strong>Data Connectors</strong>: A data connector ingest data from different sources and turn them into Document representations (text and simple metadata). These connectors are offered through <a href=\"https://llamahub.ai/\" target=\"_blank\">LlamaHub</a>, and there are over 200 of them today.</p><p>* <strong>Data Indexes</strong>: Once you‚Äôve ingested your data, LlamaIndex will help you index the data into a format that‚Äôs easy to retrieve. There are many <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html\" target=\"_blank\">types of indexes</a> (Summary, Tree, Vector, etc). Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/vector_store_guide.html\" target=\"_blank\">VectorStoreIndex</a>, which can then be paired with any of the vector stores out there (<a href=\"https://gpt-index.readthedocs.io/en/v0.6.21.post1/examples/vector_stores/ChromaIndexDemo.html\" target=\"_blank\">an example with Chroma</a>).</p><p>Querying</p><p>The RAG pipeline, during the querying phase, sources the most pertinent context from a user's prompt, forwarding it along to the LLM. This equips the LLM with current / private knowledge beyond its foundational training data. LlamaIndex offers adaptable modules tailored for building RAG pathways for Q&amp;A, chatbots, or agent use, since each of them has different requirements. For example, a chatbot should expect the user to interject with follow up questions, while an agent will try to carry out a whole task on its own without user intervention.  </p><p><strong>Building Blocks</strong></p><p>* <strong>Retrievers</strong>: A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. Vector index is the most popular mode, but there are <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/retriever/retriever_modes.html\" target=\"_blank\">other options</a> like Summary, Tree, Keyword Table, Knowledge Graph, and Document Summary.  </p><p>* <strong>Node Postprocessors</strong>: Once the retriever gets you Node objects back, you will need to do additional work like discarding low similarity ones. There are <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html\" target=\"_blank\">many options</a> here as well, such as `SimilarityPostprocessor` (i.e. drop nodes below a certain similarity score) or `LongContextReorder` which helps avoid the issues raised in the ‚Äú<a href=\"https://arxiv.org/abs/2307.03172\" target=\"_blank\">Lost in the Middle, U-shaped recollection curve</a>‚Äù paper. </p><p>* <strong>Response Synthesizers</strong>: Takes a user query and your retrieved chunks, and prompts and LLM with them. There are a few <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/response_synthesizers/usage_pattern.html#configuring-the-response-mode\" target=\"_blank\">response modes</a> here that balance thoroughness and compactness.</p><p><strong>Pipelines</strong></p><p>* <strong>Query Engines</strong>: A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM. This makes it possible to do things like <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pandas_query_engine.html\" target=\"_blank\">‚ÄúAsk panda questions‚Äù</a> by leveraging Panda dataframes as a data source. </p><p>* <strong>Chat Engines</strong>: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer). This supports traditional OpenAI-style chat interfaces, as well as more advanced ones like <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/chat_engine/chat_engine_react.html\" target=\"_blank\">ReAct</a>.</p><p>* <strong>Agents</strong>: An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines, but they have the power to both read and write data. For reasoning, you can use either OpenAI Functions or ReAct. Both can leverage the tools offered through LlamaHub for further analysis.</p><p>RAG vs Finetuning</p><p>Now that you have a full overview of what LlamaIndex does, the next question is ‚ÄúWhen should I use this and when should I fine tune?‚Äù. Jerry‚Äôs TLDR is that ‚ÄúRAG is just a hack‚Äù, but a powerful one. Each option has pros and cons:</p><p>* <strong>Lower investment: </strong>RAG requires almost 0 upfront investment, unlike finetuning which requires data cleaning, model training, increased costs for finetuned inference, etc.</p><p>* <strong>Stricter access control and higher visibility:</strong> when finetuning, the model learns everything. With RAG, you can decide what documents the index should have access to, making it more secure by default. You are also able to see everything that was passed into the context if a response doesn‚Äôt look right.</p><p>* <strong>Context window limitation: </strong>you can only fit so many tokens into the prompt due to the way models work. Finetuning helps you circumvent that by compressing the knowledge into the model weights rather than putting it in the prompt.   </p><p>As Jerry says, the best way to know this inside out is <a href=\"https://twitter.com/jerryjliu0/status/1702345670563332340\" target=\"_blank\">to learn to build RAG from scratch</a> (without LlamaIndex) - and they have plenty of tutorials on his Twitter and blog to learn this.</p><p>The other issue is that the math for finetuning isn‚Äôt well known yet as we <a href=\"https://www.latent.space/p/transformers-math#details\" target=\"_blank\">discussed with Quentin Anthony from Eleuther</a>, so unless you have money and time to invest into exploring fine tuning, you‚Äôre better off starting with RAG.  </p><p>Full YouTube Discussion!</p><p>Show Notes</p><p>* <strong>LlamaIndex</strong></p><p>* <a href=\"https://llamahub.ai/\" target=\"_blank\">LlamaHub</a></p><p>* <a href=\"https://www.secinsights.ai/\" target=\"_blank\">SEC Insights</a></p><p>* <a href=\"https://www.robustintelligence.com/\" target=\"_blank\">Robust Intelligence</a></p><p>* <a href=\"https://quorablog.quora.com/Poe-1\" target=\"_blank\">Quora‚Äôs Poe</a></p><p>* <a href=\"https://trychroma.com/\" target=\"_blank\">Chroma</a></p><p>* <a href=\"https://vespa.ai/\" target=\"_blank\">Vespa</a></p><p>* <a href=\"https://twitter.com/jerryjliu0/status/1702345670563332340\" target=\"_blank\">Why should every AI engineer learn to build RAG from scratch</a>?</p><p>* <a href=\"https://www.langchain.com/\" target=\"_blank\">LangChain</a></p><p>* <a href=\"https://gorilla.cs.berkeley.edu/\" target=\"_blank\">Gorilla</a></p><p>* <a href=\"https://arxiv.org/abs/2307.03172\" target=\"_blank\">Lost in the Middle: How Language Models Use Long Contexts</a></p><p>Timestamps</p><p>* [00:00:00] Introductions and Jerry‚Äôs background</p><p>* [00:04:30] Starting LlamaIndex as a side project</p><p>* [00:05:11] Evolution from tree-index to current LlamaIndex and LlamaHub architecture</p><p>* [00:11:39] Deciding to leave Robust to start the LlamaIndex company and raising funding</p><p>* [00:20:06] Context window size and information capacity for LLMs</p><p>* [00:21:34] Minimum viable context and maximum context for RAG</p><p>* [00:22:52] Fine-tuning vs RAG - current limitations and future potential</p><p>* [00:24:02] RAG as a hack but good hack for now</p><p>* [00:26:19] RAG benefits - transparency and access control</p><p>* [00:27:46] Potential for fine-tuning to take over some RAG capabilities</p><p>* [00:30:04] Baking everything into an end-to-end trained LLM</p><p>* [00:33:24] Similarities between iterating on ML models and LLM apps</p><p>* [00:34:47] Modularity and customization options in LlamaIndex: data loading, retrieval, synthesis, reasoning</p><p>* [00:40:16] Evaluating and optimizing each component of Lama Index system</p><p>* [00:46:02] Building retrieval benchmarks to evaluate RAG</p><p>* [00:47:24] SEC Insights - open source full stack LLM app using LlamaIndex</p><p>* [00:49:48] Enterprise platform to complement LlamaIndex open source</p><p>* [00:51:00] Community contributions for LlamaHub data loaders</p><p>* [00:53:21] LLM engine usage - majority OpenAI but options expanding</p><p>* [00:56:25] Vector store landscape</p><p>* [00:59:46] Exploring relationships and graphs within data</p><p>* [01:03:24] Additional complexity of evaluating agent loops</p><p>* [01:04:01] Lightning Round</p><p>Transcript</p><p><strong>Alessio</strong>: Hey everyone, welcome to the Latent Space Podcast. This is Alessio, partner and CTO of Residence and <a href=\"https://decibel.vc/\" target=\"_blank\">Decibel Partners</a>, and I'm joined by my co-host Swyx, founder of <a href=\"https://github.com/smol-ai\" target=\"_blank\">Smol AI</a>. [00:00:20]</p><p><strong>Swyx</strong>: And today we finally have Jerry Liu on the podcast. Hey Jerry. [00:00:24]</p><p><strong>Jerry</strong>: Hey guys. Hey Swyx and Alessio. Thanks for having me. [00:00:27]</p><p><strong>Swyx</strong>: It's kind of weird because we keep running into each other in San Francisco AI events, so it's kind of weird to finally just have a conversation recorded for everybody else. [00:00:34]</p><p><strong>Jerry</strong>: Yeah, I know. I'm really looking forward to this, aside from the questions. [00:00:38]</p><p><strong>Swyx</strong>: So I tend to introduce people on their formal background and then ask something on the more personal side. So you are part of the Princeton gang. [00:00:46]</p><p><strong>Jerry</strong>: I don't know if there is like official Princeton gang. [00:00:48]</p><p><strong>Swyx</strong>: No, small Princeton gang. Okay. I attended your meeting. There was like four of you with Prem and the others. And then you have a bachelor's in CS and a certificate in finance. That's also fun. I also did finance and I think I saw that you also interned at Two Sigma where I worked in New York. You were a machine learning engineer. [00:01:06]</p><p><strong>Jerry</strong>: You were at Two Sigma?</p><p><strong>Swyx</strong>: Yeah, very briefly.</p><p><strong>Jerry</strong>: Oh, cool. I didn't know that. [00:01:09]</p><p><strong>Swyx</strong>: That was my first like proper engineering job before I went into DevRel. [00:01:12]</p><p><strong>Jerry</strong>: Oh, okay. Nice. [00:01:14]</p><p><strong>Swyx</strong>: And then you were a machine learning engineer at Quora, AI research scientist at Uber for three years, and then two years machine learning engineer at Robust Intelligence before starting LlamaIndex. So that's your LinkedIn. It's not only LinkedIn that people should know about you. [00:01:27]</p><p><strong>Jerry</strong>: I think back during my Quora days, I had this like three-month phase where I just wrote like a ton of Quora answers. And so I think if you look at my tweets nowadays, you can basically see that as like the V2 of my three-month like Forrestant where I just like went ham on Quora for a bit. I actually, I think I was back then actually when I was working on Quora, I think the thing that everybody was fascinated in was just like general like deep learning advancements and stuff like GANs and generative like images and just like new architectures that were evolving. And it was a pretty exciting time to be a researcher actually, because you were going in like really understanding some of the new techniques. So I kind of use that as like a learning opportunity, basically just like read a bunch of papers and then answer questions on Quora. And so you can kind of see traces of that basically in my current Twitter where it's just like really about kind of like framing concepts and trying to make it understandable and educate other users on it. Yeah. [00:02:17]</p><p><strong>Swyx</strong>: I've said, so a lot of people come to me for my Twitter advice, but like, I think you are doing one of the best jobs in AI Twitter, which is explaining concepts and just consistently getting hits out. Thank you. I didn't know it was due to the Quora training. Let's just sign on on Quora. A lot of people, including myself, like kind of wrote off Quora as like one of the web 1.0 like sort of question answer forums. But now I think it's becoming, seeing a resurgence obviously due to Poe and obviously Adam and D'Angelo has always been a leading tech figure, but what do you think is kind of underrated about Quora? [00:02:46]</p><p><strong>Jerry</strong>: Well, I mean, I like the, I really liked the mission of Quora when I, when I joined. In fact, I interned there like in 2015 and I joined full time in 2017. One is like they had, and they have like a very talented engineering team and just like really, really smart people. And the other part is the whole mission of the company is to just like spread knowledge and to educate people. And to me that really resonated. I really liked the idea of just like education and democratizing the flow of information. If you imagine like kind of back then it was like, okay, you have Google, which is like for search, but then you have Quora, which is just like user generated, like grassroots type content. And I really liked that concept because it's just like, okay, there's certain types of information that aren't accessible to people, but you can make accessible by just like surfacing it. And so actually, I don't know if like most people know that about like Quora and if they've used the product, whether through like SEO, right, or kind of like actively, but that really was what drew me to it. [00:03:39]</p><p><strong>Swyx</strong>: Yeah. I think most people challenges with it is that sometimes you don't know if it's like a veiled product pitch, right? [00:03:44]</p><p><strong>Jerry</strong>: Yeah. Of course, like quality of the answer matters quite a bit. And then you start running into these like- [00:03:47]</p><p><strong>Swyx</strong>: It's like five alternatives and then here's the one I work on. Yeah. [00:03:50]</p><p><strong>Jerry</strong>: Like recommendation issues and all that stuff. I used, I worked on recsys at Quora actually, so I got a taste of some of that stuff. Well, I mean, I kind of more approached it from machine learning techniques, which might be a nice segue into RAG actually. A lot of it was just information retrieval. We weren't like solving anything that was like super different than what was standard in the industry at the time, but just like ranking based on user preferences. I think a lot of Quora was very metrics driven. So just like trying to maximize like daily active hours, like time spent on site, those types of things. And all the machine learning algorithms were really just based on embeddings. You have a user embedding and you have like item embeddings and you try to train the models to try to maximize the similarity of these. And it's basically a retrieval problem. [00:04:30]</p><p><strong>Swyx</strong>: Okay. So you've been working on RAG for longer than most people think? [00:04:33]</p><p><strong>Jerry</strong>: Well, kind of. So I worked there for like a year, right, just transparently. And then I worked at Uber where I was not working on ranking. It was more like kind of deep learning training for self-driving and computer vision and that type of stuff. But I think in the LLM world, it's kind of just like a combination of like everything these days. I mean, retrieval is not really LLMs, but like it fits within the space of like LLM apps. And then obviously like having knowledge of the underlying deep learning architectures helps. Having knowledge of basic software engineering principles helps too. And so I think it's kind of nice that like this whole LLM space is basically just a combination of just like a bunch of stuff that you probably like people have done in the past. [00:05:11]</p><p><strong>Swyx</strong>: It's good. It's like a summary capstone project. Yeah, exactly. [00:05:14]</p><p><strong>Jerry</strong>: Yeah. [00:05:15]</p><p><strong>Alessio</strong>: And before we dive into LlamaIndex, what do they feed you a robust intelligence that both you and Harrison from LangChain came out of it at the same time? Was there like, yeah. Is there any fun story of like how both of you kind of came up with kind of like core infrastructure to LLM workflows today? Or how close were you at robust? Like any fun behind the scenes? [00:05:37]</p><p><strong>Jerry</strong>: Yeah. Yeah. We, um, we work pretty closely. I mean, we were on the same team for like two years. I got to know Harrison and the rest of the team pretty well. I mean, I have a respect that people there, the people that were very driven, very passionate. And it definitely pushed me to be, you know, a better engineer and leader and those types of things. Yeah. I don't really have a concrete explanation for this. I think it's more just, we have like an LLM hackathon around like September. This was just like exploring GPT-3 or it was October actually. And then the day after I went on vacation for a week and a half, and so I just didn't track Slack or anything. And then when I came back, saw that Harrison started LangChain [00:06:09]</p><p><strong>Swyx</strong>: Oh that's cool. [00:06:10]</p><p><strong>Jerry</strong>: I was like, oh, I'll play around with LLMs a bit and then hacked around on stuff. And I think I've told the story a few times, but you know, I was like trying to feed in information into GPT-3. And then, then you deal with like context window limitations and there was no tooling or really practices to try to understand how do you, you know, get GPT-3 to navigate large amounts of data. And that's kind of how the project started. Really was just one of those things where early days, like we were just trying to build something that was interesting. Like I wanted to start a company. I had other ideas actually of what I wanted to start. And I was very interested in, for instance, like multimodal data, like video data and that type of stuff. And then this just kind of grew and eventually took over the other idea. [00:06:48]</p><p><strong>Swyx</strong>: Text is the universal interface. [00:06:50]</p><p><strong>Jerry</strong>: I think so. I think so. I actually think once the multimodal models come out, I think there's just like mathematically nicer properties of you can just get like join multiple embeddings, like clip style. But text is really nice because from a software engineering principle, it just makes things way more modular. You can just convert everything into text and then you just represent everything as text. [00:07:08]</p><p><strong>Swyx</strong>: Yeah. I'm just explaining retroactively why working on LlamaIndex took off versus if you had chose to spend your time on multimodal, we probably wouldn't be talking about whatever you ended up working on. [00:07:18]</p><p><strong>Jerry</strong>: Yeah. [00:07:19]</p><p><strong>Swyx</strong>: That's true. It's troubled. Interesting. So November 9th, that was a very productive month. I guess October, November, November 9th, you announced GPT-3 Index and you picked a tree logo. Very cool. Every project must have an emoji. [00:07:32]</p><p><strong>Jerry</strong>: Yeah. Yeah. I probably was somewhat inspired by a light train, but I will admit, yeah. [00:07:37]</p><p><strong>Swyx</strong>: It uses GPT to build a knowledge tree in a bottoms-up fashion by applying a summarization prompt for each node. Yep. Which I like that original vision. Your messaging roundabout then was also that you're creating optimized data structures. What's the sort of journey to that and how does that contrast with LlamaIndex today? Okay. [00:07:56]</p><p><strong>Jerry</strong>: Maybe I can tell a little bit about the beginning intuitions. I think when I first started, this really wasn't supposed to be something that was like a toolkit that people use. It was more just like a system. And the way I wanted to think about the system was more a thought exercise of how language models with their reasoning capabilities, if you just treat them as like brains, can organize information and then traverse it. So I didn't want to think about embeddings, right? To me, embeddings just felt like it was just an external thing that was like, well, it was just external to trying to actually tap into the capabilities of language models themselves, right? I really wanted to see, you know, just as like a human brain could like synthesize stuff, could we create some sort of like structure where this neural CPU, if you will, can like organize a bunch of information, you know, auto-summarize a bunch of stuff and then also traverse the structure that I created. That was the inspiration for this initial tree index, to be honest. And I think I said this in the first tweet, it actually works super well, right? Like GPT-4 obviously is much better at reasoning. I'm one of the first to say, you know, you shouldn't use anything pre-GPT-4 for anything that requires complex reasoning because it's just going to be unreliable, okay, disregarding stuff like fine tuning. But it worked okay. But I think it definitely struck a chord with kind of like the Twitter crowd, which is just like new ideas at the time, I guess, just like thinking about how you can actually bake this into some sort of application. Because I think what I also ended up discovering was the fact that there was starting to become a wave of developers building on top of GPT-3 and people were starting to realize that what makes them really useful is to apply them on top of your personal data. And so even if the solution itself was kind of like primitive at the time, like the problem statement itself was very powerful. And so I think being motivated by the problem statement, right, like this broad mission of how do I unlock elements on top of the data also contributed to the development of LOM index to the state it is today. And so I think part of the reason, you know, our toolkit has evolved beyond the just existing set of like data structures is we really tried to take a step back and think, okay, what exactly are the tools that would actually make this useful for a developer? And then, you know, somewhere around December, we made an active effort to basically like push towards that direction, make the code base more modular, right, more friendly as an open source library. And then also start adding in like embeddings, start thinking into practical considerations like latency, cost, performance, those types of things. And then really motivated by that mission, like start expanding the scope of the toolkit towards like covering the life cycle of like data ingestion and querying. Where you also added Llamahub and yeah, so I think that was in like January on the data loading side. And so we started adding like some data loaders, saw an opportunity there, started adding more stuff on the retrieval querying side, right? We still have like the core data structures, but how do you actually make them more modular and kind of like decouple storing state from the types of like queries that you could run on top of this a little bit. And then starting to get into more complex interactions, like chain of thought reasoning, routing and, you know, like agent loops. [00:10:44]</p><p><strong>Alessio</strong>: You and I spent a bunch of time earlier this year talking about Llamahub, what that might become. You were still at Robust. When did you decide it was time to start the company and then start to think about what LlamaIndex is today? [00:10:58]</p><p><strong>Jerry</strong>: Yeah, I mean, probably December. It was kind of interesting. I was getting some inbound from initial VCs, I was talking about this project. And then in the beginning, I was like, oh, yeah, you know, this is just like a design project. But you know, what about my other idea on like video data, right? And then I was trying to like get their thoughts on that. And then everybody was just like, oh, yeah, whatever, like that part's like a crowded market. And then it became clear that, you know, this was actually a pretty big opportunity. And like, coincidentally, right, like this actually did relate to like, my interests have always been at the intersection of AI data and kind of like building practical applications. And it was clear that this was evolving into a much bigger opportunity than the previous idea was. So around December, and then I think I gave a pretty long notice, but I left officially like early March. [00:11:39]</p><p><strong>Alessio</strong>: What were your thinkings in terms of like moats and, you know, founders kind of like overthink it sometimes. So you obviously had like a lot of open source love and like a lot of community. And you're like, were you ever thinking, okay, I don't know, this is maybe not enough to start a company or did you always have conviction about it? [00:11:59]</p><p><strong>Jerry</strong>: Oh, no, I mean, 100%. I felt like I did this exercise, like, honestly, probably more late December and then early January, because I was just existentially worried about whether or not this would actually be a company at all. And okay, what were the key questions I was thinking about? And these were the same things that like other founders, investors, and also like friends would ask me is just like, okay, what happens if context windows get much bigger? What's the point of actually structuring data right in the right way? Right? Why don't you just dump everything into the prompt, fine tuning, like, what if you just train the model over this data? And then, you know, what's the point of doing this stuff? And then some other ideas is what if like OpenAI actually just like takes this like builds upwards on top of the their existing like foundation models and starts building in some like built in orchestration capabilities around stuff like RAG and agents and those types of things. And so I basically ran through this mental exercise and, you know, I'm happy to talk a little bit more about those thoughts as well. But at a high level, well, context windows have gotten bigger, but there's obviously still a need for a rag. I think RAG is just like one of those things that like, in general, what people care about is, yes, they do care about performance, but they also care about stuff like latency and costs. And so my entire reasoning at the time was just like, okay, like, yes, maybe you will have like much bigger context windows, as we've seen with like 100k context windows. But for enterprises, like, you know, data, which is not in just like the scale of like a few documents, it's usually in like gigabytes, terabytes, petabytes. How do you actually just unlock language models over that data, right? And so it was clear there was just like, whether it's RAG or some other paradigm, no one really knew what that answer was. And so there was clearly like technical opportunity here. Like there was just stacks that needed to be invented to actually solve this type of problem, because language models themselves didn't have access to this data. The other piece here is just like, and so if like you just dumped all this data into, let's say a model had like hypothetically an infinite context window, right? And you just dump like 50 gigabytes of data into a context window. That just seemed very inefficient to me, because you have these network transfer costs of uploading 50 gigabytes of data to get back a single response. And so I kind of realized, you know, there's always going to be some curve, regardless of like the performance of the best performing models of like cost versus performance. What RAG does is it does provide extra data points along that access, because you kind of control the amount of context you actually wanted to retrieve. And of course, like RAG as a term was still evolving back then, but it was just this whole idea of like, how do you just fetch a bunch of information to actually, you know, like stuff into the prompt. And so people even back then were kind of thinking about some of those considerations. [00:14:29]</p><p><strong>Swyx</strong>: And then you fundraised in June, or you announced your fundraiser in June. Yeah. Take us through that process of thinking about the fundraise and your plans for the company, you know, at the time. Yeah, definitely. [00:14:41]</p><p><strong>Jerry</strong>: I mean, I think we knew we wanted to, I mean, obviously we knew we wanted to fundraise. There was also a bunch of like investor interest, and it was probably pretty unusual given the, you know, like hype wave of generative AI. So like a lot of investors were kind of reaching out around like December, January, February. In the end, we went with Greylock. Greylock's great. You know, they've been great partners so far. And to be honest, like there's a lot of like great VCs out there. And a lot of them who are specialized on like open source, data, infra, and that type of stuff. What we really wanted to do was, because for us, like time was of the essence, like we wanted to ship very quickly and still kind of build Mindshare in this space. We just kept the fundraising process very efficient. I think we basically did it in like a week or like three days. And so, yeah, just like front loaded it and then just like pick the one named Jerry. Yeah, exactly. Yeah. [00:15:27]</p><p><strong>Swyx</strong>: I'm kidding. I mean, he's obviously great and Greylock's a fantastic firm. [00:15:32]</p><p><strong>Jerry</strong>: Embedding some of my research. So, yeah, just we've had Greylock. They've been great partners. I think in general, when I talk to founders about like the fundraise process, it's never like the most fun period, I think, because it's always just like, you know, there's a lot of logistics, there's lawyers you have to, you know, get in the loop. And like a lot of founders just want to go back to building. I think in the end, we're happy that we kept it to a pretty efficient process. [00:15:54]</p><p><strong>Swyx</strong>: And so you fundraise with Simon. How do you split things with him? How big is your team now? [00:15:57]</p><p><strong>Jerry</strong>: The team is growing. By the time this podcast is released, we'll probably have had one more person join the team. So basically, it's between, we're rapidly getting to like eight or nine people. At the current moment, we're around like six. And so just like there'll be some exciting developments in the next few weeks. I'm excited to announce that. So the team is, has kind of like, we've been pretty selective in terms of like how we like grow the team. Obviously, like we look for people that are really active in terms of contributions to Lum Index, people that have like very strong engineering backgrounds. And primarily, we've been kind of just looking for builders, people that kind of like grow the open source and also eventually this like managed like enterprise platform as well with us. In terms of like Simon, yeah, I've known Simon for a few years now. I knew him back at Uber ATG in Toronto. He's one of the smartest people I knew, has a sense of both like a deep understanding of ML, but also just like first principles thinking about like engineering and technical concepts in general. And I think one of my criteria, criteria is when I was like looking for a co-founder for this project with someone that was like technically better than me, because I knew I wanted like a CTO. And so honestly, like there weren't a lot of people that, I mean, there's, I know a lot of people that are smarter than me, but like that fit that bill. We're willing to do a startup and also just have the same like values that I shared. Right. And just, I think doing a startup is very hard work, right? It's not like, I'm sure like you guys all know this, it's, it's a lot of hours, a lot of late nights and you want to be like in the same place together and just like being willing to hash out stuff and have that grit basically. And I really looked for that. And so Simon really fit that bill and I think I convinced him to bring Trump on board. [00:17:24]</p><p><strong>Swyx</strong>: Yeah. And obviously I've had the pleasure of chatting and working with a little bit with both of you. What would you say those, those like your top one or two values are when, when thinking about that or the culture of the company and that kind of stuff? [00:17:36]</p><p><strong>Jerry</strong>: I think in terms of the culture of the company, it's really like, I mean, there's a few things I can name off the top of my head. One is just like passion, integrity. I think that's very important for us. We want to be honest. We don't want to like, obviously like copy code or, or kind of like, you know, just like, you know, not give attribution, those types of things and, and just like be true to ourselves. I think we're all very like down to earth, like humble people, but obviously I think just willingness to just like own stuff and dive right in. And I think grit comes with it. I think in the end, like this is a very fast moving space and we want to just like be one of the, you know, like dominant forces and helping to provide like production quality outline applications. Yeah. [00:18:11]</p><p><strong>Swyx</strong>: I promise we'll get to more technical questions, but I also want to impress on the audience that this is a very conscious and intentional company building. And since your fundraising post, which was in June, and now it's September, so it's been about three months, you've actually gained 50% in terms of stars and followers. You've 3x'd your download count to 600,000 a month and your discord membership has reached 10,000. So like a lot of ongoing growth. [00:18:37]</p><p><strong>Jerry</strong>: Yeah, definitely. And obviously there's a lot of room to expand there too. And so open source growth is going to continue to be one of our core goals because in the end it's just like, we want this thing to be, well, one big, right? We all have like big ambitions, but to just like really provide value to developers and helping them in prototyping and also productionization of their apps. And I think it turns out we're in the fortunate circumstance where a lot of different companies and individuals, right, are in that phase of like, you know, maybe they've hacked around on some initial LLM applications, but they're also looking to, you know, start to think about what are the production grade challenges necessary to actually, that to solve, to actually make this thing robust and reliable in the real world. And so we want to basically provide the tooling to do that. And to do that, we need to both spread awareness and education of a lot of the key practices of what's going on. And so a lot of this is going to be continued growth, expansion, education, and we do prioritize that very heavily. [00:19:30]</p><p><strong>Alessio</strong>: Let's dive into some of the questions you were asking yourself initially around fine tuning and RAG , how these things play together. You mentioned context. What is the minimum viable context for RAG ? So what's like a context window too small? And at the same time, maybe what's like a maximum context window? We talked before about the LLMs are U-shaped reasoners. So as the context got larger, like it really only focuses on the end and the start of the prompt and then it kind of peters down. Any learnings, any kind of like tips you want to give people as they think about it? [00:20:06]</p><p><strong>Jerry</strong>: So this is a great question. And part of what I wanted to talk about a conceptual level, especially with the idea of like thinking about what is the minimum context? Like, okay, what if the minimum context was like 10 tokens versus like, you know, 2k tokens versus like a million tokens. Right. Like, and what does that really give you? And what are the limitations if it's like 10 tokens? It's kind of like, um, like eight bit, 16 bit games, right? Like back in the day, like if you play Mario and you have like the initial Mario where the graphics were very blocky and now obviously it's like full HD, 3d, just the resolution of the context and the output will change depending on how much context you can actually fit in. So the way I kind of think about this from a more principled manner is like you have like, there's this concept of like information capacity, just this idea of like entropy, like given any fixed amount of like storage space, like how much information can you actually compact in there? And so basically a context window length is just like some fixed amount of storage space, right? And so there's some theoretical limit to the maximum amount of information you can compact until like a 4,000 token storage space. And what does that storage space use for these days with LLMs? For inputs and also outputs. And so this really controls the maximum amount of information you can feed in terms of the prompt plus the granularity of the output. If you had an infinite context window, you're going to have an infinitely detailed response and also infinitely detailed memory. But if you don't, you can only kind of represent stuff in more quantized bits, right? And so the smaller the context window, just generally speaking, the less details and maybe the less, um, and for like specific, precise information, you're going to be able to surface any given point in time. [00:21:34]</p><p><strong>Alessio</strong>: So when you have short context, is the answer just like get a better model or is the answer maybe, Hey, there needs to be a balance between fine tuning and RAG to make sure you're going to like leverage the context, but at the same time, don't keep it too low resolution? [00:21:48]</p><p><strong>Jerry</strong>: Yeah, yeah. Well, there's probably some minimum threat, like I don't think anyone wants to work with like a 10. I mean, that's just a thought exercise anyways, a 10 token context window. I think nowadays the modern context window is like 2k, 4k is enough for just like doing some sort of retrieval on granular context and be able to synthesize information. I think for most intents and purposes, that level of resolution is probably fine for most people for most use cases. I think the question there is just like, um, the limitations actually more on, okay, if you're going to actually combine this thing with some sort of retrieval data structure mechanism, there's just limitations on the retrieval side because maybe you're not actually fetching the most relevant context to actually answer this question, right? Like, yes, like given the right context, 4,000 tokens is enough. But if you're just doing like top-k similarity, like you might not be able to be fetching the right information from the documents. [00:22:34]</p><p><strong>Alessio</strong>: So how should people think about when to stick with RAG versus when to even entertain and also in terms of what's like the threshold of data that you need to actually worry about fine tuning versus like just stick with rag? Obviously you're biased because you're building a RAG company, but no, no, actually, um, I [00:22:52]</p><p><strong>Jerry</strong>: think I have like a few hot takes in here, some of which sound like a little bit contradictory or what we're actually building. And I think to be honest, I don't think anyone knows the right answer. I think this is the truth. [00:23:01]</p><p><strong>Alessio</strong>: Yeah, exactly. [00:23:01]</p><p><strong>Jerry</strong>: This is just like thought exercise towards like understanding the truth. [00:23:04]</p><p><strong>Alessio</strong>: Right. [00:23:04]</p><p><strong>Jerry</strong>: So, okay. [00:23:05]</p><p><strong>Alessio</strong>: I have a few hot takes. [00:23:05]</p><p><strong>Jerry</strong>: One is like RAG is basically just, just a hack, but it turns out it's a very good hack because what is RAG rag is you keep the model fixed and you just figure out a good way to like stuff stuff into the prompt of the language model and everything that we're doing nowadays in terms of like stuffing stuff into the prompt is just algorithmic. We're just figuring out nice algorithms to, to like retrieve right information with top case similarity, do some sort of like, uh, you know, hybrid search, some sort of like a chain of thought decomp and then just like stuff stuff into a prompt. So it's all like algorithmic and it's more like just software engineering to try to make the most out of these like existing APIs. The reason I say it's a hack is just like from a pure like optimization standpoint. If you think about this from like the machine learning lens, unless the software engineering lens, there's pieces in here that are going to be like suboptimal, right? Like, like the thing about machine learning is when you optimize like some system that can be optimized within machine learning, like the set of parameters, you're really like changing like the entire system's weights to try to optimize the subjective function. [00:24:02]</p><p><strong>Jerry</strong>: And if you just cobble a bunch of stuff together, you can't really optimize the pieces are inefficient, right? And so like a retrieval interface, like doing top cam batting lookup, that part is  inefficient. [00:24:13]</p><p><strong>Jerry</strong>: If you, for instance, because there might be potentially a better, more learned retrieval algorithm, that's better. If you know, you do stuff like some sort of, I know nowadays there's this concept of how do you do like short-term and long-term memory represent stuff in some sort of vector embedding, do trunk sizes, all that stuff. It's all just like decisions that you make that aren't really optimized and it's not really automatically learned. It's more just things that you set beforehand to actually feed into the system. So I do think like there is a lot of room to actually optimize the performance of an entire LLM system, potentially in a more like machine learning based way. Right. [00:24:48]</p><p><strong>Jerry</strong>: And I will leave room for that. And this is also why I think like in the long term, I do think fine tuning will probably have like greater importance. And just like there will probably be new architectures invented that where you can actually kind of like include a lot of this under the black box, as opposed to having like hobbling together a bunch of components outside the black box. That said, just very practically given the current state of things, like even if I said RAG is a hack, it's a very good hack and it's also very easy to use. Right. [00:25:16]</p><p><strong>Jerry</strong>: And so just like for kind of like the AI engineer persona, which to be fair is kind of one of the reasons generative AI has gotten so big is because it's way more accessible for everybody to get into, as opposed to just like traditional machine learning, it tends to be good enough. [00:25:30]</p><p><strong>Jerry</strong>: Right. And if we can basically provide these existing techniques to help people really optimize how to use existing systems without having to really deeply understand machine learning, I still think that's a huge value add. And so there's very much like a UX and ease of use problem here, which is just like RAG is way easier to onboard and use. And that's probably like the primary reason why everyone should do RAG instead of fine tuning to begin with. If you think about like the 80-20 rule, like RAG very much fits within that and fine tuning doesn't really right now. And then I'm just kind of like leaving room for the future that, you know, like in the end, fine tuning can probably take over some of the aspects of like what RAG does. [00:26:04]</p><p><strong>Swyx</strong>: I don't know if this is mentioned in your explainability also allows for sourcing. And at the end of the day, like to increase trust that we have to source documents. Yeah. [00:26:14]</p><p><strong>Jerry</strong>: So, so I think what RAG does is it increases like transparency, visibility into the actual documents, right. [00:26:19]</p><p><strong>Jerry</strong>: That are getting fed into their context. [00:26:21]</p><p><strong>Swyx</strong>: Here's where they got it from. [00:26:22]</p><p><strong>Alessio</strong>: Exactly. [00:26:22]</p><p><strong>Jerry</strong>: That's definitely an advantage. I think the other piece that I think is an advantage, and I think that's something that someone actually brought up is just you can do access control with, with RAG . If you have an external storage system, you can't really do that with, with large language models. [00:26:35]</p><p><strong>Jerry</strong>: It's just like gate information to the neural net weights, like depending on the type of user for the first point, you could technically, you could technically have the  language model. [00:26:45]</p><p><strong>Jerry</strong>: Like if it memorized enough information, just like a site sources, but there's a question of just trust whether or not you're actually, yeah, well, but like it makes it up right now because it's like not good enough, but imagine a world where it is good enough and it does give accurate citations. </p><p><strong>Swyx</strong>: No, I think to establish trust, you just need a direct connection.So it's, it's kind of weird.  It's, it's this melding of deep learning systems versus very traditional information retrieval. Yeah, exactly. [00:27:11]</p><p><strong>Jerry</strong>: Well, so, so I think, I mean, I kind of think about it as analogous to like humans, right? [00:27:15]</p><p><strong>Jerry</strong>: Like, uh, we as humans, obviously we use the internet, we use tools. Uh, these tools have API interfaces are well-defined. Um, and obviously we're not like the tools aren't part of us. And so we're not like back propping or optimizing over these tools. And so when you think about like RAG , it's basically, um, LLM is learning how to use like a vector database to look up information that it doesn't know. And so then there's just a question of like how much information is inherent within the network itself and how much does it need to do some sort of like tool used to look up  stuff that it doesn't know. [00:27:42]</p><p><strong>Jerry</strong>: And I do think there'll probably be more and more of that interplay as time goes on. [00:27:46]</p><p><strong>Swyx</strong>: Yeah. Some followups on discussions that we've had, you know, we discussed fine tuning a bit and what's your current take on whether you can, you can fine tune new knowledge into LLMs. [00:27:55]</p><p><strong>Jerry</strong>: That's one of those things where I think longterm you definitely can. I think some people say you can't, I disagree. I think you definitely can. Just right now I haven't gotten it to work yet. So, so I think like we've tried, yeah, well, um, not in a very principled way, right? Like this is something that requires like an actual research scientist and not someone that has like, you know, an hour or two per night to actually look at this. [00:28:12]</p><p><strong>Swyx</strong>: Like I, you were a research scientist at Uber. I mean, it's like full-time, full-time working. [00:28:16]</p><p><strong>Jerry</strong>: So, so I think, um, what I specifically concretely did was I took OpenAI's fine tuning endpoints and then tried to, you know, it's in like a chat message interface. And so there's like, um, input question, like a user assistant message format. And so what I did was I tried to take just some piece of text and have the LLM memorize it by just asking it a bunch of questions about the text. So given a bunch of context, I would generate some questions and then generate some response and just fine tune over the question responses. That hasn't really worked super well, but that's also because I'm, I'm just like trying to like use OpenAI's endpoints as is. If you just think about like traditional, like how you train a Transformers model, there's kind of like the, uh, instruction, like fine tuning aspect, right? You like ask it stuff when guided with correct responses, but then there's also just like, um, next token production. And that's something that you can't really do with the OpenAI API, but you can do with, if you just train it yourself and that's probably possible if you just like train it over some corpus of data. I think Shashira from Berkeley said like, you know, when they trained Gorilla, they were like, Oh, you know, this, a lot of these LLMs are actually pretty good at memorizing information. Um, just the way the API interface is exposed is just no one knows how to use them right [00:29:22]</p><p><strong>Alessio</strong>: now. Right. [00:29:22]</p><p><strong>Jerry</strong>: And so, so I think that's probably one of the issues. [00:29:24]</p><p><strong>Swyx</strong>: Just to clue people in who haven't read the paper, Gorilla is the one where they train to use specific APIs. [00:29:30]</p><p><strong>Jerry</strong>: Yeah, I think this was on the Gorilla paper. Like the, the model itself could, uh, try to learn some prior over the data to decide like what tool to pick. But there's also, it's also augmented with retrieval that helps supplement it in case like the, the, the, um, prior doesn't actually work. [00:29:45]</p><p><strong>Swyx</strong>: Is that something that you'd be interested in supporting? [00:29:48]</p><p><strong>Jerry</strong>: I mean, I think in the longterm, like if like, this is kind of how fine tuning, like RAG evolves. Like I do think there'll be some aspect where fine tuning will probably memorize some high level concepts of knowledge, but then like RAG will just be there to supplement like aspects of that, that aren't work that don't, that, that it doesn't know. </p><p><strong>Jerry</strong>: Um, the way I think about this is kind of like, obviously RAG is the default way, like to be clear, RAG right now is the default way to actually augment stuff with knowledge. I think it's just an open question of how much the LM can actually internalize both high level concepts, but also details as you can like train stuff over it. And coming from an ML background, there is a certain beauty and just baking everything into some training process of a language model. Like if you just take raw chat, GPT or chat, GPT code interpreter, right? Like GPT four, it's not like you do RAG with it. You just ask it questions about like, Hey, how do I like to find a pedantic model in Python? And I'm like, can you give me an example? Can you visualize a graph? It just does it right. Like, and we'll run it through code interpreters as a tool, but that's not like a source for knowledge. [00:30:46]</p><p><strong>Jerry</strong>: It's just an execution environment. And so there is some beauty in just like having the model itself, like just, you know, instead of you kind of defining the algorithm for what the data structure should look like the model just learns it under the hood. That said, I think the reason it's not a thing right now is just like, no one knows how to do it. [00:31:01]</p><p><strong>Jerry</strong>: It probably costs too much money. And then also like the API interfaces and just like the actual ability to kind of evaluate and improve on performance, like isn't known to most people. [00:31:12]</p><p><strong>Alessio</strong>: Yeah. [00:31:12]</p><p><strong>Swyx</strong>: It also would be better with browsing. [00:31:14]</p><p><strong>Alessio</strong>: Yeah. [00:31:16]</p><p><strong>Swyx</strong>: I wonder when they're going to put that back. [00:31:18]</p><p><strong>Alessio</strong>: Okay. Yeah. [00:31:19]</p><p><strong>Swyx</strong>: So, and then one more follow up before we go into RAG for AI engineers is on your brief mentioned about security or off. How many of your, the people that you talk to, you know, you talk to a lot of people putting LlamaIndex into production. How many people actually are there versus just like, let's just dump a whole company notion into this thing. [00:31:36]</p><p><strong>Jerry</strong>: Wait, are you talking about from like the security off standpoint? [00:31:39]</p><p><strong>Alessio</strong>: Yeah. [00:31:39]</p><p><strong>Swyx</strong>: How big a need is that? Because I, I talked to some people who are thinking about building tools in that domain, but I don't know if people want it. [00:31:47]</p><p><strong>Jerry</strong>: I mean, I think bigger companies, like just bigger companies, like banks, consulting firms, like they all want this requirement, right? The way they're using LlamaIndex is not with this, obviously. Cause I don't think we have support for like access control or author that have stuff like on a hood. [00:32:02]</p><p><strong>Jerry</strong>: Cause we're more just like an orchestration framework. And so the way they build these initial apps is more kind of like prototype. Like, let's kind of, yeah. Like, you know, use some publicly available data. That's not super sensitive. Let's like, you know, assume that every user is going to be able to have access to the same amount of knowledge, those types of things. I think users have asked for it, but I don't think that's like a P zero. Like I think the P zero is more on like, can we get this thing working before we expand this to like more users within the work? [00:32:25]</p><p><strong>Alessio</strong>: There's a bunch of pieces to rag. Obviously it's not a, just an acronym. And you two recently, you think every AI engineer should build the front scratch at least once. Why is that? I think so. [00:32:37]</p><p><strong>Jerry</strong>: I'm actually kind of curious to hear your thoughts about this. Um, but this kind of relates to the initial like AI engineering posts that you put out and then also just like the role of an AI engineer and the skills that they're going to have to learn to truly succeed because there's an entire On one end, you have people that don't really, uh, like understand the fundamentals and just want to use this to like cobble something together to build something. And I think there is a beauty in that for what it's worth. Like, it's just one of those things. And Gen AI has made it so that you can just use these models in inference only mode, call something together, use it, power your app experiences, but on the other end, what we're increasingly seeing is that like more and more developers building with these apps start running into honestly, like pretty similar issues that like we'll play just a standard engineer building like a classifier model, which is just like accuracy problems, like, and hallucinations, basically just an accuracy problem,  right? [00:33:24]</p><p>Like it's not giving you the right results. So what do you do? You have to iterate on the model itself. You have to figure out what parameters you tweak. You have to gain some intuition about this entire process. That workflow is pretty similar, honestly, like even if you're not training the model to just like tuning a ML model with like hyper parameters and learning like proper ML practices of like, okay, how do I have like define a good evaluation benchmark? How do I define like the right set of metrics to do to use, right? How do I actually iterate and improve the performance of this pipeline for [00:33:52]</p><p><strong>Alessio</strong>: production? What tools do I use? [00:33:53]</p><p><strong>Jerry</strong>: Right? Like every ML engineer use like some form of weights and biases, tensor boards, or like some other experimentation tracking tool. What tools should I use to actually help build like LLM applications and optimize it for production? There's like a certain amount of just like LLM ops, like tooling and concepts and just like practices that people will kind of have to internalize if they want to optimize these. And so I think that the reason I think being able to build like RAG from scratch is important is it really gives you a sense of like how things are working to get, help you build intuition about like what parameters are within a RAG system and which ones actually tweak to make them better. Cause otherwise I think that one of the advantages of the LlamaIndex quick start is it's three lines of code. The downside of that is you have zero visibility into what's actually going on [00:34:37]</p><p><strong>Alessio</strong>: under the hood. [00:34:37]</p><p><strong>Jerry</strong>: And I think there's something that we've kind of been thinking about for a while and I'm like, okay, let's just release like a new tutorial series. That's just like, we're in set, not no three lines of code. We're just going to go in and actually show you how the thing actually works on [00:34:47]</p><p><strong>Alessio</strong>: the hood. Right. [00:34:47]</p><p><strong>Jerry</strong>: And so I like, does everybody need this? Like probably not as for some people, the three lines of code might work, but I think increasingly, like honestly, 90% of the users I talked to have questions about how to improve the performance of their app. And so just like, given this, it's just like one of those things that's like better for the understanding. [00:35:03]</p><p><strong>Alessio</strong>: Yeah. [00:35:03]</p><p><strong>Swyx</strong>: I'd say it is one of the most useful tools of any sort of developer education toolkit to write things yourself from scratch. So Kelsey Hightower famously wrote Kubernetes the hard way, which is don't use Kubernetes. Here's everything that you would have to do by yourself. And you should be able to put all these things together yourself to understand the value of Kubernetes. And the same thing for LLlamaIndex. I've done, I was the guy who did the same for React. And it's a pretty good exercise for you to just fully understand everything that's going on under the hood. And I was actually going to suggest while in one of the previous conversations, there's all these like hyperparameters, like the size of the chunks and all that. And I was thinking like, what would hyperparameter optimization for RAG look [00:35:44]</p><p><strong>Alessio</strong>: like? [00:35:44]</p><p><strong>Jerry</strong>: Yeah, definitely. I mean, so absolutely. I think that's going to be an increasing thing. I think that's something we're kind of looking at because like, I think someone [00:35:52]</p><p><strong>Swyx</strong>: should just put, do like some large scale study and then just ablate everything. And just you, you tell us. [00:35:57]</p><p><strong>Jerry</strong>: I think it's going to be hard to find a universal default that works for [00:36:00]</p><p><strong>Alessio</strong>: everybody. [00:36:00]</p><p><strong>Jerry</strong>: I think it's going to be somewhat, I do think it's going to be somewhat like dependent on the data and use case. I think if there was a universal default, that would be amazing. But I think increasingly we found, you know, people are just defining their own like custom parsers for like PDFs, markdown files for like, you know, SEC filings versus like Slack conversations. And then like the use case too, like, do you want like a summarization, like the granularity of the response? Like it really affects the parameters that you want to pick. I do like the idea of hyperparameter optimization though, but it's kind of like one of those things where you are kind of like training the model basically kind of on your own data domain. [00:36:36]</p><p><strong>Alessio</strong>: Yeah. [00:36:36]</p><p><strong>Swyx</strong>: You mentioned custom parsers. You've designed LlamaIndex, maybe we can talk about like the surface area of the [00:36:41]</p><p><strong>Alessio</strong>: framework. [00:36:41]</p><p><strong>Swyx</strong>: You designed LlamaIndex in a way that it's more modular, like you mentioned. How would you describe the different components and what's customizable in each? [00:36:50]</p><p><strong>Jerry</strong>: Yeah, I think they're all customizable. And I think that there is a certain burden on us to make that more clear through the [00:36:57]</p><p><strong>Alessio</strong>: docs. [00:36:57]</p><p><strong>Jerry</strong>: Well, number four is customization tutorials. [00:36:59]</p><p><strong>Swyx</strong>: Yeah, yeah. [00:37:00]</p><p><strong>Jerry</strong>: But I think like just in general, I think we do try to make it so that you can plug in the out of the box stuff. But if you want to customize more lower level components, like we definitely encourage you to do that and plug it into the rest of our abstractions. So let me just walk through like maybe some of the basic components of LlamaIndex. There's data loaders. You can load data from different data sources. We have Llama Hub, which you guys brought up, which is, you know, a collection of different data loaders of like unstructured and unstructured data, like PDFs, file types, like Slack, Notion, all that stuff. Now you load in this data. We have a bunch of like parsers and transformers. You can split the text. You can add metadata to the text and then basically figure out a way to load it into like a vector store. So, I mean, you worked at like Airbrite, right? It's kind of like there is some aspect like E and T, right? And in terms of like transforming this data and then the L, right, loading it into some storage abstraction, we have like a bunch of integrations with different document storage systems. [00:37:49]</p><p><strong>Alessio</strong>: So that's data. [00:37:50]</p><p><strong>Jerry</strong>: And then the second piece really is about like, how do you retrieve this data? How do you like synthesize this data and how do you like do some sort of higher level reasoning over this data? So retrieval is one of the core abstractions that we have. We do encourage people to like customize, define your own retrievers, that section on kind of like how do you define your own, like custom retriever, but also we have like out of the box ones. The retrieval algorithm kind of depends on how you structure the data, obviously. Like if you just flat index everything with like chunks with like embeddings, then you can really only do like top K like lookup plus maybe like keyword search or something. But if you can index it in some sort of like hierarchy, like defined relationships, you can do more interesting things like actually traverse relationships between nodes. Then after you have this data, how do you like synthesize the data? [00:38:32]</p><p><strong>Alessio</strong>: Right. [00:38:32]</p><p><strong>Jerry</strong>: Um, and, and this is the part where you feed it into the language model. There's some response abstraction that can abstract away over like long contacts to actually still give you a response, even if the context overflows a context window. And then there's kind of these like higher level, like reasoning primitives that I'm going to define broadly. And I'm just going to call them in some general bucket of like agents, even though everybody has different definitions of agents, but you're the first to data agents, [00:38:56]</p><p><strong>Swyx</strong>: which I was very excited. [00:38:57]</p><p><strong>Alessio</strong>: Yeah. [00:38:57]</p><p><strong>Jerry</strong>: We, we kind of like coin, coin that term. And the way we, we thought about it was, you know, we wanted to think about how to use agents for, uh, like data workflows basically. And, and so what are the reasoning primitives that you want to do? So the most simple reasoning primitive you can do is some sort of routing module. It's a classifier, like given a query, just make some automated decision on what choice to pick, right? You could use LLMs. You don't have to use LLMs. You could just try and classifier basically. That's something that we might actually explore. And then the next piece is, okay, what are some higher level things? You can have the LLM like define like a query plan, right. To actually execute over the data. You can do some sort of while loop, right? That's basically what an agent loop is, which is like react a chain of thought, like the open AI function calling, like while loop to try to like take a question and try to break it down into some, some, uh, series of steps to actually try to execute to get back a response. And so there's a range and complexity from like simple reasoning primitives to more advanced ones. The way we kind of think about it is like, which ones should we implement and how do [00:39:50]</p><p><strong>Alessio</strong>: they work? [00:39:50]</p><p><strong>Jerry</strong>: Well, like, do they work well over like the types of like data tasks that we give them? [00:39:54]</p><p><strong>Alessio</strong>: How do you think about optimizing each piece? So take, um, embedding models is one piece of it. You offer fine tuning, embedding models. And I saw it was like fine tuning gives you like 5, 10% increase. What's kind of like the Delta left on the embedding side? Do you think we can get models that are like a lot better? Do you think like that's one piece where people should really not spend too much time? [00:40:16]</p><p><strong>Jerry</strong>: I just think it's, it's not the only parameter. Cause I think in the end, if you think about everything that goes into retrieval, the chunking algorithm, um, how you define like metadata will bias your embedding representations. Then there's the actual embedding model itself, which is something that you can try optimizing. And then there's like the retrieval algorithm. Are you going to just do top K? Are you going to do like hybrid search? Are you going to do auto retrieval? Like there's a bunch of parameters. And so I do think it's something everybody should try. I think by default we use like OpenAI's embedding model. A lot of people these days use like sentence transformers because it's, it's just like free open source and you can actually optimize, directly optimize it. This is an active area of exploration. I do think one of our goals is it should ideally be relatively free for every developer to just run some fine tuning process over their data to squeeze out some more points and performance. And if it's that relatively free and there's no downsides, everybody should basically do [00:41:04]</p><p><strong>Alessio</strong>: it. [00:41:04]</p><p><strong>Jerry</strong>: There's just some complexities, right? In terms of optimizing your embedding model, especially in a production grade data pipeline. If you actually fine tune the embedding model and the embedding space changes, you're going to have to reindex all your documents. And for a lot of people, that's not feasible. And so I think like Joe from Vespa on our webinars, like there's this idea that depending on if you're just using like document and query embeddings, you could keep the document embeddings frozen and just train a linear transform on the query or, or any sort of transform on the query, right? So therefore it's just a query side transformation instead of actually having to reindex all the document embeddings. That's pretty smart. We weren't able to get like huge performance gains there, but it does like improve performance a little bit. And that's something that basically, you know, everybody should be able to kick off. You can actually do that on LLlamaIndex too. [00:41:45]</p><p><strong>Swyx</strong>: OpenAIO has a cookbook on adding bias to the embeddings too, right? [00:41:49]</p><p><strong>Alessio</strong>: Yeah. [00:41:49]</p><p><strong>Jerry</strong>: There's just like different parameters that you can, you can try adding to try to like optimize the retrieval process. And the idea is just like, okay, by default you have all this text. It kind of lives in some latent space, right? [00:42:01]</p><p><strong>Swyx</strong>: Yeah. Shut out, shut out latent space. You should take a drink every time. [00:42:05]</p><p><strong>Jerry</strong>: But it lives in some latent space. But like depending on the type, specific types of questions that the user might want to ask, the latent space might not be optimized to actually retrieve the relevant piece of context that the user want to ask. So can you shift the embedding points a little bit, right? And how do we do that? Basically, that's really a key question here. So optimizing the embedding model, even changing the way you like chunk things, these all shift the embeddings. [00:42:26]</p><p><strong>Alessio</strong>: So the retrieval is interesting. I got a bunch of startup pitches that are like, like ragged school, but like there's a lot of stuff in terms of ranking that could be better. There's a lot of stuff in terms of sun setting data. Once it starts to become stale, that could be better. Are you going to move into that part too? So like you have SEC Insights as one of kind of like your demos. And that's like a great example of, Hey, I don't want to embed all the historical documents because a lot of them are outdated and I don't want them to be in the context. [00:42:55]</p><p><strong>Jerry</strong>: What's that problem space? [00:42:57]</p><p><strong>Alessio</strong>: Like how much of it are you going to also help with and versus how much you expect others to take care of? [00:43:03]</p><p><strong>Jerry</strong>: Yeah, I'm happy to talk about SEC Insights in just a bit. I think more broadly about the like overall retrieval space. We're very interested in it because a lot of these are very practical problems that [00:43:11]</p><p><strong>Alessio</strong>: people have asked us. [00:43:11]</p><p><strong>Jerry</strong>: And so the idea of outdated data, I think, how do you like deprecate or time wait data and do that in a reliable manner, I guess. So you don't just like set some parameter and all of a sudden that affects your, all your retrieval items, like is pretty important because people have started bringing [00:43:25]</p><p><strong>Alessio</strong>: that up. [00:43:25]</p><p><strong>Jerry</strong>: Like I have a bunch of duplicate documents, things get out of date. How do I like sunset documents? And then remind me, what was the, what was the first thing you said? Cause I think there was, there was something like the ranking ranking, right? [00:43:35]</p><p><strong>Alessio</strong>: Yeah. [00:43:35]</p><p><strong>Jerry</strong>: So I think this space is not new. I think everybody who is new to this space starts learning some basic concepts of information retrieval, which to be fair has been around for quite a bit. But our goal is to kind of like take some of like just general ranking and information retrieval concepts. So by encoding, like crossing coding, right? Like we're based models versus like kind of keyword based search. How do you actually evaluate retrieval? These things start becoming relevant. And so I think for us, like rather than inventing like new retriever techniques for the sake of like just inventing better ranking, we want to take existing ranking techniques and kind of like package it in a way that's like intuitive and easy for people to understand. That said, I think there are interesting and new retrieval techniques that are kind of in place that can be done when you tie it into some downstream rack system. The reason for this is just like, if you think about the idea of like chunking text, right? Like that just really wasn't a thing, or at least for this specific purpose, like the reason chunking is a thing in RAG right now is because like you want to fit within the context bundle of an LLM, right? Like why do you want to chunk a document? That just was less of a thing. I think back then, if you wanted to like transform a document, it was more for like structured data extraction or something in the past. And so there's kind of like certain new concepts that you got to play with that you can use to invent kind of more interesting retrieval techniques. Another example here is actually LLM based reasoning, like LLM based chain of thought reasoning. You can take a question, break it down into smaller components and use that to actually send to your retrieval system. And that gives you better results. And it's kind of like sending the full question to a retrieval system. That also wasn't really a thing back then, but then you can kind of figure out an interesting way to like blending old and the new, right? With LLMs and data. [00:45:13]</p><p><strong>Swyx</strong>: There's a lot of ideas that you come across. Do you have a store of them? [00:45:17]</p><p><strong>Jerry</strong>: Yeah, I think I, sometimes I get like inspiration. There's like some problem statement and I'm just like, oh, it's like, following you is [00:45:23]</p><p><strong>Swyx</strong>: very hard because it's just a lot of homework. [00:45:25]</p><p><strong>Jerry</strong>: So I think I've, I've started to like step on the brakes just a little bit. Cause then I start, no, no, no. Well, the, the reason is just like, okay, if I just have invent like a hundred more retrieval techniques, like, like sure. But like, how do people know which one is good and which one's like bad. [00:45:41]</p><p><strong>Alessio</strong>: Right. [00:45:41]</p><p><strong>Jerry</strong>: And so have a librarian, right? [00:45:42]</p><p><strong>Swyx</strong>: Like it's going to catalog it and you're going to need some like benchmarks. [00:45:45]</p><p><strong>Jerry</strong>: And so I think that's probably the focus for the next, next few weeks is actually like properly kind of like having an understanding of like, oh, you know, when should you do this or like, what does this actually work well? [00:45:54]</p><p><strong>Alessio</strong>: Yeah. [00:45:54]</p><p><strong>Swyx</strong>: Some kind of like a, maybe like a flow chart, decision tree type of thing. Yeah, exactly. When this do that, you know, something like that, that would be really helpful for me. [00:46:02]</p><p><strong>Alessio</strong>: Thank you. [00:46:02]</p><p><strong>Swyx</strong>: It seems like your most successful side project. Yeah. What is SEC Insights for our listeners? [00:46:07]</p><p><strong>Jerry</strong>: Um, our SEC Insights is a full stack LLM chatbot application, um, that does. Analysis of your sec 10 K and 10 Q filings. And so the goal for building this project is really twofold. The reason we started building this was one, it was a great way to dog food, the production readiness for our library. We actually ended up like adding a bunch of stuff and fixing a ton of bugs because of this. And I think it was great because like, you know, thinking about how we handle like callbacks streaming, actually generating like reliable sub responses and bubbling up sources, citations. These are all things that like, you know, if you're just building the library in isolation, you don't really think about it. But if you're trying to tie this into a downstream application, like it really starts mattering for your error messages. When you talk about bubbling up stuff for like sources, like if you go into SEC Insights and you type something, you can actually see the highlights in the right side. That was something that like took a little bit of like, um, understanding to figure out how to build wall. And so it was great for dog fooding improvement of the library itself. And then as we're building the app, um, the second thing was we're starting to talk to users and just like trying to showcase like kind of, uh, bigger companies, like the potential of LLM index as a framework, because these days obviously building a chatbot, right. With Streamlight or something, it'll take you like 30 minutes or an hour. Like there's plenty of templates out there on LLM index, like train, like you can just build a chatbot, but how do you build something that kind of like satisfies some of these, uh, this like criteria of surfacing, like citations, being transparent, seeing like, uh, having a good UX, um, and then also being able to handle different types of questions, right? Like more complex questions that compare different documents. That's something that I think people are still trying to explore. And so what we did was like, we showed, well, first like organizations, the possibilities of like what you can do when you actually build something like this. And then after like, you know, we kind of like stealth launched this for fun, just as a separate project, uh, just to see if we could get feedback from users who are using this world to see like, you know, how we can improve stuff. And then we were thought, we thought like, ah, you know, we built this, right? Obviously we're not going to sell like a financial app. Like that's not really our, in our wheelhouse, but we're just going to open source the entire thing. And so that now is basically just like a really nice, like full stack app template you can use and customize on your own, right. To build your own chatbot, whether it is a really financial documents or like other types of documents. Um, and it provides like a nice template for basically anybody to kind of like go in and get started. There's certain components though, that like aren't released yet that we're going to going to, and then next few weeks, like one is just like kind of more detailed guides on like different modular components within it. So if you're like a full stack developer, you can go in and actually take the pieces that you want and actually kind of build your own custom flows. The second piece is like, take, there's like certain components in there that might not be directly related to the LLM app that would be nice to just like have people use, uh, an example is the PDF viewer, like the PDF viewer with like citations. I think we're just going to give that right. So, you know, you could be using any library you want, but then you can just, you know, just drop in a PDF viewer. [00:48:53]</p><p><strong>Alessio</strong>: Right. [00:48:53]</p><p><strong>Jerry</strong>: So that it's just like a fun little module that you can do. [00:48:55]</p><p><strong>Swyx</strong>: Nice. That's really good community service right there. I want to talk a little bit about your cloud offering, because you mentioned, I forget the name that you had for it. [00:49:04]</p><p><strong>Alessio</strong>: Enterprise something. [00:49:04]</p><p><strong>Jerry</strong>: Well, one, we haven't come up with a name. Uh, we're kind of calling it LLM index platform, platform LLM index enterprise. I'm open to suggestions here. Um, and the second thing is I don't actually know how much I can, I can share right now because it's mostly kind of like, uh, we, we, yeah, exactly. [00:49:20]</p><p><strong>Swyx</strong>: To the extent that you can talk about LLM index as a business. Um, always just want to give people in the mind, like, Hey, like you sell things too, you know what I mean? [00:49:28]</p><p><strong>Jerry</strong>: Yeah, a hundred percent. So I think the high level of what I can probably say is just like, I think we're looking at ways of like actively kind of complimenting the developer experience, like building LLM index. We've always been very focused on stuff around like plugging in your data into the language model. And so can we build tools that help like augment that experience beyond the open [00:49:47]</p><p><strong>Alessio</strong>: source library? Right. [00:49:48]</p><p><strong>Jerry</strong>: And so I think what we're going to do is like make a build an experience where it's very seamless to transition from the open source library with like a one line toggle, you can basically get this like complimentary service and then figure out a way to like monetize in a bit. I think where our revenue focus this year is less emphasized. Like it's more just about like, can we build some manage offering that like provides complimentary value to what the open source library provides? [00:50:09]</p><p><strong>Alessio</strong>: Yeah. [00:50:10]</p><p><strong>Swyx</strong>: I think it's the classic thing about all open source is you want to start building the most popular open source projects in your category to own that category. You're going to make it very easy to host. Therefore you're just built your biggest competitor, which is you. [00:50:22]</p><p><strong>Jerry</strong>: I think it will be like complimentary. Cause I think it will be like, you know, use the open source library and then you have a toggle and all of a sudden, you know, you can see this basically like a pipeline ish thing pop up and then it will be able to kind of like, you'll have a UI. There'll be some enterprise guarantees and the end goal would be to help you build like a production RAG app more easily. [00:50:42]</p><p><strong>Alessio</strong>: Data loaders. There's a lot of them. What are maybe some of the most popular, maybe under, not underrated, but like underexpected, you know, and how has the open source side of it helped with like getting a lot more connectors, you only have six people on the team today, so you couldn't have done it all yourself. [00:51:00]</p><p><strong>Jerry</strong>: Yeah. I think the nice thing about like Walmart hub itself, it's supposed to be a community driven hub. Um, and so actually the bulk of the peers are completely community contributed. Um, and so we haven't written that many like first party connectors actually for this, it's more just like a kind of encouraging people to contribute to the community in terms of the most popular tools, uh, or the data loaders. I think we have Google analytics on this and I forgot the specifics. It's some mix of like the PDF loaders. We have like 10 of them, but there's some subset of them that are popular. And then there's Google, like I think Gmail and like G drive. Um, and then I think maybe it's like one of Slack or notion. One thing I will say though, uh, and I think like Swix might probably knows this better than I do, given that you were, she used to work at air bite. It's very hard to build, like, especially for full on service, like notion Slack or like Salesforce to build like a really, really high quality loader that really extracts all the information that people want. [00:51:51]</p><p><strong>Alessio</strong>: Right. [00:51:51]</p><p><strong>Jerry</strong>: And so I think the thing is when people start out, like they will probably use these loaders and it's a great tool to get started. And for a lot of people, it's like good enough. And they submit PRs if they want more additional features. But if you get to a point where you actually want to call like an API that hasn't been supported yet, or, you know, you want to load in stuff that like in metadata or something that hasn't been directly baked into the logic of a loader itself, people start adding up, like writing their own custom loaders. And that is a thing that we're seeing. That's something that we're okay with. [00:52:18]</p><p><strong>Alessio</strong>: Right. [00:52:18]</p><p><strong>Jerry</strong>: Cause like a lot of this is more just like community driven. And if you want to submit a PR to improve the existing one, you can, otherwise you can create your own custom ones. [00:52:24]</p><p><strong>Alessio</strong>: Yeah. [00:52:25]</p><p><strong>Swyx</strong>: And all that is custom loaders all supported within LLlamaIndex, or do you pair it with something else? [00:52:29]</p><p><strong>Jerry</strong>: Oh, it's just like, I mean, you just define your own subclass. I think, I think that's it. [00:52:33]</p><p><strong>Alessio</strong>: Yeah. Yeah. [00:52:33]</p><p><strong>Swyx</strong>: Cause typically in the data ecosystem with everybody, everybody has his own strategies with custom loaders, but also you could write your own with like Dagster or like Prefect or one of those tools. [00:52:43]</p><p><strong>Alessio</strong>: Yeah. [00:52:44]</p><p><strong>Jerry</strong>: Yeah, exactly. So I think for us, it's more, we just have a very flexible like document abstraction that you can fill in with any content that you want. [00:52:50]</p><p><strong>Swyx</strong>: Are people really dumping all their Gmail into these things? You said Gmail is number two. Uh, I'm not sure actually. I mean, that's these, you know, that's the most private data source. [00:52:59]</p><p><strong>Alessio</strong>: That's true. [00:53:00]</p><p><strong>Swyx</strong>: So I'm surprised that people are dumping too. I mean, I'm sure some, some people are, but like, I'm sure I'm surprised it's [00:53:06]</p><p><strong>Alessio</strong>: popular. [00:53:06]</p><p><strong>Swyx</strong>: Well, and then, so, uh, the LLM engine, uh, I assume OpenAI is going to be a majority. Is it an overwhelming majority? Uh, how, what's the market share between like OpenAI, Cohere, Anthropic, you know, whatever you're seeing. [00:53:21]</p><p><strong>Alessio</strong>: OpenSource too. [00:53:21]</p><p><strong>Jerry</strong>: Yeah, I think it's probably some, uh, OpenAI has a majority, but then like there's Anthropic and there's also, um, OpenSource. I think there is a lot of people trying out like Llama 2, um, and, and, um, some variant of like a top OpenSource model. [00:53:33]</p><p><strong>Swyx</strong>: Side note, any confusion there, Llama 2 versus Llama? [00:53:36]</p><p><strong>Jerry</strong>: Yeah, I think whenever I go to these talks, I always open it up with like, we started before it. Yeah, exactly. We start before meta, right? [00:53:43]</p><p><strong>Alessio</strong>: I want to point that out. [00:53:43]</p><p><strong>Jerry</strong>: Uh, but no, for us, we try to use it for like branding. We just add two llamas when we have like a Llama 2 integration instead of one llama. So I think a lot of people are trying out the popular OpenSource models. Uh, there's a lot of toolkits and OpenSource projects that allow you to self-host and deploy Llama 2 and like, oh, Llama is just a very recent example. I think that we, we added integration with, and so we just, uh, by virtue of having more of these services, I think more and more people are trying it out. [00:54:07]</p><p><strong>Swyx</strong>: Do you think there's, there's potential there? Is like, um, is that going to be an increasing trend? Like OpenSource? [00:54:12]</p><p><strong>Alessio</strong>: Yeah. [00:54:12]</p><p><strong>Jerry</strong>: Yeah, definitely. I think in general people hate monopolies. And so, um, like there's a, whenever like OpenAI has something really cool or like any, um, company has something really cool, even meta, like there's just going to be a huge competitive pressure from other people to do something that's more open and better. Um, and so I do think just market pressures will, will improve like OpenSource adoption. [00:54:32]</p><p><strong>Swyx</strong>: Last thing I'll say about this, which is just really like, it gets clicks. It's people like psychologically want that, but then at the end of the day, they want, they fall for brand name and popular and performance benchmarks. You know, at the end of the day, OpenAI still wins on that. I think that's true. [00:54:47]</p><p><strong>Jerry</strong>: But I, I just think like, unless you were like an active employee at OpenAI, right? Like all these research labs are putting out like ML, like PhDs or kind of like other companies too, that are investing a lot of dollars. Uh, there's going to be a lot of like competitive pressures developed, like better models. So is it going to be like all fully open source with like a permissive license? Like, I'm not completely sure, but like, there's just a lot of just incentive for people to develop their stuff here. [00:55:09]</p><p><strong>Swyx</strong>: Have you looked at like RAG specific models, like contextual? [00:55:12]</p><p><strong>Alessio</strong>: No. [00:55:13]</p><p><strong>Jerry</strong>: Is it public? [00:55:14]</p><p><strong>Swyx</strong>: No, they literally just, uh, so Dewey Keeler. I think it's his name. And you probably came across him. He wrote the RAG paper at Meta and just started contextual AI to create a RAG specific model. I don't know what that means. I was hoping that you do, cause it's your business. [00:55:29]</p><p><strong>Jerry</strong>: I had insider information. I mean, you know, to be honest, I think this, this kind of relates to my previous point on like RAG and fine tuning, like a RAG specific model is a model architecture that's designed for better RAG and it's less the software engineering principle of like, how can I take existing stuff and just plug and play different components into it? Um, and there's a beauty in that from ease of use and modularity, but when you want to end to end optimize the thing, you might want a more specific model. I think, I think building your own models is honestly pretty hard. Um, and I think the issue is if you also build your own models, like you're also just gonna have to keep up with like the rate of LM advances, like how, like basically the question is when GPT five and six and whatever, like anthropic cloud three comes out, how can you prove that you're actually better than, uh, software developers cobbling together and components on top of a base model. Right. Even if it's just like conceptually, this is better than maybe like GPT three or GPT four. [00:56:21]</p><p><strong>Alessio</strong>: What about vector stores? I know Spooks is wearing a chroma sweatshirt. [00:56:25]</p><p><strong>Swyx</strong>: Yeah, because they use a swagging. [00:56:27]</p><p><strong>Jerry</strong>: I have, I have the mug from Chroma. [00:56:29]</p><p><strong>Alessio</strong>: Yeah. It's been great. Yeah. [00:56:30]</p><p><strong>Jerry</strong>: What do you think there? [00:56:31]</p><p><strong>Alessio</strong>: Like there's a lot of them. Are they pretty interchangeable for like your users use case? Uh, is HNSW all we need? Is there room for improvements? [00:56:40]</p><p><strong>Swyx</strong>: Is NTRA all we need? [00:56:42]</p><p><strong>Jerry</strong>: I think, um, yeah, we try to remain unopinionated about storage providers. So it's not like we don't try to like play favorites. So we have like a bunch of integrations obviously. And we, the way we try to do it is we just tried to find like some standard interfaces, but obviously like different vector stores will support kind of like, uh, slightly additional things like metadata filters and those things. I mean, the goal is to have our users basically leave it up to them to try to figure out like what makes sense for their use case in terms of like the algorithm itself, I don't think the Delta on like improving the vector store, like. Embedding lookup algorithm. [00:57:10]</p><p><strong>Alessio</strong>: Is that high? [00:57:10]</p><p><strong>Jerry</strong>: I think the stuff has been mostly solved or at least there's just a lot of other stuff you can do to try to improve the overall performance. No, I mean like everything else that we just talked about, like in terms of like [00:57:20]</p><p><strong>Alessio</strong>: accuracy, right. [00:57:20]</p><p><strong>Jerry</strong>: To improve rag, like everything that we talked about, like chunking, like metadata, like. [00:57:24]</p><p><strong>Swyx</strong>: I mean, I was just thinking like, maybe for me, the interesting question is, you know, there are like eight, it's a kind of game of thrones. There's like eight, the war of eight databases right now. Oh, I see. Um, how do they stand out and how did they become very good partners? [00:57:36]</p><p><strong>Alessio</strong>: If not my index. [00:57:36]</p><p><strong>Jerry</strong>: Yeah, we're pretty good partners with, with most of them. [00:57:39]</p><p><strong>Alessio</strong>: Uh, let's see. [00:57:39]</p><p><strong>Swyx</strong>: Well, like if you're a, you know, vector database founder, like what do you, what do you work on? [00:57:44]</p><p><strong>Alessio</strong>: It's a good question. [00:57:44]</p><p><strong>Jerry</strong>: I think one thing I'm very interested in is, and this is something I think I've started to see a general trend towards is combining structured data querying with unstructured data querying. Um, and I think that will probably just expand the query sophistication of these vector stores and basically make it so that users don't have to think about whether they would just call this like hybrid querying. [00:58:05]</p><p><strong>Swyx</strong>: Is that what we've it's doing? [00:58:06]</p><p><strong>Alessio</strong>: Yeah. [00:58:07]</p><p><strong>Jerry</strong>: I mean, I think like, if you think about metadata filters, that's basically a structured filter. It's like our select where something equals something, and then you combine that with semantic search. I think like Lance DB or something was like, uh, try, I was trying to do some like joint interface. The reason is like most data is semi-structured. There's some structured annotations and there's some like unstructured texts. And so like, um, somehow combining all the expressivity of like SQL with like the flexibility of semantic search is something that I think is going to be really important. We have some basic hacks right now that allow you to jointly query both a SQL database and like a separate SQL database and a vector store to like combine the information. That's obviously going to be less efficient than if you just combined it into one [00:58:46]</p><p><strong>Alessio</strong>: system. Yeah. [00:58:46]</p><p><strong>Jerry</strong>: And so I think like PG vector, like, you know, that type of stuff, I think it's starting to get there, but like in general, like how do you have an expressive query language to actually do like structured querying along with like all the capabilities, semantic search. [00:58:57]</p><p><strong>Swyx</strong>: So your current favorite is just put it into Postgres. No, no, no. We don't play with Postgres language, the query language. [00:59:05]</p><p><strong>Jerry</strong>: I actually don't know what the best language would be for this, because I think it will be something that like the model hasn't been fine-tuned over. Um, and so you might want to train the model over this, but some way of like expressing structured data filters, and this could be include time too, right? It could, it doesn't have to just be like a where clause with this idea of like a [00:59:26]</p><p><strong>Alessio</strong>: semantic search. Yeah. [00:59:27]</p><p><strong>Swyx</strong>: And we talked about, uh, graph representations. [00:59:30]</p><p><strong>Alessio</strong>: Yeah. Oh yeah. [00:59:30]</p><p><strong>Jerry</strong>: That's another thing too. And there's like, yeah. So that's actually something I didn't even bring up yet. Like there's this interesting idea of like, can you actually have the language model, like explore like relationships within the data too, right? And somehow combine that information with stuff that's like more and more, um, structured within the DB. [00:59:46]</p><p><strong>Alessio</strong>: Awesome. [00:59:46]</p><p><strong>Swyx</strong>: What are your current strong beliefs about how to evaluate RAG ? [00:59:49]</p><p><strong>Jerry</strong>: I think I have thoughts. I think we're trying to curate this into some like more opinionated principles because there's some like open questions here. I think one question I had to think about is whether you should do like evals like component by component first, or is yours do the end to end thing? I think you should, you might actually just want to do the end to end thing first, just to do a sanity check of whether or not like this, uh, given a query and the final response, whether or not it even makes sense, like you eyeball [01:00:11]</p><p><strong>Alessio</strong>: it, right. [01:00:11]</p><p><strong>Jerry</strong>: And then you like try to do some basic evals. And then once you like diagnose what the issue is, then you go into the kind of like specific area to define some more, uh, solid benchmarks and try to like [01:00:21]</p><p><strong>Alessio</strong>: improve stuff. [01:00:21]</p><p><strong>Jerry</strong>: So what is Antoine evals? Like it's, you, um, have a query, it goes in through retrieval system. You get back something, you synthesize response, and that's your final thing. And you evaluate the quality of the final response. And these days, there's plenty of projects like startups, like companies research, doing stuff around like GPT-4, right. As like a human judge to basically kind of like synthetically generate data. [01:00:41]</p><p><strong>Swyx</strong>: I don't know from the startup side. [01:00:43]</p><p><strong>Jerry</strong>: I just know from a technical side, I think, I think people are going to do more of it. The main issue right now is just, uh, it's really unreliable. Like it's, it's just, uh, like there's like variants on the response, whatever you want. [01:00:54]</p><p><strong>Alessio</strong>: They won't do more of it. [01:00:54]</p><p><strong>Swyx</strong>: I mean, cause it's bad. [01:00:55]</p><p><strong>Jerry</strong>: No, but, but these models will get better and you'll probably fine tune a model to [01:00:59]</p><p><strong>Alessio</strong>: be a better judge. [01:00:59]</p><p><strong>Jerry</strong>: I think that's probably what's going to happen. So I'm like reasonably bullish on this because I don't think there's really a good alternative beyond you just human annotating a bunch of data sets, um, and then trying to like just manually go through and curating, like evaluating eval metrics. And so this is just going to be a more scalable solution in terms of the [01:01:17]</p><p><strong>Alessio</strong>: startups. Yeah. [01:01:17]</p><p><strong>Jerry</strong>: I mean, I think there's a bunch of companies doing this in the end. It probably comes down to some aspect of like UX speed, whether you can like fine tune a model. So that's end to end evals. And then I think like what we found is for rag, a lot of times, like, uh, what ends up affecting this, like end response is retrieval. You're just not able to retrieve the right response. And so I think having proper retrieval benchmarks, especially if you want to do production RAG is, is actually quite important. I think what does having good retrieval metrics tell you? It tells you that at least like the retrieval is good. It doesn't necessarily guarantee the end generation is good, but at least it gives you some, uh, sanity track, right? So you can like fix one component while optimizing the rest, what retrieval like evaluation is pretty standard. And it's been around for a while. It's just like an IR problem. Basically you have some like input query, you get back some retrieves out of context, and then there's some ground truth and that ranked set. And then you try to measure it based on ranking metrics. So the closer that ground truth is to the top, the more you reward the evals. And then the closer it is to the bottom where if it's not in the retrieve side at all, then you penalize the evals. Um, and so that's just like a classic ranking problem. I think like most people starting out probably don't know how to do this right [01:02:28]</p><p><strong>Alessio</strong>: now. [01:02:28]</p><p><strong>Jerry</strong>: We, we just launched them like basic retrieval evaluation modules to help users [01:02:32]</p><p><strong>Alessio</strong>: do this. [01:02:32]</p><p><strong>Jerry</strong>: One is just like curating this data set in the first place. And one thing that we're very interested in is this idea of like synthetic data set generation for evals. So how can you give in some context, generate a set of questions with Drupal 2.4, and then all of a sudden you have like question and then context pairs, and that becomes your ground truth. [01:02:47]</p><p><strong>Swyx</strong>: Are data agent evals the same thing, or is there a separate set of stuff for agents that you think is relevant here? [01:02:53]</p><p><strong>Jerry</strong>: Yeah, I think data agents add like another layer of complexity. Cause then it's just like, you have just more loops in the system. Like you can evaluate like each chain of thought loop itself, like every LLM call to see whether or not the input to that specific step in the chain of thought process actually works or is correct. Or you can evaluate like the final response to see if that's correct. This gets even more complicated when you do like multi-agent stuff, because now you have like some communication between like different agents. Like you have a top level orchestration agent passing it on to some low level [01:03:24]</p><p><strong>Alessio</strong>: stuff. [01:03:24]</p><p><strong>Jerry</strong>: I'm probably less familiar with kind of like agent eval frameworks. I know they're, they're starting to be, become a thing. Talking to like June from the Drown of Agents paper, which is pretty unrelated to what we're doing now. But it's very interesting where it's like, so you can kind of evaluate like overall agent simulations by just like kind of understanding whether or not they like modeled the distribution of human behavior. But that's not like a very macro principle. [01:03:46]</p><p><strong>Alessio</strong>: Right. [01:03:46]</p><p><strong>Jerry</strong>: And that's very much to evaluate stuff, to kind of like model the distribution of [01:03:51]</p><p><strong>Alessio</strong>: things. [01:03:51]</p><p><strong>Jerry</strong>: And I think that works well when you're trying to like generate something for like creative purposes, but for stuff where you really want the agent to like achieve a certain task, it really is like whether or not it achieved the task or not. [01:04:01]</p><p><strong>Alessio</strong>: Right. [01:04:01]</p><p><strong>Jerry</strong>: Cause then it's not like, Oh, does it generally mimic human behavior? It's like, no, like did you like send this email or not? [01:04:07]</p><p><strong>Alessio</strong>: Right. [01:04:07]</p><p><strong>Jerry</strong>: Like, cause otherwise like this, this thing didn't work. [01:04:09]</p><p><strong>Alessio</strong>: Awesome. Let's jump into a lightning round. So we have two questions, acceleration, exploration, and then one final tag away. The acceleration question is what's something that already happened in AI that you thought would take much longer to get here? [01:04:23]</p><p><strong>Jerry</strong>: I think just the ability of LLMs to generate believable outputs and for text and also for images. And I think just the whole reason I started hacking around with LLMs, honestly, I felt like I got into it pretty late. I should've gotten into it like early 2022 because UB23 had been out for a while. Like just the fact that there was this engine that was capable of like reasoning and no one was really like tapping into it. And then the fact that, you know, I used to work in image generation for a while. Like I did GANs and stuff back in the day. And that was like pretty hard to train. You would generate these like 32 by 32 images. And then now taking a look at some of the stuff by like Dolly and, and, you know, mid journey and those things. So it's, it's just, it's, it's very good. [01:04:59]</p><p><strong>Alessio</strong>: Yeah. [01:04:59]</p><p><strong>Swyx</strong>: Exploration. What do you think is the most interesting unsolved question in AI? [01:05:03]</p><p><strong>Jerry</strong>: Yeah, I'd probably work on some aspect of, um, like personalization of memory. Like, I think I actually think that I don't think anyone's like, I think a lot of people have thoughts about that, but like, for what it's worth, I don't think the final state will be right. I think it will be some, some like fancy algorithm or architecture where you like bake it into like the, the architecture of the model itself. Like if, if you have like a personalized assistant that you can talk to that will like learn behaviors over time, right. And learn stuff through like conversation history, what exactly is the right architecture there? I do think that will be part of like the wrong continuous fine tuning. [01:05:38]</p><p><strong>Swyx</strong>: Yeah. [01:05:39]</p><p><strong>Jerry</strong>: Like some aspect of that, right. [01:05:40]</p><p><strong>Alessio</strong>: Right. [01:05:40]</p><p><strong>Jerry</strong>: Like these are like, I don't actually know the specific technique, but I don't think it's just going to be something where you have like a fixed vector store and that, that thing will be like the thing that restores all your memories. [01:05:48]</p><p><strong>Swyx</strong>: It's interesting because I feel like using model weights for memory, it's just such an unreliable storage device. [01:05:56]</p><p><strong>Jerry</strong>: I know. But like, I just think, uh, from like the AGI, like, you know, just modeling like the human brain perspective, I think that there is something nice about just like being able to optimize that system. [01:06:08]</p><p><strong>Alessio</strong>: Right. [01:06:08]</p><p><strong>Jerry</strong>: And to optimize a system, you need parameters and then that's where you just get into the neural net piece. [01:06:12]</p><p><strong>Alessio</strong>: Cool. Cool. Uh, and yeah, take away, you got the audience ear. What's something you want everyone to think about or yeah, take away from this conversation and your thinking. [01:06:24]</p><p><strong>Jerry</strong>: I think there were a few key things. Uh, so we talked about two of them already, which was SEC Insights, which if you guys haven't tracked it out, I've definitely encouraged you to do so because it's not just like a random like sec app, it's like a full stack thing that we open source, right. And so if you guys want to track it out, I would definitely do that. It provides a template for you to build kind of like production grade rack apps. Um, and we're going to open source like, and modularize more components of that soon and do a workshop on, um, yeah. And the second piece is I think we are thinking a lot about like retrieval and evals. Um, I think right now we're kind of exploring integrations with like a few different partners. And so hopefully some of that will be, uh, really soon. And so just like, how do you basically have an experience where you just like write law index code, all of a sudden you can easily run like retrievals, evals, and like traces, all that stuff. And, and like a service. And so I think we're working with like a few providers on that. And then the other piece, which we did talk about already is this idea of like, yeah, building like RAG from scratch. I mean, I think everybody should do it. I think I would check out the guide. If you guys haven't already, I think it's in our docs, but instead of just using, you know, either the kind of like the retriever query engine and lamin decks or like the conversational QA train and Lang train, it's, I would take a look at how do you actually chunk parse data and do like top cam batting retrieval, because I really think that by doing that process, it helps you understand the decisions, the prompts, the language models to use. [01:07:42]</p><p><strong>Alessio</strong>: That's it. Yeah. [01:07:44]</p><p><strong>Swyx</strong>: Thank you so much, Jerry. [01:07:45]</p><p><strong>Alessio</strong>: Yeah. [01:07:45]</p><p><strong>Jerry</strong>: Thank you. [01:07:46]</p> <br /><br />Get full access to Latent Space at <a href=\"https://www.latent.space/subscribe?utm_medium=podcast&#38;utm_campaign=CTA_4\">www.latent.space/subscribe</a>\n",
      "Transcript</p><p><strong>Alessio</strong>: Hey everyone, welcome to the Latent Space Podcast. This is Alessio, partner and CTO of Residence and <a href=\"https://decibel.vc/\" target=\"_blank\">Decibel Partners</a>, and I'm joined by my co-host Swyx, founder of <a href=\"https://github.com/smol-ai\" target=\"_blank\">Smol AI</a>. [00:00:20]</p><p><strong>Swyx</strong>: And today we finally have Jerry Liu on the podcast. Hey Jerry. [00:00:24]</p><p><strong>Jerry</strong>: Hey guys. Hey Swyx and Alessio. Thanks for having me. [00:00:27]</p><p><strong>Swyx</strong>: It's kind of weird because we keep running into each other in San Francisco AI events, so it's kind of weird to finally just have a conversation recorded for everybody else. [00:00:34]</p><p><strong>Jerry</strong>: Yeah, I know. I'm really looking forward to this, aside from the questions. [00:00:38]</p><p><strong>Swyx</strong>: So I tend to introduce people on their formal background and then ask something on the more personal side. So you are part of the Princeton gang. [00:00:46]</p><p><strong>Jerry</strong>: I don't know if there is like official Princeton gang. [00:00:48]</p><p><strong>Swyx</strong>: No, small Princeton gang. Okay. I attended your meeting. There was like four of you with Prem and the others. And then you have a bachelor's in CS and a certificate in finance. That's also fun. I also did finance and I think I saw that you also interned at Two Sigma where I worked in New York. You were a machine learning engineer. [00:01:06]</p><p><strong>Jerry</strong>: You were at Two Sigma?</p><p><strong>Swyx</strong>: Yeah, very briefly.</p><p><strong>Jerry</strong>: Oh, cool. I didn't know that. [00:01:09]</p><p><strong>Swyx</strong>: That was my first like proper engineering job before I went into DevRel. [00:01:12]</p><p><strong>Jerry</strong>: Oh, okay. Nice. [00:01:14]</p><p><strong>Swyx</strong>: And then you were a machine learning engineer at Quora, AI research scientist at Uber for three years, and then two years machine learning engineer at Robust Intelligence before starting LlamaIndex. So that's your LinkedIn. It's not only LinkedIn that people should know about you. [00:01:27]</p><p><strong>Jerry</strong>: I think back during my Quora days, I had this like three-month phase where I just wrote like a ton of Quora answers. And so I think if you look at my tweets nowadays, you can basically see that as like the V2 of my three-month like Forrestant where I just like went ham on Quora for a bit. I actually, I think I was back then actually when I was working on Quora, I think the thing that everybody was fascinated in was just like general like deep learning advancements and stuff like GANs and generative like images and just like new architectures that were evolving. And it was a pretty exciting time to be a researcher actually, because you were going in like really understanding some of the new techniques. So I kind of use that as like a learning opportunity, basically just like read a bunch of papers and then answer questions on Quora. And so you can kind of see traces of that basically in my current Twitter where it's just like really about kind of like framing concepts and trying to make it understandable and educate other users on it. Yeah. [00:02:17]</p><p><strong>Swyx</strong>: I've said, so a lot of people come to me for my Twitter advice, but like, I think you are doing one of the best jobs in AI Twitter, which is explaining concepts and just consistently getting hits out. Thank you. I didn't know it was due to the Quora training. Let's just sign on on Quora. A lot of people, including myself, like kind of wrote off Quora as like one of the web 1.0 like sort of question answer forums. But now I think it's becoming, seeing a resurgence obviously due to Poe and obviously Adam and D'Angelo has always been a leading tech figure, but what do you think is kind of underrated about Quora? [00:02:46]</p><p><strong>Jerry</strong>: Well, I mean, I like the, I really liked the mission of Quora when I, when I joined. In fact, I interned there like in 2015 and I joined full time in 2017. One is like they had, and they have like a very talented engineering team and just like really, really smart people. And the other part is the whole mission of the company is to just like spread knowledge and to educate people. And to me that really resonated. I really liked the idea of just like education and democratizing the flow of information. If you imagine like kind of back then it was like, okay, you have Google, which is like for search, but then you have Quora, which is just like user generated, like grassroots type content. And I really liked that concept because it's just like, okay, there's certain types of information that aren't accessible to people, but you can make accessible by just like surfacing it. And so actually, I don't know if like most people know that about like Quora and if they've used the product, whether through like SEO, right, or kind of like actively, but that really was what drew me to it. [00:03:39]</p><p><strong>Swyx</strong>: Yeah. I think most people challenges with it is that sometimes you don't know if it's like a veiled product pitch, right? [00:03:44]</p><p><strong>Jerry</strong>: Yeah. Of course, like quality of the answer matters quite a bit. And then you start running into these like- [00:03:47]</p><p><strong>Swyx</strong>: It's like five alternatives and then here's the one I work on. Yeah. [00:03:50]</p><p><strong>Jerry</strong>: Like recommendation issues and all that stuff. I used, I worked on recsys at Quora actually, so I got a taste of some of that stuff. Well, I mean, I kind of more approached it from machine learning techniques, which might be a nice segue into RAG actually. A lot of it was just information retrieval. We weren't like solving anything that was like super different than what was standard in the industry at the time, but just like ranking based on user preferences. I think a lot of Quora was very metrics driven. So just like trying to maximize like daily active hours, like time spent on site, those types of things. And all the machine learning algorithms were really just based on embeddings. You have a user embedding and you have like item embeddings and you try to train the models to try to maximize the similarity of these. And it's basically a retrieval problem. [00:04:30]</p><p><strong>Swyx</strong>: Okay. So you've been working on RAG for longer than most people think? [00:04:33]</p><p><strong>Jerry</strong>: Well, kind of. So I worked there for like a year, right, just transparently. And then I worked at Uber where I was not working on ranking. It was more like kind of deep learning training for self-driving and computer vision and that type of stuff. But I think in the LLM world, it's kind of just like a combination of like everything these days. I mean, retrieval is not really LLMs, but like it fits within the space of like LLM apps. And then obviously like having knowledge of the underlying deep learning architectures helps. Having knowledge of basic software engineering principles helps too. And so I think it's kind of nice that like this whole LLM space is basically just a combination of just like a bunch of stuff that you probably like people have done in the past. [00:05:11]</p><p><strong>Swyx</strong>: It's good. It's like a summary capstone project. Yeah, exactly. [00:05:14]</p><p><strong>Jerry</strong>: Yeah. [00:05:15]</p><p><strong>Alessio</strong>: And before we dive into LlamaIndex, what do they feed you a robust intelligence that both you and Harrison from LangChain came out of it at the same time? Was there like, yeah. Is there any fun story of like how both of you kind of came up with kind of like core infrastructure to LLM workflows today? Or how close were you at robust? Like any fun behind the scenes? [00:05:37]</p><p><strong>Jerry</strong>: Yeah. Yeah. We, um, we work pretty closely. I mean, we were on the same team for like two years. I got to know Harrison and the rest of the team pretty well. I mean, I have a respect that people there, the people that were very driven, very passionate. And it definitely pushed me to be, you know, a better engineer and leader and those types of things. Yeah. I don't really have a concrete explanation for this. I think it's more just, we have like an LLM hackathon around like September. This was just like exploring GPT-3 or it was October actually. And then the day after I went on vacation for a week and a half, and so I just didn't track Slack or anything. And then when I came back, saw that Harrison started LangChain [00:06:09]</p><p><strong>Swyx</strong>: Oh that's cool. [00:06:10]</p><p><strong>Jerry</strong>: I was like, oh, I'll play around with LLMs a bit and then hacked around on stuff. And I think I've told the story a few times, but you know, I was like trying to feed in information into GPT-3. And then, then you deal with like context window limitations and there was no tooling or really practices to try to understand how do you, you know, get GPT-3 to navigate large amounts of data. And that's kind of how the project started. Really was just one of those things where early days, like we were just trying to build something that was interesting. Like I wanted to start a company. I had other ideas actually of what I wanted to start. And I was very interested in, for instance, like multimodal data, like video data and that type of stuff. And then this just kind of grew and eventually took over the other idea. [00:06:48]</p><p><strong>Swyx</strong>: Text is the universal interface. [00:06:50]</p><p><strong>Jerry</strong>: I think so. I think so. I actually think once the multimodal models come out, I think there's just like mathematically nicer properties of you can just get like join multiple embeddings, like clip style. But text is really nice because from a software engineering principle, it just makes things way more modular. You can just convert everything into text and then you just represent everything as text. [00:07:08]</p><p><strong>Swyx</strong>: Yeah. I'm just explaining retroactively why working on LlamaIndex took off versus if you had chose to spend your time on multimodal, we probably wouldn't be talking about whatever you ended up working on. [00:07:18]</p><p><strong>Jerry</strong>: Yeah. [00:07:19]</p><p><strong>Swyx</strong>: That's true. It's troubled. Interesting. So November 9th, that was a very productive month. I guess October, November, November 9th, you announced GPT-3 Index and you picked a tree logo. Very cool. Every project must have an emoji. [00:07:32]</p><p><strong>Jerry</strong>: Yeah. Yeah. I probably was somewhat inspired by a light train, but I will admit, yeah. [00:07:37]</p><p><strong>Swyx</strong>: It uses GPT to build a knowledge tree in a bottoms-up fashion by applying a summarization prompt for each node. Yep. Which I like that original vision. Your messaging roundabout then was also that you're creating optimized data structures. What's the sort of journey to that and how does that contrast with LlamaIndex today? Okay. [00:07:56]</p><p><strong>Jerry</strong>: Maybe I can tell a little bit about the beginning intuitions. I think when I first started, this really wasn't supposed to be something that was like a toolkit that people use. It was more just like a system. And the way I wanted to think about the system was more a thought exercise of how language models with their reasoning capabilities, if you just treat them as like brains, can organize information and then traverse it. So I didn't want to think about embeddings, right? To me, embeddings just felt like it was just an external thing that was like, well, it was just external to trying to actually tap into the capabilities of language models themselves, right? I really wanted to see, you know, just as like a human brain could like synthesize stuff, could we create some sort of like structure where this neural CPU, if you will, can like organize a bunch of information, you know, auto-summarize a bunch of stuff and then also traverse the structure that I created. That was the inspiration for this initial tree index, to be honest. And I think I said this in the first tweet, it actually works super well, right? Like GPT-4 obviously is much better at reasoning. I'm one of the first to say, you know, you shouldn't use anything pre-GPT-4 for anything that requires complex reasoning because it's just going to be unreliable, okay, disregarding stuff like fine tuning. But it worked okay. But I think it definitely struck a chord with kind of like the Twitter crowd, which is just like new ideas at the time, I guess, just like thinking about how you can actually bake this into some sort of application. Because I think what I also ended up discovering was the fact that there was starting to become a wave of developers building on top of GPT-3 and people were starting to realize that what makes them really useful is to apply them on top of your personal data. And so even if the solution itself was kind of like primitive at the time, like the problem statement itself was very powerful. And so I think being motivated by the problem statement, right, like this broad mission of how do I unlock elements on top of the data also contributed to the development of LOM index to the state it is today. And so I think part of the reason, you know, our toolkit has evolved beyond the just existing set of like data structures is we really tried to take a step back and think, okay, what exactly are the tools that would actually make this useful for a developer? And then, you know, somewhere around December, we made an active effort to basically like push towards that direction, make the code base more modular, right, more friendly as an open source library. And then also start adding in like embeddings, start thinking into practical considerations like latency, cost, performance, those types of things. And then really motivated by that mission, like start expanding the scope of the toolkit towards like covering the life cycle of like data ingestion and querying. Where you also added Llamahub and yeah, so I think that was in like January on the data loading side. And so we started adding like some data loaders, saw an opportunity there, started adding more stuff on the retrieval querying side, right? We still have like the core data structures, but how do you actually make them more modular and kind of like decouple storing state from the types of like queries that you could run on top of this a little bit. And then starting to get into more complex interactions, like chain of thought reasoning, routing and, you know, like agent loops. [00:10:44]</p><p><strong>Alessio</strong>: You and I spent a bunch of time earlier this year talking about Llamahub, what that might become. You were still at Robust. When did you decide it was time to start the company and then start to think about what LlamaIndex is today? [00:10:58]</p><p><strong>Jerry</strong>: Yeah, I mean, probably December. It was kind of interesting. I was getting some inbound from initial VCs, I was talking about this project. And then in the beginning, I was like, oh, yeah, you know, this is just like a design project. But you know, what about my other idea on like video data, right? And then I was trying to like get their thoughts on that. And then everybody was just like, oh, yeah, whatever, like that part's like a crowded market. And then it became clear that, you know, this was actually a pretty big opportunity. And like, coincidentally, right, like this actually did relate to like, my interests have always been at the intersection of AI data and kind of like building practical applications. And it was clear that this was evolving into a much bigger opportunity than the previous idea was. So around December, and then I think I gave a pretty long notice, but I left officially like early March. [00:11:39]</p><p><strong>Alessio</strong>: What were your thinkings in terms of like moats and, you know, founders kind of like overthink it sometimes. So you obviously had like a lot of open source love and like a lot of community. And you're like, were you ever thinking, okay, I don't know, this is maybe not enough to start a company or did you always have conviction about it? [00:11:59]</p><p><strong>Jerry</strong>: Oh, no, I mean, 100%. I felt like I did this exercise, like, honestly, probably more late December and then early January, because I was just existentially worried about whether or not this would actually be a company at all. And okay, what were the key questions I was thinking about? And these were the same things that like other founders, investors, and also like friends would ask me is just like, okay, what happens if context windows get much bigger? What's the point of actually structuring data right in the right way? Right? Why don't you just dump everything into the prompt, fine tuning, like, what if you just train the model over this data? And then, you know, what's the point of doing this stuff? And then some other ideas is what if like OpenAI actually just like takes this like builds upwards on top of the their existing like foundation models and starts building in some like built in orchestration capabilities around stuff like RAG and agents and those types of things. And so I basically ran through this mental exercise and, you know, I'm happy to talk a little bit more about those thoughts as well. But at a high level, well, context windows have gotten bigger, but there's obviously still a need for a rag. I think RAG is just like one of those things that like, in general, what people care about is, yes, they do care about performance, but they also care about stuff like latency and costs. And so my entire reasoning at the time was just like, okay, like, yes, maybe you will have like much bigger context windows, as we've seen with like 100k context windows. But for enterprises, like, you know, data, which is not in just like the scale of like a few documents, it's usually in like gigabytes, terabytes, petabytes. How do you actually just unlock language models over that data, right? And so it was clear there was just like, whether it's RAG or some other paradigm, no one really knew what that answer was. And so there was clearly like technical opportunity here. Like there was just stacks that needed to be invented to actually solve this type of problem, because language models themselves didn't have access to this data. The other piece here is just like, and so if like you just dumped all this data into, let's say a model had like hypothetically an infinite context window, right? And you just dump like 50 gigabytes of data into a context window. That just seemed very inefficient to me, because you have these network transfer costs of uploading 50 gigabytes of data to get back a single response. And so I kind of realized, you know, there's always going to be some curve, regardless of like the performance of the best performing models of like cost versus performance. What RAG does is it does provide extra data points along that access, because you kind of control the amount of context you actually wanted to retrieve. And of course, like RAG as a term was still evolving back then, but it was just this whole idea of like, how do you just fetch a bunch of information to actually, you know, like stuff into the prompt. And so people even back then were kind of thinking about some of those considerations. [00:14:29]</p><p><strong>Swyx</strong>: And then you fundraised in June, or you announced your fundraiser in June. Yeah. Take us through that process of thinking about the fundraise and your plans for the company, you know, at the time. Yeah, definitely. [00:14:41]</p><p><strong>Jerry</strong>: I mean, I think we knew we wanted to, I mean, obviously we knew we wanted to fundraise. There was also a bunch of like investor interest, and it was probably pretty unusual given the, you know, like hype wave of generative AI. So like a lot of investors were kind of reaching out around like December, January, February. In the end, we went with Greylock. Greylock's great. You know, they've been great partners so far. And to be honest, like there's a lot of like great VCs out there. And a lot of them who are specialized on like open source, data, infra, and that type of stuff. What we really wanted to do was, because for us, like time was of the essence, like we wanted to ship very quickly and still kind of build Mindshare in this space. We just kept the fundraising process very efficient. I think we basically did it in like a week or like three days. And so, yeah, just like front loaded it and then just like pick the one named Jerry. Yeah, exactly. Yeah. [00:15:27]</p><p><strong>Swyx</strong>: I'm kidding. I mean, he's obviously great and Greylock's a fantastic firm. [00:15:32]</p><p><strong>Jerry</strong>: Embedding some of my research. So, yeah, just we've had Greylock. They've been great partners. I think in general, when I talk to founders about like the fundraise process, it's never like the most fun period, I think, because it's always just like, you know, there's a lot of logistics, there's lawyers you have to, you know, get in the loop. And like a lot of founders just want to go back to building. I think in the end, we're happy that we kept it to a pretty efficient process. [00:15:54]</p><p><strong>Swyx</strong>: And so you fundraise with Simon. How do you split things with him? How big is your team now? [00:15:57]</p><p><strong>Jerry</strong>: The team is growing. By the time this podcast is released, we'll probably have had one more person join the team. So basically, it's between, we're rapidly getting to like eight or nine people. At the current moment, we're around like six. And so just like there'll be some exciting developments in the next few weeks. I'm excited to announce that. So the team is, has kind of like, we've been pretty selective in terms of like how we like grow the team. Obviously, like we look for people that are really active in terms of contributions to Lum Index, people that have like very strong engineering backgrounds. And primarily, we've been kind of just looking for builders, people that kind of like grow the open source and also eventually this like managed like enterprise platform as well with us. In terms of like Simon, yeah, I've known Simon for a few years now. I knew him back at Uber ATG in Toronto. He's one of the smartest people I knew, has a sense of both like a deep understanding of ML, but also just like first principles thinking about like engineering and technical concepts in general. And I think one of my criteria, criteria is when I was like looking for a co-founder for this project with someone that was like technically better than me, because I knew I wanted like a CTO. And so honestly, like there weren't a lot of people that, I mean, there's, I know a lot of people that are smarter than me, but like that fit that bill. We're willing to do a startup and also just have the same like values that I shared. Right. And just, I think doing a startup is very hard work, right? It's not like, I'm sure like you guys all know this, it's, it's a lot of hours, a lot of late nights and you want to be like in the same place together and just like being willing to hash out stuff and have that grit basically. And I really looked for that. And so Simon really fit that bill and I think I convinced him to bring Trump on board. [00:17:24]</p><p><strong>Swyx</strong>: Yeah. And obviously I've had the pleasure of chatting and working with a little bit with both of you. What would you say those, those like your top one or two values are when, when thinking about that or the culture of the company and that kind of stuff? [00:17:36]</p><p><strong>Jerry</strong>: I think in terms of the culture of the company, it's really like, I mean, there's a few things I can name off the top of my head. One is just like passion, integrity. I think that's very important for us. We want to be honest. We don't want to like, obviously like copy code or, or kind of like, you know, just like, you know, not give attribution, those types of things and, and just like be true to ourselves. I think we're all very like down to earth, like humble people, but obviously I think just willingness to just like own stuff and dive right in. And I think grit comes with it. I think in the end, like this is a very fast moving space and we want to just like be one of the, you know, like dominant forces and helping to provide like production quality outline applications. Yeah. [00:18:11]</p><p><strong>Swyx</strong>: I promise we'll get to more technical questions, but I also want to impress on the audience that this is a very conscious and intentional company building. And since your fundraising post, which was in June, and now it's September, so it's been about three months, you've actually gained 50% in terms of stars and followers. You've 3x'd your download count to 600,000 a month and your discord membership has reached 10,000. So like a lot of ongoing growth. [00:18:37]</p><p><strong>Jerry</strong>: Yeah, definitely. And obviously there's a lot of room to expand there too. And so open source growth is going to continue to be one of our core goals because in the end it's just like, we want this thing to be, well, one big, right? We all have like big ambitions, but to just like really provide value to developers and helping them in prototyping and also productionization of their apps. And I think it turns out we're in the fortunate circumstance where a lot of different companies and individuals, right, are in that phase of like, you know, maybe they've hacked around on some initial LLM applications, but they're also looking to, you know, start to think about what are the production grade challenges necessary to actually, that to solve, to actually make this thing robust and reliable in the real world. And so we want to basically provide the tooling to do that. And to do that, we need to both spread awareness and education of a lot of the key practices of what's going on. And so a lot of this is going to be continued growth, expansion, education, and we do prioritize that very heavily. [00:19:30]</p><p><strong>Alessio</strong>: Let's dive into some of the questions you were asking yourself initially around fine tuning and RAG , how these things play together. You mentioned context. What is the minimum viable context for RAG ? So what's like a context window too small? And at the same time, maybe what's like a maximum context window? We talked before about the LLMs are U-shaped reasoners. So as the context got larger, like it really only focuses on the end and the start of the prompt and then it kind of peters down. Any learnings, any kind of like tips you want to give people as they think about it? [00:20:06]</p><p><strong>Jerry</strong>: So this is a great question. And part of what I wanted to talk about a conceptual level, especially with the idea of like thinking about what is the minimum context? Like, okay, what if the minimum context was like 10 tokens versus like, you know, 2k tokens versus like a million tokens. Right. Like, and what does that really give you? And what are the limitations if it's like 10 tokens? It's kind of like, um, like eight bit, 16 bit games, right? Like back in the day, like if you play Mario and you have like the initial Mario where the graphics were very blocky and now obviously it's like full HD, 3d, just the resolution of the context and the output will change depending on how much context you can actually fit in. So the way I kind of think about this from a more principled manner is like you have like, there's this concept of like information capacity, just this idea of like entropy, like given any fixed amount of like storage space, like how much information can you actually compact in there? And so basically a context window length is just like some fixed amount of storage space, right? And so there's some theoretical limit to the maximum amount of information you can compact until like a 4,000 token storage space. And what does that storage space use for these days with LLMs? For inputs and also outputs. And so this really controls the maximum amount of information you can feed in terms of the prompt plus the granularity of the output. If you had an infinite context window, you're going to have an infinitely detailed response and also infinitely detailed memory. But if you don't, you can only kind of represent stuff in more quantized bits, right? And so the smaller the context window, just generally speaking, the less details and maybe the less, um, and for like specific, precise information, you're going to be able to surface any given point in time. [00:21:34]</p><p><strong>Alessio</strong>: So when you have short context, is the answer just like get a better model or is the answer maybe, Hey, there needs to be a balance between fine tuning and RAG to make sure you're going to like leverage the context, but at the same time, don't keep it too low resolution? [00:21:48]</p><p><strong>Jerry</strong>: Yeah, yeah. Well, there's probably some minimum threat, like I don't think anyone wants to work with like a 10. I mean, that's just a thought exercise anyways, a 10 token context window. I think nowadays the modern context window is like 2k, 4k is enough for just like doing some sort of retrieval on granular context and be able to synthesize information. I think for most intents and purposes, that level of resolution is probably fine for most people for most use cases. I think the question there is just like, um, the limitations actually more on, okay, if you're going to actually combine this thing with some sort of retrieval data structure mechanism, there's just limitations on the retrieval side because maybe you're not actually fetching the most relevant context to actually answer this question, right? Like, yes, like given the right context, 4,000 tokens is enough. But if you're just doing like top-k similarity, like you might not be able to be fetching the right information from the documents. [00:22:34]</p><p><strong>Alessio</strong>: So how should people think about when to stick with RAG versus when to even entertain and also in terms of what's like the threshold of data that you need to actually worry about fine tuning versus like just stick with rag? Obviously you're biased because you're building a RAG company, but no, no, actually, um, I [00:22:52]</p><p><strong>Jerry</strong>: think I have like a few hot takes in here, some of which sound like a little bit contradictory or what we're actually building. And I think to be honest, I don't think anyone knows the right answer. I think this is the truth. [00:23:01]</p><p><strong>Alessio</strong>: Yeah, exactly. [00:23:01]</p><p><strong>Jerry</strong>: This is just like thought exercise towards like understanding the truth. [00:23:04]</p><p><strong>Alessio</strong>: Right. [00:23:04]</p><p><strong>Jerry</strong>: So, okay. [00:23:05]</p><p><strong>Alessio</strong>: I have a few hot takes. [00:23:05]</p><p><strong>Jerry</strong>: One is like RAG is basically just, just a hack, but it turns out it's a very good hack because what is RAG rag is you keep the model fixed and you just figure out a good way to like stuff stuff into the prompt of the language model and everything that we're doing nowadays in terms of like stuffing stuff into the prompt is just algorithmic. We're just figuring out nice algorithms to, to like retrieve right information with top case similarity, do some sort of like, uh, you know, hybrid search, some sort of like a chain of thought decomp and then just like stuff stuff into a prompt. So it's all like algorithmic and it's more like just software engineering to try to make the most out of these like existing APIs. The reason I say it's a hack is just like from a pure like optimization standpoint. If you think about this from like the machine learning lens, unless the software engineering lens, there's pieces in here that are going to be like suboptimal, right? Like, like the thing about machine learning is when you optimize like some system that can be optimized within machine learning, like the set of parameters, you're really like changing like the entire system's weights to try to optimize the subjective function. [00:24:02]</p><p><strong>Jerry</strong>: And if you just cobble a bunch of stuff together, you can't really optimize the pieces are inefficient, right? And so like a retrieval interface, like doing top cam batting lookup, that part is  inefficient. [00:24:13]</p><p><strong>Jerry</strong>: If you, for instance, because there might be potentially a better, more learned retrieval algorithm, that's better. If you know, you do stuff like some sort of, I know nowadays there's this concept of how do you do like short-term and long-term memory represent stuff in some sort of vector embedding, do trunk sizes, all that stuff. It's all just like decisions that you make that aren't really optimized and it's not really automatically learned. It's more just things that you set beforehand to actually feed into the system. So I do think like there is a lot of room to actually optimize the performance of an entire LLM system, potentially in a more like machine learning based way. Right. [00:24:48]</p><p><strong>Jerry</strong>: And I will leave room for that. And this is also why I think like in the long term, I do think fine tuning will probably have like greater importance. And just like there will probably be new architectures invented that where you can actually kind of like include a lot of this under the black box, as opposed to having like hobbling together a bunch of components outside the black box. That said, just very practically given the current state of things, like even if I said RAG is a hack, it's a very good hack and it's also very easy to use. Right. [00:25:16]</p><p><strong>Jerry</strong>: And so just like for kind of like the AI engineer persona, which to be fair is kind of one of the reasons generative AI has gotten so big is because it's way more accessible for everybody to get into, as opposed to just like traditional machine learning, it tends to be good enough. [00:25:30]</p><p><strong>Jerry</strong>: Right. And if we can basically provide these existing techniques to help people really optimize how to use existing systems without having to really deeply understand machine learning, I still think that's a huge value add. And so there's very much like a UX and ease of use problem here, which is just like RAG is way easier to onboard and use. And that's probably like the primary reason why everyone should do RAG instead of fine tuning to begin with. If you think about like the 80-20 rule, like RAG very much fits within that and fine tuning doesn't really right now. And then I'm just kind of like leaving room for the future that, you know, like in the end, fine tuning can probably take over some of the aspects of like what RAG does. [00:26:04]</p><p><strong>Swyx</strong>: I don't know if this is mentioned in your explainability also allows for sourcing. And at the end of the day, like to increase trust that we have to source documents. Yeah. [00:26:14]</p><p><strong>Jerry</strong>: So, so I think what RAG does is it increases like transparency, visibility into the actual documents, right. [00:26:19]</p><p><strong>Jerry</strong>: That are getting fed into their context. [00:26:21]</p><p><strong>Swyx</strong>: Here's where they got it from. [00:26:22]</p><p><strong>Alessio</strong>: Exactly. [00:26:22]</p><p><strong>Jerry</strong>: That's definitely an advantage. I think the other piece that I think is an advantage, and I think that's something that someone actually brought up is just you can do access control with, with RAG . If you have an external storage system, you can't really do that with, with large language models. [00:26:35]</p><p><strong>Jerry</strong>: It's just like gate information to the neural net weights, like depending on the type of user for the first point, you could technically, you could technically have the  language model. [00:26:45]</p><p><strong>Jerry</strong>: Like if it memorized enough information, just like a site sources, but there's a question of just trust whether or not you're actually, yeah, well, but like it makes it up right now because it's like not good enough, but imagine a world where it is good enough and it does give accurate citations. </p><p><strong>Swyx</strong>: No, I think to establish trust, you just need a direct connection.So it's, it's kind of weird.  It's, it's this melding of deep learning systems versus very traditional information retrieval. Yeah, exactly. [00:27:11]</p><p><strong>Jerry</strong>: Well, so, so I think, I mean, I kind of think about it as analogous to like humans, right? [00:27:15]</p><p><strong>Jerry</strong>: Like, uh, we as humans, obviously we use the internet, we use tools. Uh, these tools have API interfaces are well-defined. Um, and obviously we're not like the tools aren't part of us. And so we're not like back propping or optimizing over these tools. And so when you think about like RAG , it's basically, um, LLM is learning how to use like a vector database to look up information that it doesn't know. And so then there's just a question of like how much information is inherent within the network itself and how much does it need to do some sort of like tool used to look up  stuff that it doesn't know. [00:27:42]</p><p><strong>Jerry</strong>: And I do think there'll probably be more and more of that interplay as time goes on. [00:27:46]</p><p><strong>Swyx</strong>: Yeah. Some followups on discussions that we've had, you know, we discussed fine tuning a bit and what's your current take on whether you can, you can fine tune new knowledge into LLMs. [00:27:55]</p><p><strong>Jerry</strong>: That's one of those things where I think longterm you definitely can. I think some people say you can't, I disagree. I think you definitely can. Just right now I haven't gotten it to work yet. So, so I think like we've tried, yeah, well, um, not in a very principled way, right? Like this is something that requires like an actual research scientist and not someone that has like, you know, an hour or two per night to actually look at this. [00:28:12]</p><p><strong>Swyx</strong>: Like I, you were a research scientist at Uber. I mean, it's like full-time, full-time working. [00:28:16]</p><p><strong>Jerry</strong>: So, so I think, um, what I specifically concretely did was I took OpenAI's fine tuning endpoints and then tried to, you know, it's in like a chat message interface. And so there's like, um, input question, like a user assistant message format. And so what I did was I tried to take just some piece of text and have the LLM memorize it by just asking it a bunch of questions about the text. So given a bunch of context, I would generate some questions and then generate some response and just fine tune over the question responses. That hasn't really worked super well, but that's also because I'm, I'm just like trying to like use OpenAI's endpoints as is. If you just think about like traditional, like how you train a Transformers model, there's kind of like the, uh, instruction, like fine tuning aspect, right? You like ask it stuff when guided with correct responses, but then there's also just like, um, next token production. And that's something that you can't really do with the OpenAI API, but you can do with, if you just train it yourself and that's probably possible if you just like train it over some corpus of data. I think Shashira from Berkeley said like, you know, when they trained Gorilla, they were like, Oh, you know, this, a lot of these LLMs are actually pretty good at memorizing information. Um, just the way the API interface is exposed is just no one knows how to use them right [00:29:22]</p><p><strong>Alessio</strong>: now. Right. [00:29:22]</p><p><strong>Jerry</strong>: And so, so I think that's probably one of the issues. [00:29:24]</p><p><strong>Swyx</strong>: Just to clue people in who haven't read the paper, Gorilla is the one where they train to use specific APIs. [00:29:30]</p><p><strong>Jerry</strong>: Yeah, I think this was on the Gorilla paper. Like the, the model itself could, uh, try to learn some prior over the data to decide like what tool to pick. But there's also, it's also augmented with retrieval that helps supplement it in case like the, the, the, um, prior doesn't actually work. [00:29:45]</p><p><strong>Swyx</strong>: Is that something that you'd be interested in supporting? [00:29:48]</p><p><strong>Jerry</strong>: I mean, I think in the longterm, like if like, this is kind of how fine tuning, like RAG evolves. Like I do think there'll be some aspect where fine tuning will probably memorize some high level concepts of knowledge, but then like RAG will just be there to supplement like aspects of that, that aren't work that don't, that, that it doesn't know. </p><p><strong>Jerry</strong>: Um, the way I think about this is kind of like, obviously RAG is the default way, like to be clear, RAG right now is the default way to actually augment stuff with knowledge. I think it's just an open question of how much the LM can actually internalize both high level concepts, but also details as you can like train stuff over it. And coming from an ML background, there is a certain beauty and just baking everything into some training process of a language model. Like if you just take raw chat, GPT or chat, GPT code interpreter, right? Like GPT four, it's not like you do RAG with it. You just ask it questions about like, Hey, how do I like to find a pedantic model in Python? And I'm like, can you give me an example? Can you visualize a graph? It just does it right. Like, and we'll run it through code interpreters as a tool, but that's not like a source for knowledge. [00:30:46]</p><p><strong>Jerry</strong>: It's just an execution environment. And so there is some beauty in just like having the model itself, like just, you know, instead of you kind of defining the algorithm for what the data structure should look like the model just learns it under the hood. That said, I think the reason it's not a thing right now is just like, no one knows how to do it. [00:31:01]</p><p><strong>Jerry</strong>: It probably costs too much money. And then also like the API interfaces and just like the actual ability to kind of evaluate and improve on performance, like isn't known to most people. [00:31:12]</p><p><strong>Alessio</strong>: Yeah. [00:31:12]</p><p><strong>Swyx</strong>: It also would be better with browsing. [00:31:14]</p><p><strong>Alessio</strong>: Yeah. [00:31:16]</p><p><strong>Swyx</strong>: I wonder when they're going to put that back. [00:31:18]</p><p><strong>Alessio</strong>: Okay. Yeah. [00:31:19]</p><p><strong>Swyx</strong>: So, and then one more follow up before we go into RAG for AI engineers is on your brief mentioned about security or off. How many of your, the people that you talk to, you know, you talk to a lot of people putting LlamaIndex into production. How many people actually are there versus just like, let's just dump a whole company notion into this thing. [00:31:36]</p><p><strong>Jerry</strong>: Wait, are you talking about from like the security off standpoint? [00:31:39]</p><p><strong>Alessio</strong>: Yeah. [00:31:39]</p><p><strong>Swyx</strong>: How big a need is that? Because I, I talked to some people who are thinking about building tools in that domain, but I don't know if people want it. [00:31:47]</p><p><strong>Jerry</strong>: I mean, I think bigger companies, like just bigger companies, like banks, consulting firms, like they all want this requirement, right? The way they're using LlamaIndex is not with this, obviously. Cause I don't think we have support for like access control or author that have stuff like on a hood. [00:32:02]</p><p><strong>Jerry</strong>: Cause we're more just like an orchestration framework. And so the way they build these initial apps is more kind of like prototype. Like, let's kind of, yeah. Like, you know, use some publicly available data. That's not super sensitive. Let's like, you know, assume that every user is going to be able to have access to the same amount of knowledge, those types of things. I think users have asked for it, but I don't think that's like a P zero. Like I think the P zero is more on like, can we get this thing working before we expand this to like more users within the work? [00:32:25]</p><p><strong>Alessio</strong>: There's a bunch of pieces to rag. Obviously it's not a, just an acronym. And you two recently, you think every AI engineer should build the front scratch at least once. Why is that? I think so. [00:32:37]</p><p><strong>Jerry</strong>: I'm actually kind of curious to hear your thoughts about this. Um, but this kind of relates to the initial like AI engineering posts that you put out and then also just like the role of an AI engineer and the skills that they're going to have to learn to truly succeed because there's an entire On one end, you have people that don't really, uh, like understand the fundamentals and just want to use this to like cobble something together to build something. And I think there is a beauty in that for what it's worth. Like, it's just one of those things. And Gen AI has made it so that you can just use these models in inference only mode, call something together, use it, power your app experiences, but on the other end, what we're increasingly seeing is that like more and more developers building with these apps start running into honestly, like pretty similar issues that like we'll play just a standard engineer building like a classifier model, which is just like accuracy problems, like, and hallucinations, basically just an accuracy problem,  right? [00:33:24]</p><p>Like it's not giving you the right results. So what do you do? You have to iterate on the model itself. You have to figure out what parameters you tweak. You have to gain some intuition about this entire process. That workflow is pretty similar, honestly, like even if you're not training the model to just like tuning a ML model with like hyper parameters and learning like proper ML practices of like, okay, how do I have like define a good evaluation benchmark? How do I define like the right set of metrics to do to use, right? How do I actually iterate and improve the performance of this pipeline for [00:33:52]</p><p><strong>Alessio</strong>: production? What tools do I use? [00:33:53]</p><p><strong>Jerry</strong>: Right? Like every ML engineer use like some form of weights and biases, tensor boards, or like some other experimentation tracking tool. What tools should I use to actually help build like LLM applications and optimize it for production? There's like a certain amount of just like LLM ops, like tooling and concepts and just like practices that people will kind of have to internalize if they want to optimize these. And so I think that the reason I think being able to build like RAG from scratch is important is it really gives you a sense of like how things are working to get, help you build intuition about like what parameters are within a RAG system and which ones actually tweak to make them better. Cause otherwise I think that one of the advantages of the LlamaIndex quick start is it's three lines of code. The downside of that is you have zero visibility into what's actually going on [00:34:37]</p><p><strong>Alessio</strong>: under the hood. [00:34:37]</p><p><strong>Jerry</strong>: And I think there's something that we've kind of been thinking about for a while and I'm like, okay, let's just release like a new tutorial series. That's just like, we're in set, not no three lines of code. We're just going to go in and actually show you how the thing actually works on [00:34:47]</p><p><strong>Alessio</strong>: the hood. Right. [00:34:47]</p><p><strong>Jerry</strong>: And so I like, does everybody need this? Like probably not as for some people, the three lines of code might work, but I think increasingly, like honestly, 90% of the users I talked to have questions about how to improve the performance of their app. And so just like, given this, it's just like one of those things that's like better for the understanding. [00:35:03]</p><p><strong>Alessio</strong>: Yeah. [00:35:03]</p><p><strong>Swyx</strong>: I'd say it is one of the most useful tools of any sort of developer education toolkit to write things yourself from scratch. So Kelsey Hightower famously wrote Kubernetes the hard way, which is don't use Kubernetes. Here's everything that you would have to do by yourself. And you should be able to put all these things together yourself to understand the value of Kubernetes. And the same thing for LLlamaIndex. I've done, I was the guy who did the same for React. And it's a pretty good exercise for you to just fully understand everything that's going on under the hood. And I was actually going to suggest while in one of the previous conversations, there's all these like hyperparameters, like the size of the chunks and all that. And I was thinking like, what would hyperparameter optimization for RAG look [00:35:44]</p><p><strong>Alessio</strong>: like? [00:35:44]</p><p><strong>Jerry</strong>: Yeah, definitely. I mean, so absolutely. I think that's going to be an increasing thing. I think that's something we're kind of looking at because like, I think someone [00:35:52]</p><p><strong>Swyx</strong>: should just put, do like some large scale study and then just ablate everything. And just you, you tell us. [00:35:57]</p><p><strong>Jerry</strong>: I think it's going to be hard to find a universal default that works for [00:36:00]</p><p><strong>Alessio</strong>: everybody. [00:36:00]</p><p><strong>Jerry</strong>: I think it's going to be somewhat, I do think it's going to be somewhat like dependent on the data and use case. I think if there was a universal default, that would be amazing. But I think increasingly we found, you know, people are just defining their own like custom parsers for like PDFs, markdown files for like, you know, SEC filings versus like Slack conversations. And then like the use case too, like, do you want like a summarization, like the granularity of the response? Like it really affects the parameters that you want to pick. I do like the idea of hyperparameter optimization though, but it's kind of like one of those things where you are kind of like training the model basically kind of on your own data domain. [00:36:36]</p><p><strong>Alessio</strong>: Yeah. [00:36:36]</p><p><strong>Swyx</strong>: You mentioned custom parsers. You've designed LlamaIndex, maybe we can talk about like the surface area of the [00:36:41]</p><p><strong>Alessio</strong>: framework. [00:36:41]</p><p><strong>Swyx</strong>: You designed LlamaIndex in a way that it's more modular, like you mentioned. How would you describe the different components and what's customizable in each? [00:36:50]</p><p><strong>Jerry</strong>: Yeah, I think they're all customizable. And I think that there is a certain burden on us to make that more clear through the [00:36:57]</p><p><strong>Alessio</strong>: docs. [00:36:57]</p><p><strong>Jerry</strong>: Well, number four is customization tutorials. [00:36:59]</p><p><strong>Swyx</strong>: Yeah, yeah. [00:37:00]</p><p><strong>Jerry</strong>: But I think like just in general, I think we do try to make it so that you can plug in the out of the box stuff. But if you want to customize more lower level components, like we definitely encourage you to do that and plug it into the rest of our abstractions. So let me just walk through like maybe some of the basic components of LlamaIndex. There's data loaders. You can load data from different data sources. We have Llama Hub, which you guys brought up, which is, you know, a collection of different data loaders of like unstructured and unstructured data, like PDFs, file types, like Slack, Notion, all that stuff. Now you load in this data. We have a bunch of like parsers and transformers. You can split the text. You can add metadata to the text and then basically figure out a way to load it into like a vector store. So, I mean, you worked at like Airbrite, right? It's kind of like there is some aspect like E and T, right? And in terms of like transforming this data and then the L, right, loading it into some storage abstraction, we have like a bunch of integrations with different document storage systems. [00:37:49]</p><p><strong>Alessio</strong>: So that's data. [00:37:50]</p><p><strong>Jerry</strong>: And then the second piece really is about like, how do you retrieve this data? How do you like synthesize this data and how do you like do some sort of higher level reasoning over this data? So retrieval is one of the core abstractions that we have. We do encourage people to like customize, define your own retrievers, that section on kind of like how do you define your own, like custom retriever, but also we have like out of the box ones. The retrieval algorithm kind of depends on how you structure the data, obviously. Like if you just flat index everything with like chunks with like embeddings, then you can really only do like top K like lookup plus maybe like keyword search or something. But if you can index it in some sort of like hierarchy, like defined relationships, you can do more interesting things like actually traverse relationships between nodes. Then after you have this data, how do you like synthesize the data? [00:38:32]</p><p><strong>Alessio</strong>: Right. [00:38:32]</p><p><strong>Jerry</strong>: Um, and, and this is the part where you feed it into the language model. There's some response abstraction that can abstract away over like long contacts to actually still give you a response, even if the context overflows a context window. And then there's kind of these like higher level, like reasoning primitives that I'm going to define broadly. And I'm just going to call them in some general bucket of like agents, even though everybody has different definitions of agents, but you're the first to data agents, [00:38:56]</p><p><strong>Swyx</strong>: which I was very excited. [00:38:57]</p><p><strong>Alessio</strong>: Yeah. [00:38:57]</p><p><strong>Jerry</strong>: We, we kind of like coin, coin that term. And the way we, we thought about it was, you know, we wanted to think about how to use agents for, uh, like data workflows basically. And, and so what are the reasoning primitives that you want to do? So the most simple reasoning primitive you can do is some sort of routing module. It's a classifier, like given a query, just make some automated decision on what choice to pick, right? You could use LLMs. You don't have to use LLMs. You could just try and classifier basically. That's something that we might actually explore. And then the next piece is, okay, what are some higher level things? You can have the LLM like define like a query plan, right. To actually execute over the data. You can do some sort of while loop, right? That's basically what an agent loop is, which is like react a chain of thought, like the open AI function calling, like while loop to try to like take a question and try to break it down into some, some, uh, series of steps to actually try to execute to get back a response. And so there's a range and complexity from like simple reasoning primitives to more advanced ones. The way we kind of think about it is like, which ones should we implement and how do [00:39:50]</p><p><strong>Alessio</strong>: they work? [00:39:50]</p><p><strong>Jerry</strong>: Well, like, do they work well over like the types of like data tasks that we give them? [00:39:54]</p><p><strong>Alessio</strong>: How do you think about optimizing each piece? So take, um, embedding models is one piece of it. You offer fine tuning, embedding models. And I saw it was like fine tuning gives you like 5, 10% increase. What's kind of like the Delta left on the embedding side? Do you think we can get models that are like a lot better? Do you think like that's one piece where people should really not spend too much time? [00:40:16]</p><p><strong>Jerry</strong>: I just think it's, it's not the only parameter. Cause I think in the end, if you think about everything that goes into retrieval, the chunking algorithm, um, how you define like metadata will bias your embedding representations. Then there's the actual embedding model itself, which is something that you can try optimizing. And then there's like the retrieval algorithm. Are you going to just do top K? Are you going to do like hybrid search? Are you going to do auto retrieval? Like there's a bunch of parameters. And so I do think it's something everybody should try. I think by default we use like OpenAI's embedding model. A lot of people these days use like sentence transformers because it's, it's just like free open source and you can actually optimize, directly optimize it. This is an active area of exploration. I do think one of our goals is it should ideally be relatively free for every developer to just run some fine tuning process over their data to squeeze out some more points and performance. And if it's that relatively free and there's no downsides, everybody should basically do [00:41:04]</p><p><strong>Alessio</strong>: it. [00:41:04]</p><p><strong>Jerry</strong>: There's just some complexities, right? In terms of optimizing your embedding model, especially in a production grade data pipeline. If you actually fine tune the embedding model and the embedding space changes, you're going to have to reindex all your documents. And for a lot of people, that's not feasible. And so I think like Joe from Vespa on our webinars, like there's this idea that depending on if you're just using like document and query embeddings, you could keep the document embeddings frozen and just train a linear transform on the query or, or any sort of transform on the query, right? So therefore it's just a query side transformation instead of actually having to reindex all the document embeddings. That's pretty smart. We weren't able to get like huge performance gains there, but it does like improve performance a little bit. And that's something that basically, you know, everybody should be able to kick off. You can actually do that on LLlamaIndex too. [00:41:45]</p><p><strong>Swyx</strong>: OpenAIO has a cookbook on adding bias to the embeddings too, right? [00:41:49]</p><p><strong>Alessio</strong>: Yeah. [00:41:49]</p><p><strong>Jerry</strong>: There's just like different parameters that you can, you can try adding to try to like optimize the retrieval process. And the idea is just like, okay, by default you have all this text. It kind of lives in some latent space, right? [00:42:01]</p><p><strong>Swyx</strong>: Yeah. Shut out, shut out latent space. You should take a drink every time. [00:42:05]</p><p><strong>Jerry</strong>: But it lives in some latent space. But like depending on the type, specific types of questions that the user might want to ask, the latent space might not be optimized to actually retrieve the relevant piece of context that the user want to ask. So can you shift the embedding points a little bit, right? And how do we do that? Basically, that's really a key question here. So optimizing the embedding model, even changing the way you like chunk things, these all shift the embeddings. [00:42:26]</p><p><strong>Alessio</strong>: So the retrieval is interesting. I got a bunch of startup pitches that are like, like ragged school, but like there's a lot of stuff in terms of ranking that could be better. There's a lot of stuff in terms of sun setting data. Once it starts to become stale, that could be better. Are you going to move into that part too? So like you have SEC Insights as one of kind of like your demos. And that's like a great example of, Hey, I don't want to embed all the historical documents because a lot of them are outdated and I don't want them to be in the context. [00:42:55]</p><p><strong>Jerry</strong>: What's that problem space? [00:42:57]</p><p><strong>Alessio</strong>: Like how much of it are you going to also help with and versus how much you expect others to take care of? [00:43:03]</p><p><strong>Jerry</strong>: Yeah, I'm happy to talk about SEC Insights in just a bit. I think more broadly about the like overall retrieval space. We're very interested in it because a lot of these are very practical problems that [00:43:11]</p><p><strong>Alessio</strong>: people have asked us. [00:43:11]</p><p><strong>Jerry</strong>: And so the idea of outdated data, I think, how do you like deprecate or time wait data and do that in a reliable manner, I guess. So you don't just like set some parameter and all of a sudden that affects your, all your retrieval items, like is pretty important because people have started bringing [00:43:25]</p><p><strong>Alessio</strong>: that up. [00:43:25]</p><p><strong>Jerry</strong>: Like I have a bunch of duplicate documents, things get out of date. How do I like sunset documents? And then remind me, what was the, what was the first thing you said? Cause I think there was, there was something like the ranking ranking, right? [00:43:35]</p><p><strong>Alessio</strong>: Yeah. [00:43:35]</p><p><strong>Jerry</strong>: So I think this space is not new. I think everybody who is new to this space starts learning some basic concepts of information retrieval, which to be fair has been around for quite a bit. But our goal is to kind of like take some of like just general ranking and information retrieval concepts. So by encoding, like crossing coding, right? Like we're based models versus like kind of keyword based search. How do you actually evaluate retrieval? These things start becoming relevant. And so I think for us, like rather than inventing like new retriever techniques for the sake of like just inventing better ranking, we want to take existing ranking techniques and kind of like package it in a way that's like intuitive and easy for people to understand. That said, I think there are interesting and new retrieval techniques that are kind of in place that can be done when you tie it into some downstream rack system. The reason for this is just like, if you think about the idea of like chunking text, right? Like that just really wasn't a thing, or at least for this specific purpose, like the reason chunking is a thing in RAG right now is because like you want to fit within the context bundle of an LLM, right? Like why do you want to chunk a document? That just was less of a thing. I think back then, if you wanted to like transform a document, it was more for like structured data extraction or something in the past. And so there's kind of like certain new concepts that you got to play with that you can use to invent kind of more interesting retrieval techniques. Another example here is actually LLM based reasoning, like LLM based chain of thought reasoning. You can take a question, break it down into smaller components and use that to actually send to your retrieval system. And that gives you better results. And it's kind of like sending the full question to a retrieval system. That also wasn't really a thing back then, but then you can kind of figure out an interesting way to like blending old and the new, right? With LLMs and data. [00:45:13]</p><p><strong>Swyx</strong>: There's a lot of ideas that you come across. Do you have a store of them? [00:45:17]</p><p><strong>Jerry</strong>: Yeah, I think I, sometimes I get like inspiration. There's like some problem statement and I'm just like, oh, it's like, following you is [00:45:23]</p><p><strong>Swyx</strong>: very hard because it's just a lot of homework. [00:45:25]</p><p><strong>Jerry</strong>: So I think I've, I've started to like step on the brakes just a little bit. Cause then I start, no, no, no. Well, the, the reason is just like, okay, if I just have invent like a hundred more retrieval techniques, like, like sure. But like, how do people know which one is good and which one's like bad. [00:45:41]</p><p><strong>Alessio</strong>: Right. [00:45:41]</p><p><strong>Jerry</strong>: And so have a librarian, right? [00:45:42]</p><p><strong>Swyx</strong>: Like it's going to catalog it and you're going to need some like benchmarks. [00:45:45]</p><p><strong>Jerry</strong>: And so I think that's probably the focus for the next, next few weeks is actually like properly kind of like having an understanding of like, oh, you know, when should you do this or like, what does this actually work well? [00:45:54]</p><p><strong>Alessio</strong>: Yeah. [00:45:54]</p><p><strong>Swyx</strong>: Some kind of like a, maybe like a flow chart, decision tree type of thing. Yeah, exactly. When this do that, you know, something like that, that would be really helpful for me. [00:46:02]</p><p><strong>Alessio</strong>: Thank you. [00:46:02]</p><p><strong>Swyx</strong>: It seems like your most successful side project. Yeah. What is SEC Insights for our listeners? [00:46:07]</p><p><strong>Jerry</strong>: Um, our SEC Insights is a full stack LLM chatbot application, um, that does. Analysis of your sec 10 K and 10 Q filings. And so the goal for building this project is really twofold. The reason we started building this was one, it was a great way to dog food, the production readiness for our library. We actually ended up like adding a bunch of stuff and fixing a ton of bugs because of this. And I think it was great because like, you know, thinking about how we handle like callbacks streaming, actually generating like reliable sub responses and bubbling up sources, citations. These are all things that like, you know, if you're just building the library in isolation, you don't really think about it. But if you're trying to tie this into a downstream application, like it really starts mattering for your error messages. When you talk about bubbling up stuff for like sources, like if you go into SEC Insights and you type something, you can actually see the highlights in the right side. That was something that like took a little bit of like, um, understanding to figure out how to build wall. And so it was great for dog fooding improvement of the library itself. And then as we're building the app, um, the second thing was we're starting to talk to users and just like trying to showcase like kind of, uh, bigger companies, like the potential of LLM index as a framework, because these days obviously building a chatbot, right. With Streamlight or something, it'll take you like 30 minutes or an hour. Like there's plenty of templates out there on LLM index, like train, like you can just build a chatbot, but how do you build something that kind of like satisfies some of these, uh, this like criteria of surfacing, like citations, being transparent, seeing like, uh, having a good UX, um, and then also being able to handle different types of questions, right? Like more complex questions that compare different documents. That's something that I think people are still trying to explore. And so what we did was like, we showed, well, first like organizations, the possibilities of like what you can do when you actually build something like this. And then after like, you know, we kind of like stealth launched this for fun, just as a separate project, uh, just to see if we could get feedback from users who are using this world to see like, you know, how we can improve stuff. And then we were thought, we thought like, ah, you know, we built this, right? Obviously we're not going to sell like a financial app. Like that's not really our, in our wheelhouse, but we're just going to open source the entire thing. And so that now is basically just like a really nice, like full stack app template you can use and customize on your own, right. To build your own chatbot, whether it is a really financial documents or like other types of documents. Um, and it provides like a nice template for basically anybody to kind of like go in and get started. There's certain components though, that like aren't released yet that we're going to going to, and then next few weeks, like one is just like kind of more detailed guides on like different modular components within it. So if you're like a full stack developer, you can go in and actually take the pieces that you want and actually kind of build your own custom flows. The second piece is like, take, there's like certain components in there that might not be directly related to the LLM app that would be nice to just like have people use, uh, an example is the PDF viewer, like the PDF viewer with like citations. I think we're just going to give that right. So, you know, you could be using any library you want, but then you can just, you know, just drop in a PDF viewer. [00:48:53]</p><p><strong>Alessio</strong>: Right. [00:48:53]</p><p><strong>Jerry</strong>: So that it's just like a fun little module that you can do. [00:48:55]</p><p><strong>Swyx</strong>: Nice. That's really good community service right there. I want to talk a little bit about your cloud offering, because you mentioned, I forget the name that you had for it. [00:49:04]</p><p><strong>Alessio</strong>: Enterprise something. [00:49:04]</p><p><strong>Jerry</strong>: Well, one, we haven't come up with a name. Uh, we're kind of calling it LLM index platform, platform LLM index enterprise. I'm open to suggestions here. Um, and the second thing is I don't actually know how much I can, I can share right now because it's mostly kind of like, uh, we, we, yeah, exactly. [00:49:20]</p><p><strong>Swyx</strong>: To the extent that you can talk about LLM index as a business. Um, always just want to give people in the mind, like, Hey, like you sell things too, you know what I mean? [00:49:28]</p><p><strong>Jerry</strong>: Yeah, a hundred percent. So I think the high level of what I can probably say is just like, I think we're looking at ways of like actively kind of complimenting the developer experience, like building LLM index. We've always been very focused on stuff around like plugging in your data into the language model. And so can we build tools that help like augment that experience beyond the open [00:49:47]</p><p><strong>Alessio</strong>: source library? Right. [00:49:48]</p><p><strong>Jerry</strong>: And so I think what we're going to do is like make a build an experience where it's very seamless to transition from the open source library with like a one line toggle, you can basically get this like complimentary service and then figure out a way to like monetize in a bit. I think where our revenue focus this year is less emphasized. Like it's more just about like, can we build some manage offering that like provides complimentary value to what the open source library provides? [00:50:09]</p><p><strong>Alessio</strong>: Yeah. [00:50:10]</p><p><strong>Swyx</strong>: I think it's the classic thing about all open source is you want to start building the most popular open source projects in your category to own that category. You're going to make it very easy to host. Therefore you're just built your biggest competitor, which is you. [00:50:22]</p><p><strong>Jerry</strong>: I think it will be like complimentary. Cause I think it will be like, you know, use the open source library and then you have a toggle and all of a sudden, you know, you can see this basically like a pipeline ish thing pop up and then it will be able to kind of like, you'll have a UI. There'll be some enterprise guarantees and the end goal would be to help you build like a production RAG app more easily. [00:50:42]</p><p><strong>Alessio</strong>: Data loaders. There's a lot of them. What are maybe some of the most popular, maybe under, not underrated, but like underexpected, you know, and how has the open source side of it helped with like getting a lot more connectors, you only have six people on the team today, so you couldn't have done it all yourself. [00:51:00]</p><p><strong>Jerry</strong>: Yeah. I think the nice thing about like Walmart hub itself, it's supposed to be a community driven hub. Um, and so actually the bulk of the peers are completely community contributed. Um, and so we haven't written that many like first party connectors actually for this, it's more just like a kind of encouraging people to contribute to the community in terms of the most popular tools, uh, or the data loaders. I think we have Google analytics on this and I forgot the specifics. It's some mix of like the PDF loaders. We have like 10 of them, but there's some subset of them that are popular. And then there's Google, like I think Gmail and like G drive. Um, and then I think maybe it's like one of Slack or notion. One thing I will say though, uh, and I think like Swix might probably knows this better than I do, given that you were, she used to work at air bite. It's very hard to build, like, especially for full on service, like notion Slack or like Salesforce to build like a really, really high quality loader that really extracts all the information that people want. [00:51:51]</p><p><strong>Alessio</strong>: Right. [00:51:51]</p><p><strong>Jerry</strong>: And so I think the thing is when people start out, like they will probably use these loaders and it's a great tool to get started. And for a lot of people, it's like good enough. And they submit PRs if they want more additional features. But if you get to a point where you actually want to call like an API that hasn't been supported yet, or, you know, you want to load in stuff that like in metadata or something that hasn't been directly baked into the logic of a loader itself, people start adding up, like writing their own custom loaders. And that is a thing that we're seeing. That's something that we're okay with. [00:52:18]</p><p><strong>Alessio</strong>: Right. [00:52:18]</p><p><strong>Jerry</strong>: Cause like a lot of this is more just like community driven. And if you want to submit a PR to improve the existing one, you can, otherwise you can create your own custom ones. [00:52:24]</p><p><strong>Alessio</strong>: Yeah. [00:52:25]</p><p><strong>Swyx</strong>: And all that is custom loaders all supported within LLlamaIndex, or do you pair it with something else? [00:52:29]</p><p><strong>Jerry</strong>: Oh, it's just like, I mean, you just define your own subclass. I think, I think that's it. [00:52:33]</p><p><strong>Alessio</strong>: Yeah. Yeah. [00:52:33]</p><p><strong>Swyx</strong>: Cause typically in the data ecosystem with everybody, everybody has his own strategies with custom loaders, but also you could write your own with like Dagster or like Prefect or one of those tools. [00:52:43]</p><p><strong>Alessio</strong>: Yeah. [00:52:44]</p><p><strong>Jerry</strong>: Yeah, exactly. So I think for us, it's more, we just have a very flexible like document abstraction that you can fill in with any content that you want. [00:52:50]</p><p><strong>Swyx</strong>: Are people really dumping all their Gmail into these things? You said Gmail is number two. Uh, I'm not sure actually. I mean, that's these, you know, that's the most private data source. [00:52:59]</p><p><strong>Alessio</strong>: That's true. [00:53:00]</p><p><strong>Swyx</strong>: So I'm surprised that people are dumping too. I mean, I'm sure some, some people are, but like, I'm sure I'm surprised it's [00:53:06]</p><p><strong>Alessio</strong>: popular. [00:53:06]</p><p><strong>Swyx</strong>: Well, and then, so, uh, the LLM engine, uh, I assume OpenAI is going to be a majority. Is it an overwhelming majority? Uh, how, what's the market share between like OpenAI, Cohere, Anthropic, you know, whatever you're seeing. [00:53:21]</p><p><strong>Alessio</strong>: OpenSource too. [00:53:21]</p><p><strong>Jerry</strong>: Yeah, I think it's probably some, uh, OpenAI has a majority, but then like there's Anthropic and there's also, um, OpenSource. I think there is a lot of people trying out like Llama 2, um, and, and, um, some variant of like a top OpenSource model. [00:53:33]</p><p><strong>Swyx</strong>: Side note, any confusion there, Llama 2 versus Llama? [00:53:36]</p><p><strong>Jerry</strong>: Yeah, I think whenever I go to these talks, I always open it up with like, we started before it. Yeah, exactly. We start before meta, right? [00:53:43]</p><p><strong>Alessio</strong>: I want to point that out. [00:53:43]</p><p><strong>Jerry</strong>: Uh, but no, for us, we try to use it for like branding. We just add two llamas when we have like a Llama 2 integration instead of one llama. So I think a lot of people are trying out the popular OpenSource models. Uh, there's a lot of toolkits and OpenSource projects that allow you to self-host and deploy Llama 2 and like, oh, Llama is just a very recent example. I think that we, we added integration with, and so we just, uh, by virtue of having more of these services, I think more and more people are trying it out. [00:54:07]</p><p><strong>Swyx</strong>: Do you think there's, there's potential there? Is like, um, is that going to be an increasing trend? Like OpenSource? [00:54:12]</p><p><strong>Alessio</strong>: Yeah. [00:54:12]</p><p><strong>Jerry</strong>: Yeah, definitely. I think in general people hate monopolies. And so, um, like there's a, whenever like OpenAI has something really cool or like any, um, company has something really cool, even meta, like there's just going to be a huge competitive pressure from other people to do something that's more open and better. Um, and so I do think just market pressures will, will improve like OpenSource adoption. [00:54:32]</p><p><strong>Swyx</strong>: Last thing I'll say about this, which is just really like, it gets clicks. It's people like psychologically want that, but then at the end of the day, they want, they fall for brand name and popular and performance benchmarks. You know, at the end of the day, OpenAI still wins on that. I think that's true. [00:54:47]</p><p><strong>Jerry</strong>: But I, I just think like, unless you were like an active employee at OpenAI, right? Like all these research labs are putting out like ML, like PhDs or kind of like other companies too, that are investing a lot of dollars. Uh, there's going to be a lot of like competitive pressures developed, like better models. So is it going to be like all fully open source with like a permissive license? Like, I'm not completely sure, but like, there's just a lot of just incentive for people to develop their stuff here. [00:55:09]</p><p><strong>Swyx</strong>: Have you looked at like RAG specific models, like contextual? [00:55:12]</p><p><strong>Alessio</strong>: No. [00:55:13]</p><p><strong>Jerry</strong>: Is it public? [00:55:14]</p><p><strong>Swyx</strong>: No, they literally just, uh, so Dewey Keeler. I think it's his name. And you probably came across him. He wrote the RAG paper at Meta and just started contextual AI to create a RAG specific model. I don't know what that means. I was hoping that you do, cause it's your business. [00:55:29]</p><p><strong>Jerry</strong>: I had insider information. I mean, you know, to be honest, I think this, this kind of relates to my previous point on like RAG and fine tuning, like a RAG specific model is a model architecture that's designed for better RAG and it's less the software engineering principle of like, how can I take existing stuff and just plug and play different components into it? Um, and there's a beauty in that from ease of use and modularity, but when you want to end to end optimize the thing, you might want a more specific model. I think, I think building your own models is honestly pretty hard. Um, and I think the issue is if you also build your own models, like you're also just gonna have to keep up with like the rate of LM advances, like how, like basically the question is when GPT five and six and whatever, like anthropic cloud three comes out, how can you prove that you're actually better than, uh, software developers cobbling together and components on top of a base model. Right. Even if it's just like conceptually, this is better than maybe like GPT three or GPT four. [00:56:21]</p><p><strong>Alessio</strong>: What about vector stores? I know Spooks is wearing a chroma sweatshirt. [00:56:25]</p><p><strong>Swyx</strong>: Yeah, because they use a swagging. [00:56:27]</p><p><strong>Jerry</strong>: I have, I have the mug from Chroma. [00:56:29]</p><p><strong>Alessio</strong>: Yeah. It's been great. Yeah. [00:56:30]</p><p><strong>Jerry</strong>: What do you think there? [00:56:31]</p><p><strong>Alessio</strong>: Like there's a lot of them. Are they pretty interchangeable for like your users use case? Uh, is HNSW all we need? Is there room for improvements? [00:56:40]</p><p><strong>Swyx</strong>: Is NTRA all we need? [00:56:42]</p><p><strong>Jerry</strong>: I think, um, yeah, we try to remain unopinionated about storage providers. So it's not like we don't try to like play favorites. So we have like a bunch of integrations obviously. And we, the way we try to do it is we just tried to find like some standard interfaces, but obviously like different vector stores will support kind of like, uh, slightly additional things like metadata filters and those things. I mean, the goal is to have our users basically leave it up to them to try to figure out like what makes sense for their use case in terms of like the algorithm itself, I don't think the Delta on like improving the vector store, like. Embedding lookup algorithm. [00:57:10]</p><p><strong>Alessio</strong>: Is that high? [00:57:10]</p><p><strong>Jerry</strong>: I think the stuff has been mostly solved or at least there's just a lot of other stuff you can do to try to improve the overall performance. No, I mean like everything else that we just talked about, like in terms of like [00:57:20]</p><p><strong>Alessio</strong>: accuracy, right. [00:57:20]</p><p><strong>Jerry</strong>: To improve rag, like everything that we talked about, like chunking, like metadata, like. [00:57:24]</p><p><strong>Swyx</strong>: I mean, I was just thinking like, maybe for me, the interesting question is, you know, there are like eight, it's a kind of game of thrones. There's like eight, the war of eight databases right now. Oh, I see. Um, how do they stand out and how did they become very good partners? [00:57:36]</p><p><strong>Alessio</strong>: If not my index. [00:57:36]</p><p><strong>Jerry</strong>: Yeah, we're pretty good partners with, with most of them. [00:57:39]</p><p><strong>Alessio</strong>: Uh, let's see. [00:57:39]</p><p><strong>Swyx</strong>: Well, like if you're a, you know, vector database founder, like what do you, what do you work on? [00:57:44]</p><p><strong>Alessio</strong>: It's a good question. [00:57:44]</p><p><strong>Jerry</strong>: I think one thing I'm very interested in is, and this is something I think I've started to see a general trend towards is combining structured data querying with unstructured data querying. Um, and I think that will probably just expand the query sophistication of these vector stores and basically make it so that users don't have to think about whether they would just call this like hybrid querying. [00:58:05]</p><p><strong>Swyx</strong>: Is that what we've it's doing? [00:58:06]</p><p><strong>Alessio</strong>: Yeah. [00:58:07]</p><p><strong>Jerry</strong>: I mean, I think like, if you think about metadata filters, that's basically a structured filter. It's like our select where something equals something, and then you combine that with semantic search. I think like Lance DB or something was like, uh, try, I was trying to do some like joint interface. The reason is like most data is semi-structured. There's some structured annotations and there's some like unstructured texts. And so like, um, somehow combining all the expressivity of like SQL with like the flexibility of semantic search is something that I think is going to be really important. We have some basic hacks right now that allow you to jointly query both a SQL database and like a separate SQL database and a vector store to like combine the information. That's obviously going to be less efficient than if you just combined it into one [00:58:46]</p><p><strong>Alessio</strong>: system. Yeah. [00:58:46]</p><p><strong>Jerry</strong>: And so I think like PG vector, like, you know, that type of stuff, I think it's starting to get there, but like in general, like how do you have an expressive query language to actually do like structured querying along with like all the capabilities, semantic search. [00:58:57]</p><p><strong>Swyx</strong>: So your current favorite is just put it into Postgres. No, no, no. We don't play with Postgres language, the query language. [00:59:05]</p><p><strong>Jerry</strong>: I actually don't know what the best language would be for this, because I think it will be something that like the model hasn't been fine-tuned over. Um, and so you might want to train the model over this, but some way of like expressing structured data filters, and this could be include time too, right? It could, it doesn't have to just be like a where clause with this idea of like a [00:59:26]</p><p><strong>Alessio</strong>: semantic search. Yeah. [00:59:27]</p><p><strong>Swyx</strong>: And we talked about, uh, graph representations. [00:59:30]</p><p><strong>Alessio</strong>: Yeah. Oh yeah. [00:59:30]</p><p><strong>Jerry</strong>: That's another thing too. And there's like, yeah. So that's actually something I didn't even bring up yet. Like there's this interesting idea of like, can you actually have the language model, like explore like relationships within the data too, right? And somehow combine that information with stuff that's like more and more, um, structured within the DB. [00:59:46]</p><p><strong>Alessio</strong>: Awesome. [00:59:46]</p><p><strong>Swyx</strong>: What are your current strong beliefs about how to evaluate RAG ? [00:59:49]</p><p><strong>Jerry</strong>: I think I have thoughts. I think we're trying to curate this into some like more opinionated principles because there's some like open questions here. I think one question I had to think about is whether you should do like evals like component by component first, or is yours do the end to end thing? I think you should, you might actually just want to do the end to end thing first, just to do a sanity check of whether or not like this, uh, given a query and the final response, whether or not it even makes sense, like you eyeball [01:00:11]</p><p><strong>Alessio</strong>: it, right. [01:00:11]</p><p><strong>Jerry</strong>: And then you like try to do some basic evals. And then once you like diagnose what the issue is, then you go into the kind of like specific area to define some more, uh, solid benchmarks and try to like [01:00:21]</p><p><strong>Alessio</strong>: improve stuff. [01:00:21]</p><p><strong>Jerry</strong>: So what is Antoine evals? Like it's, you, um, have a query, it goes in through retrieval system. You get back something, you synthesize response, and that's your final thing. And you evaluate the quality of the final response. And these days, there's plenty of projects like startups, like companies research, doing stuff around like GPT-4, right. As like a human judge to basically kind of like synthetically generate data. [01:00:41]</p><p><strong>Swyx</strong>: I don't know from the startup side. [01:00:43]</p><p><strong>Jerry</strong>: I just know from a technical side, I think, I think people are going to do more of it. The main issue right now is just, uh, it's really unreliable. Like it's, it's just, uh, like there's like variants on the response, whatever you want. [01:00:54]</p><p><strong>Alessio</strong>: They won't do more of it. [01:00:54]</p><p><strong>Swyx</strong>: I mean, cause it's bad. [01:00:55]</p><p><strong>Jerry</strong>: No, but, but these models will get better and you'll probably fine tune a model to [01:00:59]</p><p><strong>Alessio</strong>: be a better judge. [01:00:59]</p><p><strong>Jerry</strong>: I think that's probably what's going to happen. So I'm like reasonably bullish on this because I don't think there's really a good alternative beyond you just human annotating a bunch of data sets, um, and then trying to like just manually go through and curating, like evaluating eval metrics. And so this is just going to be a more scalable solution in terms of the [01:01:17]</p><p><strong>Alessio</strong>: startups. Yeah. [01:01:17]</p><p><strong>Jerry</strong>: I mean, I think there's a bunch of companies doing this in the end. It probably comes down to some aspect of like UX speed, whether you can like fine tune a model. So that's end to end evals. And then I think like what we found is for rag, a lot of times, like, uh, what ends up affecting this, like end response is retrieval. You're just not able to retrieve the right response. And so I think having proper retrieval benchmarks, especially if you want to do production RAG is, is actually quite important. I think what does having good retrieval metrics tell you? It tells you that at least like the retrieval is good. It doesn't necessarily guarantee the end generation is good, but at least it gives you some, uh, sanity track, right? So you can like fix one component while optimizing the rest, what retrieval like evaluation is pretty standard. And it's been around for a while. It's just like an IR problem. Basically you have some like input query, you get back some retrieves out of context, and then there's some ground truth and that ranked set. And then you try to measure it based on ranking metrics. So the closer that ground truth is to the top, the more you reward the evals. And then the closer it is to the bottom where if it's not in the retrieve side at all, then you penalize the evals. Um, and so that's just like a classic ranking problem. I think like most people starting out probably don't know how to do this right [01:02:28]</p><p><strong>Alessio</strong>: now. [01:02:28]</p><p><strong>Jerry</strong>: We, we just launched them like basic retrieval evaluation modules to help users [01:02:32]</p><p><strong>Alessio</strong>: do this. [01:02:32]</p><p><strong>Jerry</strong>: One is just like curating this data set in the first place. And one thing that we're very interested in is this idea of like synthetic data set generation for evals. So how can you give in some context, generate a set of questions with Drupal 2.4, and then all of a sudden you have like question and then context pairs, and that becomes your ground truth. [01:02:47]</p><p><strong>Swyx</strong>: Are data agent evals the same thing, or is there a separate set of stuff for agents that you think is relevant here? [01:02:53]</p><p><strong>Jerry</strong>: Yeah, I think data agents add like another layer of complexity. Cause then it's just like, you have just more loops in the system. Like you can evaluate like each chain of thought loop itself, like every LLM call to see whether or not the input to that specific step in the chain of thought process actually works or is correct. Or you can evaluate like the final response to see if that's correct. This gets even more complicated when you do like multi-agent stuff, because now you have like some communication between like different agents. Like you have a top level orchestration agent passing it on to some low level [01:03:24]</p><p><strong>Alessio</strong>: stuff. [01:03:24]</p><p><strong>Jerry</strong>: I'm probably less familiar with kind of like agent eval frameworks. I know they're, they're starting to be, become a thing. Talking to like June from the Drown of Agents paper, which is pretty unrelated to what we're doing now. But it's very interesting where it's like, so you can kind of evaluate like overall agent simulations by just like kind of understanding whether or not they like modeled the distribution of human behavior. But that's not like a very macro principle. [01:03:46]</p><p><strong>Alessio</strong>: Right. [01:03:46]</p><p><strong>Jerry</strong>: And that's very much to evaluate stuff, to kind of like model the distribution of [01:03:51]</p><p><strong>Alessio</strong>: things. [01:03:51]</p><p><strong>Jerry</strong>: And I think that works well when you're trying to like generate something for like creative purposes, but for stuff where you really want the agent to like achieve a certain task, it really is like whether or not it achieved the task or not. [01:04:01]</p><p><strong>Alessio</strong>: Right. [01:04:01]</p><p><strong>Jerry</strong>: Cause then it's not like, Oh, does it generally mimic human behavior? It's like, no, like did you like send this email or not? [01:04:07]</p><p><strong>Alessio</strong>: Right. [01:04:07]</p><p><strong>Jerry</strong>: Like, cause otherwise like this, this thing didn't work. [01:04:09]</p><p><strong>Alessio</strong>: Awesome. Let's jump into a lightning round. So we have two questions, acceleration, exploration, and then one final tag away. The acceleration question is what's something that already happened in AI that you thought would take much longer to get here? [01:04:23]</p><p><strong>Jerry</strong>: I think just the ability of LLMs to generate believable outputs and for text and also for images. And I think just the whole reason I started hacking around with LLMs, honestly, I felt like I got into it pretty late. I should've gotten into it like early 2022 because UB23 had been out for a while. Like just the fact that there was this engine that was capable of like reasoning and no one was really like tapping into it. And then the fact that, you know, I used to work in image generation for a while. Like I did GANs and stuff back in the day. And that was like pretty hard to train. You would generate these like 32 by 32 images. And then now taking a look at some of the stuff by like Dolly and, and, you know, mid journey and those things. So it's, it's just, it's, it's very good. [01:04:59]</p><p><strong>Alessio</strong>: Yeah. [01:04:59]</p><p><strong>Swyx</strong>: Exploration. What do you think is the most interesting unsolved question in AI? [01:05:03]</p><p><strong>Jerry</strong>: Yeah, I'd probably work on some aspect of, um, like personalization of memory. Like, I think I actually think that I don't think anyone's like, I think a lot of people have thoughts about that, but like, for what it's worth, I don't think the final state will be right. I think it will be some, some like fancy algorithm or architecture where you like bake it into like the, the architecture of the model itself. Like if, if you have like a personalized assistant that you can talk to that will like learn behaviors over time, right. And learn stuff through like conversation history, what exactly is the right architecture there? I do think that will be part of like the wrong continuous fine tuning. [01:05:38]</p><p><strong>Swyx</strong>: Yeah. [01:05:39]</p><p><strong>Jerry</strong>: Like some aspect of that, right. [01:05:40]</p><p><strong>Alessio</strong>: Right. [01:05:40]</p><p><strong>Jerry</strong>: Like these are like, I don't actually know the specific technique, but I don't think it's just going to be something where you have like a fixed vector store and that, that thing will be like the thing that restores all your memories. [01:05:48]</p><p><strong>Swyx</strong>: It's interesting because I feel like using model weights for memory, it's just such an unreliable storage device. [01:05:56]</p><p><strong>Jerry</strong>: I know. But like, I just think, uh, from like the AGI, like, you know, just modeling like the human brain perspective, I think that there is something nice about just like being able to optimize that system. [01:06:08]</p><p><strong>Alessio</strong>: Right. [01:06:08]</p><p><strong>Jerry</strong>: And to optimize a system, you need parameters and then that's where you just get into the neural net piece. [01:06:12]</p><p><strong>Alessio</strong>: Cool. Cool. Uh, and yeah, take away, you got the audience ear. What's something you want everyone to think about or yeah, take away from this conversation and your thinking. [01:06:24]</p><p><strong>Jerry</strong>: I think there were a few key things. Uh, so we talked about two of them already, which was SEC Insights, which if you guys haven't tracked it out, I've definitely encouraged you to do so because it's not just like a random like sec app, it's like a full stack thing that we open source, right. And so if you guys want to track it out, I would definitely do that. It provides a template for you to build kind of like production grade rack apps. Um, and we're going to open source like, and modularize more components of that soon and do a workshop on, um, yeah. And the second piece is I think we are thinking a lot about like retrieval and evals. Um, I think right now we're kind of exploring integrations with like a few different partners. And so hopefully some of that will be, uh, really soon. And so just like, how do you basically have an experience where you just like write law index code, all of a sudden you can easily run like retrievals, evals, and like traces, all that stuff. And, and like a service. And so I think we're working with like a few providers on that. And then the other piece, which we did talk about already is this idea of like, yeah, building like RAG from scratch. I mean, I think everybody should do it. I think I would check out the guide. If you guys haven't already, I think it's in our docs, but instead of just using, you know, either the kind of like the retriever query engine and lamin decks or like the conversational QA train and Lang train, it's, I would take a look at how do you actually chunk parse data and do like top cam batting retrieval, because I really think that by doing that process, it helps you understand the decisions, the prompts, the language models to use. [01:07:42]</p><p><strong>Alessio</strong>: That's it. Yeah. [01:07:44]</p><p><strong>Swyx</strong>: Thank you so much, Jerry. [01:07:45]</p><p><strong>Alessio</strong>: Yeah. [01:07:45]</p><p><strong>Jerry</strong>: Thank you. [01:07:46]</p> <br /><br />Get full access to Latent Space at <a href=\"https://www.latent.space/subscribe?utm_medium=podcast&#38;utm_campaign=CTA_4\">www.latent.space/subscribe</a>\n",
      "<p><em>Want to help define </em><em>the AI Engineer stack</em><em>? >800 folks have weighed in on the top tools, communities and builders for the first </em><em>State of AI Engineering</em><em> survey, which we will present for the first time at next week‚Äôs </em><em>AI Engineer Summit</em><em>. Join us </em><em>online</em><em>!</em></p><p><em>This post had robust discussion on </em><em>HN</em><em> and </em><em>Twitter</em><em>.</em></p><p>In October 2022, Robust Intelligence hosted an internal hackathon to play around with LLMs which led to the creation of two of the most important AI Engineering tools: LangChain ü¶ú‚õìÔ∏è (<a href=\"https://www.latent.space/p/langchain#details\" target=\"_blank\">our interview with Harrison here</a>) and LlamaIndex ü¶ô by Jerry Liu, which we‚Äôll cover today. In less than a year, LlamaIndex has crossed 600,000 monthly downloads, raised $8.5M from Greylock, has a fast growing open source community that contributes to LlamaHub, and it doesn‚Äôt seem to be slowing down.</p><p><strong>LlamaIndex‚Äôs Origin (aka GPT Tree Index)</strong></p><p>Jerry struggled to make large amounts of data work with GPT-3 (which had a 4,096 tokens context window). Today LlamaIndex is at the forefront of the RAG wave (Retrieval Augmented Generation), but in the beginning Jerry wasn‚Äôt focused on embeddings and search, but rather on understanding how models could summarize, link, and reason about data. </p><p>On November 5th, Jerry pushed the <a href=\"https://github.com/run-llama/llama_index/commit/2e62c6987808797611e9bb7c1ae8c86e72a88727\" target=\"_blank\">first version</a> to Github under the name ‚ÄúGPT Tree Index‚Äù: </p><p><em>The GPT Tree Index first takes in a large dataset of unprocessed text data as input. It then builds up a tree-index in a bottom-up fashion; each parent node is able to summarize the children nodes using a general </em><em>summarization prompt</em><em>; each intermediate node containing summary text summarizing the components below. Once the index is built, it can be saved to disk and loaded for future use.</em></p><p><em>Then, say the user wants to use GPT-3 to answer a question. Using a </em><em>query prompt template</em><em>, GPT-3 will be able to recursively perform tree traversal in a top-down fashion in order to answer a question. For example, in the very beginning GPT-3 is tasked with selecting between *n* top-level nodes which best answers a provided query, by outputting a number as a multiple-choice problem. The GPT Tree Index then uses the number to select the corresponding node, and the process repeats recursively among the children nodes until a leaf node is reached.</em></p><p>[‚Ä¶]</p><p><em>How is this better than an embeddings-based approach / other state-of-the-art QA and retrieval methods?</em></p><p><em>The intent is not to compete against existing methods. A simpler embedding-based technique could be to just encode each chunk as an embedding and do a simple question-document embedding look-up to retrieve the result. This project is a simple exercise to test how GPT can organize and lookup information.</em></p><p>The project attracted a lot of attention early on (the announcement tweet has ~330 likes), but it wasn‚Äôt until ~February 2023 that the open source community really started to explode, which was around the same time that LlamaHub was released. LlamaHub made it easy for developers to import data from Google Drive, Discord, Slack, databases, and more into their LlamaIndex projects.  </p><p>What is LlamaIndex? </p><p>As we mentioned, LlamaIndex is leading the charge in the development of the RAG stack. RAG boils down to two parts:</p><p>* <strong>Indexing</strong> (i.e. how do you load and index the data in your knowledge base)</p><p>* <strong>Querying</strong> (i.e. how do you surface the data and fit it in the model context) </p><p>Indexing</p><p>To get your data from all your sources to your RAG knowledge base, you can leverage a few tools: </p><p>* <strong>Documents / Nodes</strong>: A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A Node is the atomic unit of data in LlamaIndex and represents a ‚Äúchunk‚Äù of a source Document (i.e. one Document has many Node) as well as its relationship to other Node objects.</p><p>* <strong>Data Connectors</strong>: A data connector ingest data from different sources and turn them into Document representations (text and simple metadata). These connectors are offered through <a href=\"https://llamahub.ai/\" target=\"_blank\">LlamaHub</a>, and there are over 200 of them today.</p><p>* <strong>Data Indexes</strong>: Once you‚Äôve ingested your data, LlamaIndex will help you index the data into a format that‚Äôs easy to retrieve. There are many <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/index_guide.html\" target=\"_blank\">types of indexes</a> (Summary, Tree, Vector, etc). Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/vector_store_guide.html\" target=\"_blank\">VectorStoreIndex</a>, which can then be paired with any of the vector stores out there (<a href=\"https://gpt-index.readthedocs.io/en/v0.6.21.post1/examples/vector_stores/ChromaIndexDemo.html\" target=\"_blank\">an example with Chroma</a>).</p><p>Querying</p><p>The RAG pipeline, during the querying phase, sources the most pertinent context from a user's prompt, forwarding it along to the LLM. This equips the LLM with current / private knowledge beyond its foundational training data. LlamaIndex offers adaptable modules tailored for building RAG pathways for Q&amp;A, chatbots, or agent use, since each of them has different requirements. For example, a chatbot should expect the user to interject with follow up questions, while an agent will try to carry out a whole task on its own without user intervention.  </p><p><strong>Building Blocks</strong></p><p>* <strong>Retrievers</strong>: A retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query. Vector index is the most popular mode, but there are <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/retriever/retriever_modes.html\" target=\"_blank\">other options</a> like Summary, Tree, Keyword Table, Knowledge Graph, and Document Summary.  </p><p>* <strong>Node Postprocessors</strong>: Once the retriever gets you Node objects back, you will need to do additional work like discarding low similarity ones. There are <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/node_postprocessors/modules.html\" target=\"_blank\">many options</a> here as well, such as `SimilarityPostprocessor` (i.e. drop nodes below a certain similarity score) or `LongContextReorder` which helps avoid the issues raised in the ‚Äú<a href=\"https://arxiv.org/abs/2307.03172\" target=\"_blank\">Lost in the Middle, U-shaped recollection curve</a>‚Äù paper. </p><p>* <strong>Response Synthesizers</strong>: Takes a user query and your retrieved chunks, and prompts and LLM with them. There are a few <a href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/response_synthesizers/usage_pattern.html#configuring-the-response-mode\" target=\"_blank\">response modes</a> here that balance thoroughness and compactness.</p><p><strong>Pipelines</strong></p><p>* <strong>Query Engines</strong>: A query engine is an end-to-end pipeline that allow you to ask question over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM. This makes it possible to do things like <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pandas_query_engine.html\" target=\"_blank\">‚ÄúAsk panda questions‚Äù</a> by leveraging Panda dataframes as a data source. </p><p>* <strong>Chat Engines</strong>: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question & answer). This supports traditional OpenAI-style chat interfaces, as well as more advanced ones like <a href=\"https://gpt-index.readthedocs.io/en/latest/examples/chat_engine/chat_engine_react.html\" target=\"_blank\">ReAct</a>.</p><p>* <strong>Agents</strong>: An agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools. Agent may be used in the same fashion as query engines or chat engines, but they have the power to both read and write data. For reasoning, you can use either OpenAI Functions or ReAct. Both can leverage the tools offered through LlamaHub for further analysis.</p><p>RAG vs Finetuning</p><p>Now that you have a full overview of what LlamaIndex does, the next question is ‚ÄúWhen should I use this and when should I fine tune?‚Äù. Jerry‚Äôs TLDR is that ‚ÄúRAG is just a hack‚Äù, but a powerful one. Each option has pros and cons:</p><p>* <strong>Lower investment: </strong>RAG requires almost 0 upfront investment, unlike finetuning which requires data cleaning, model training, increased costs for finetuned inference, etc.</p><p>* <strong>Stricter access control and higher visibility:</strong> when finetuning, the model learns everything. With RAG, you can decide what documents the index should have access to, making it more secure by default. You are also able to see everything that was passed into the context if a response doesn‚Äôt look right.</p><p>* <strong>Context window limitation: </strong>you can only fit so many tokens into the prompt due to the way models work. Finetuning helps you circumvent that by compressing the knowledge into the model weights rather than putting it in the prompt.   </p><p>As Jerry says, the best way to know this inside out is <a href=\"https://twitter.com/jerryjliu0/status/1702345670563332340\" target=\"_blank\">to learn to build RAG from scratch</a> (without LlamaIndex) - and they have plenty of tutorials on his Twitter and blog to learn this.</p><p>The other issue is that the math for finetuning isn‚Äôt well known yet as we <a href=\"https://www.latent.space/p/transformers-math#details\" target=\"_blank\">discussed with Quentin Anthony from Eleuther</a>, so unless you have money and time to invest into exploring fine tuning, you‚Äôre better off starting with RAG.  </p><p>Full YouTube Discussion!</p><p>Show Notes</p><p>* <strong>LlamaIndex</strong></p><p>* <a href=\"https://llamahub.ai/\" target=\"_blank\">LlamaHub</a></p><p>* <a href=\"https://www.secinsights.ai/\" target=\"_blank\">SEC Insights</a></p><p>* <a href=\"https://www.robustintelligence.com/\" target=\"_blank\">Robust Intelligence</a></p><p>* <a href=\"https://quorablog.quora.com/Poe-1\" target=\"_blank\">Quora‚Äôs Poe</a></p><p>* <a href=\"https://trychroma.com/\" target=\"_blank\">Chroma</a></p><p>* <a href=\"https://vespa.ai/\" target=\"_blank\">Vespa</a></p><p>* <a href=\"https://twitter.com/jerryjliu0/status/1702345670563332340\" target=\"_blank\">Why should every AI engineer learn to build RAG from scratch</a>?</p><p>* <a href=\"https://www.langchain.com/\" target=\"_blank\">LangChain</a></p><p>* <a href=\"https://gorilla.cs.berkeley.edu/\" target=\"_blank\">Gorilla</a></p><p>* <a href=\"https://arxiv.org/abs/2307.03172\" target=\"_blank\">Lost in the Middle: How Language Models Use Long Contexts</a></p><p>Timestamps</p><p>* [00:00:00] Introductions and Jerry‚Äôs background</p><p>* [00:04:30] Starting LlamaIndex as a side project</p><p>* [00:05:11] Evolution from tree-index to current LlamaIndex and LlamaHub architecture</p><p>* [00:11:39] Deciding to leave Robust to start the LlamaIndex company and raising funding</p><p>* [00:20:06] Context window size and information capacity for LLMs</p><p>* [00:21:34] Minimum viable context and maximum context for RAG</p><p>* [00:22:52] Fine-tuning vs RAG - current limitations and future potential</p><p>* [00:24:02] RAG as a hack but good hack for now</p><p>* [00:26:19] RAG benefits - transparency and access control</p><p>* [00:27:46] Potential for fine-tuning to take over some RAG capabilities</p><p>* [00:30:04] Baking everything into an end-to-end trained LLM</p><p>* [00:33:24] Similarities between iterating on ML models and LLM apps</p><p>* [00:34:47] Modularity and customization options in LlamaIndex: data loading, retrieval, synthesis, reasoning</p><p>* [00:40:16] Evaluating and optimizing each component of Lama Index system</p><p>* [00:46:02] Building retrieval benchmarks to evaluate RAG</p><p>* [00:47:24] SEC Insights - open source full stack LLM app using LlamaIndex</p><p>* [00:49:48] Enterprise platform to complement LlamaIndex open source</p><p>* [00:51:00] Community contributions for LlamaHub data loaders</p><p>* [00:53:21] LLM engine usage - majority OpenAI but options expanding</p><p>* [00:56:25] Vector store landscape</p><p>* [00:59:46] Exploring relationships and graphs within data</p><p>* [01:03:24] Additional complexity of evaluating agent loops</p><p>* [01:04:01] Lightning Round</p><p>\n"
     ]
    }
   ],
   "source": [
    "print(episode_summary)\n",
    "print(episode_summary[episode_summary.find(\"Transcript\"):])\n",
    "print(episode_summary[:episode_summary.find(\"Transcript\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alessio: Hey everyone, welcome to the Latent Space Podcast. This is Alessio, partner and CTO of Residence and Decibel Partners, and I'm joined by my co-host Swyx, founder of Smol AI. [00:00:20]\n",
      "Swyx: And today we finally have Jerry Liu on the podcast. Hey Jerry. [00:00:24]\n",
      "Jerry: Hey guys. Hey Swyx and Alessio. Thanks for having me. [00:00:27]\n",
      "Swyx: It's kind of weird because we keep running into each other in San Francisco AI events, so it's kind of weird to finally just have a conversation recorded for everybody else. [00:00:34]\n",
      "Jerry: Yeah, I know. I'm really looking forward to this, aside from the questions. [00:00:38]\n",
      "Swyx: So I tend to introduce people on their formal background and then ask something on the more personal side. So you are part of the Princeton gang. [00:00:46]\n",
      "Jerry: I don't know if there is like official Princeton gang. [00:00:48]\n",
      "Swyx: No, small Princeton gang. Okay. I attended your meeting. There was like four of you with Prem and the others. And then you have a bachelor's in CS and a certificate in finance. That's also fun. I also did finance and I think I saw that you also interned at Two Sigma where I worked in New York. You were a machine learning engineer. [00:01:06]\n",
      "Jerry: You were at Two Sigma?\n",
      "Swyx: Yeah, very briefly.\n",
      "Jerry: Oh, cool. I didn't know that. [00:01:09]\n",
      "Swyx: That was my first like proper engineering job before I went into DevRel. [00:01:12]\n",
      "Jerry: Oh, okay. Nice. [00:01:14]\n",
      "Swyx: And then you were a machine learning engineer at Quora, AI research scientist at Uber for three years, and then two years machine learning engineer at Robust Intelligence before starting LlamaIndex. So that's your LinkedIn. It's not only LinkedIn that people should know about you. [00:01:27]\n",
      "Jerry: I think back during my Quora days, I had this like three-month phase where I just wrote like a ton of Quora answers. And so I think if you look at my tweets nowadays, you can basically see that as like the V2 of my three-month like Forrestant where I just like went ham on Quora for a bit. I actually, I think I was back then actually when I was working on Quora, I think the thing that everybody was fascinated in was just like general like deep learning advancements and stuff like GANs and generative like images and just like new architectures that were evolving. And it was a pretty exciting time to be a researcher actually, because you were going in like really understanding some of the new techniques. So I kind of use that as like a learning opportunity, basically just like read a bunch of papers and then answer questions on Quora. And so you can kind of see traces of that basically in my current Twitter where it's just like really about kind of like framing concepts and trying to make it understandable and educate other users on it. Yeah. [00:02:17]\n",
      "Swyx: I've said, so a lot of people come to me for my Twitter advice, but like, I think you are doing one of the best jobs in AI Twitter, which is explaining concepts and just consistently getting hits out. Thank you. I didn't know it was due to the Quora training. Let's just sign on on Quora. A lot of people, including myself, like kind of wrote off Quora as like one of the web 1.0 like sort of question answer forums. But now I think it's becoming, seeing a resurgence obviously due to Poe and obviously Adam and D'Angelo has always been a leading tech figure, but what do you think is kind of underrated about Quora? [00:02:46]\n",
      "Jerry: Well, I mean, I like the, I really liked the mission of Quora when I, when I joined. In fact, I interned there like in 2015 and I joined full time in 2017. One is like they had, and they have like a very talented engineering team and just like really, really smart people. And the other part is the whole mission of the company is to just like spread knowledge and to educate people. And to me that really resonated. I really liked the idea of just like education and democratizing the flow of information. If you imagine like kind of back then it was like, okay, you have Google, which is like for search, but then you have Quora, which is just like user generated, like grassroots type content. And I really liked that concept because it's just like, okay, there's certain types of information that aren't accessible to people, but you can make accessible by just like surfacing it. And so actually, I don't know if like most people know that about like Quora and if they've used the product, whether through like SEO, right, or kind of like actively, but that really was what drew me to it. [00:03:39]\n",
      "Swyx: Yeah. I think most people challenges with it is that sometimes you don't know if it's like a veiled product pitch, right? [00:03:44]\n",
      "Jerry: Yeah. Of course, like quality of the answer matters quite a bit. And then you start running into these like- [00:03:47]\n",
      "Swyx: It's like five alternatives and then here's the one I work on. Yeah. [00:03:50]\n",
      "Jerry: Like recommendation issues and all that stuff. I used, I worked on recsys at Quora actually, so I got a taste of some of that stuff. Well, I mean, I kind of more approached it from machine learning techniques, which might be a nice segue into RAG actually. A lot of it was just information retrieval. We weren't like solving anything that was like super different than what was standard in the industry at the time, but just like ranking based on user preferences. I think a lot of Quora was very metrics driven. So just like trying to maximize like daily active hours, like time spent on site, those types of things. And all the machine learning algorithms were really just based on embeddings. You have a user embedding and you have like item embeddings and you try to train the models to try to maximize the similarity of these. And it's basically a retrieval problem. [00:04:30]\n",
      "Swyx: Okay. So you've been working on RAG for longer than most people think? [00:04:33]\n",
      "Jerry: Well, kind of. So I worked there for like a year, right, just transparently. And then I worked at Uber where I was not working on ranking. It was more like kind of deep learning training for self-driving and computer vision and that type of stuff. But I think in the LLM world, it's kind of just like a combination of like everything these days. I mean, retrieval is not really LLMs, but like it fits within the space of like LLM apps. And then obviously like having knowledge of the underlying deep learning architectures helps. Having knowledge of basic software engineering principles helps too. And so I think it's kind of nice that like this whole LLM space is basically just a combination of just like a bunch of stuff that you probably like people have done in the past. [00:05:11]\n",
      "Swyx: It's good. It's like a summary capstone project. Yeah, exactly. [00:05:14]\n",
      "Jerry: Yeah. [00:05:15]\n",
      "Alessio: And before we dive into LlamaIndex, what do they feed you a robust intelligence that both you and Harrison from LangChain came out of it at the same time? Was there like, yeah. Is there any fun story of like how both of you kind of came up with kind of like core infrastructure to LLM workflows today? Or how close were you at robust? Like any fun behind the scenes? [00:05:37]\n",
      "Jerry: Yeah. Yeah. We, um, we work pretty closely. I mean, we were on the same team for like two years. I got to know Harrison and the rest of the team pretty well. I mean, I have a respect that people there, the people that were very driven, very passionate. And it definitely pushed me to be, you know, a better engineer and leader and those types of things. Yeah. I don't really have a concrete explanation for this. I think it's more just, we have like an LLM hackathon around like September. This was just like exploring GPT-3 or it was October actually. And then the day after I went on vacation for a week and a half, and so I just didn't track Slack or anything. And then when I came back, saw that Harrison started LangChain [00:06:09]\n",
      "Swyx: Oh that's cool. [00:06:10]\n",
      "Jerry: I was like, oh, I'll play around with LLMs a bit and then hacked around on stuff. And I think I've told the story a few times, but you know, I was like trying to feed in information into GPT-3. And then, then you deal with like context window limitations and there was no tooling or really practices to try to understand how do you, you know, get GPT-3 to navigate large amounts of data. And that's kind of how the project started. Really was just one of those things where early days, like we were just trying to build something that was interesting. Like I wanted to start a company. I had other ideas actually of what I wanted to start. And I was very interested in, for instance, like multimodal data, like video data and that type of stuff. And then this just kind of grew and eventually took over the other idea. [00:06:48]\n",
      "Swyx: Text is the universal interface. [00:06:50]\n",
      "Jerry: I think so. I think so. I actually think once the multimodal models come out, I think there's just like mathematically nicer properties of you can just get like join multiple embeddings, like clip style. But text is really nice because from a software engineering principle, it just makes things way more modular. You can just convert everything into text and then you just represent everything as text. [00:07:08]\n",
      "Swyx: Yeah. I'm just explaining retroactively why working on LlamaIndex took off versus if you had chose to spend your time on multimodal, we probably wouldn't be talking about whatever you ended up working on. [00:07:18]\n",
      "Jerry: Yeah. [00:07:19]\n",
      "Swyx: That's true. It's troubled. Interesting. So November 9th, that was a very productive month. I guess October, November, November 9th, you announced GPT-3 Index and you picked a tree logo. Very cool. Every project must have an emoji. [00:07:32]\n",
      "Jerry: Yeah. Yeah. I probably was somewhat inspired by a light train, but I will admit, yeah. [00:07:37]\n",
      "Swyx: It uses GPT to build a knowledge tree in a bottoms-up fashion by applying a summarization prompt for each node. Yep. Which I like that original vision. Your messaging roundabout then was also that you're creating optimized data structures. What's the sort of journey to that and how does that contrast with LlamaIndex today? Okay. [00:07:56]\n",
      "Jerry: Maybe I can tell a little bit about the beginning intuitions. I think when I first started, this really wasn't supposed to be something that was like a toolkit that people use. It was more just like a system. And the way I wanted to think about the system was more a thought exercise of how language models with their reasoning capabilities, if you just treat them as like brains, can organize information and then traverse it. So I didn't want to think about embeddings, right? To me, embeddings just felt like it was just an external thing that was like, well, it was just external to trying to actually tap into the capabilities of language models themselves, right? I really wanted to see, you know, just as like a human brain could like synthesize stuff, could we create some sort of like structure where this neural CPU, if you will, can like organize a bunch of information, you know, auto-summarize a bunch of stuff and then also traverse the structure that I created. That was the inspiration for this initial tree index, to be honest. And I think I said this in the first tweet, it actually works super well, right? Like GPT-4 obviously is much better at reasoning. I'm one of the first to say, you know, you shouldn't use anything pre-GPT-4 for anything that requires complex reasoning because it's just going to be unreliable, okay, disregarding stuff like fine tuning. But it worked okay. But I think it definitely struck a chord with kind of like the Twitter crowd, which is just like new ideas at the time, I guess, just like thinking about how you can actually bake this into some sort of application. Because I think what I also ended up discovering was the fact that there was starting to become a wave of developers building on top of GPT-3 and people were starting to realize that what makes them really useful is to apply them on top of your personal data. And so even if the solution itself was kind of like primitive at the time, like the problem statement itself was very powerful. And so I think being motivated by the problem statement, right, like this broad mission of how do I unlock elements on top of the data also contributed to the development of LOM index to the state it is today. And so I think part of the reason, you know, our toolkit has evolved beyond the just existing set of like data structures is we really tried to take a step back and think, okay, what exactly are the tools that would actually make this useful for a developer? And then, you know, somewhere around December, we made an active effort to basically like push towards that direction, make the code base more modular, right, more friendly as an open source library. And then also start adding in like embeddings, start thinking into practical considerations like latency, cost, performance, those types of things. And then really motivated by that mission, like start expanding the scope of the toolkit towards like covering the life cycle of like data ingestion and querying. Where you also added Llamahub and yeah, so I think that was in like January on the data loading side. And so we started adding like some data loaders, saw an opportunity there, started adding more stuff on the retrieval querying side, right? We still have like the core data structures, but how do you actually make them more modular and kind of like decouple storing state from the types of like queries that you could run on top of this a little bit. And then starting to get into more complex interactions, like chain of thought reasoning, routing and, you know, like agent loops. [00:10:44]\n",
      "Alessio: You and I spent a bunch of time earlier this year talking about Llamahub, what that might become. You were still at Robust. When did you decide it was time to start the company and then start to think about what LlamaIndex is today? [00:10:58]\n",
      "Jerry: Yeah, I mean, probably December. It was kind of interesting. I was getting some inbound from initial VCs, I was talking about this project. And then in the beginning, I was like, oh, yeah, you know, this is just like a design project. But you know, what about my other idea on like video data, right? And then I was trying to like get their thoughts on that. And then everybody was just like, oh, yeah, whatever, like that part's like a crowded market. And then it became clear that, you know, this was actually a pretty big opportunity. And like, coincidentally, right, like this actually did relate to like, my interests have always been at the intersection of AI data and kind of like building practical applications. And it was clear that this was evolving into a much bigger opportunity than the previous idea was. So around December, and then I think I gave a pretty long notice, but I left officially like early March. [00:11:39]\n",
      "Alessio: What were your thinkings in terms of like moats and, you know, founders kind of like overthink it sometimes. So you obviously had like a lot of open source love and like a lot of community. And you're like, were you ever thinking, okay, I don't know, this is maybe not enough to start a company or did you always have conviction about it? [00:11:59]\n",
      "Jerry: Oh, no, I mean, 100%. I felt like I did this exercise, like, honestly, probably more late December and then early January, because I was just existentially worried about whether or not this would actually be a company at all. And okay, what were the key questions I was thinking about? And these were the same things that like other founders, investors, and also like friends would ask me is just like, okay, what happens if context windows get much bigger? What's the point of actually structuring data right in the right way? Right? Why don't you just dump everything into the prompt, fine tuning, like, what if you just train the model over this data? And then, you know, what's the point of doing this stuff? And then some other ideas is what if like OpenAI actually just like takes this like builds upwards on top of the their existing like foundation models and starts building in some like built in orchestration capabilities around stuff like RAG and agents and those types of things. And so I basically ran through this mental exercise and, you know, I'm happy to talk a little bit more about those thoughts as well. But at a high level, well, context windows have gotten bigger, but there's obviously still a need for a rag. I think RAG is just like one of those things that like, in general, what people care about is, yes, they do care about performance, but they also care about stuff like latency and costs. And so my entire reasoning at the time was just like, okay, like, yes, maybe you will have like much bigger context windows, as we've seen with like 100k context windows. But for enterprises, like, you know, data, which is not in just like the scale of like a few documents, it's usually in like gigabytes, terabytes, petabytes. How do you actually just unlock language models over that data, right? And so it was clear there was just like, whether it's RAG or some other paradigm, no one really knew what that answer was. And so there was clearly like technical opportunity here. Like there was just stacks that needed to be invented to actually solve this type of problem, because language models themselves didn't have access to this data. The other piece here is just like, and so if like you just dumped all this data into, let's say a model had like hypothetically an infinite context window, right? And you just dump like 50 gigabytes of data into a context window. That just seemed very inefficient to me, because you have these network transfer costs of uploading 50 gigabytes of data to get back a single response. And so I kind of realized, you know, there's always going to be some curve, regardless of like the performance of the best performing models of like cost versus performance. What RAG does is it does provide extra data points along that access, because you kind of control the amount of context you actually wanted to retrieve. And of course, like RAG as a term was still evolving back then, but it was just this whole idea of like, how do you just fetch a bunch of information to actually, you know, like stuff into the prompt. And so people even back then were kind of thinking about some of those considerations. [00:14:29]\n",
      "Swyx: And then you fundraised in June, or you announced your fundraiser in June. Yeah. Take us through that process of thinking about the fundraise and your plans for the company, you know, at the time. Yeah, definitely. [00:14:41]\n",
      "Jerry: I mean, I think we knew we wanted to, I mean, obviously we knew we wanted to fundraise. There was also a bunch of like investor interest, and it was probably pretty unusual given the, you know, like hype wave of generative AI. So like a lot of investors were kind of reaching out around like December, January, February. In the end, we went with Greylock. Greylock's great. You know, they've been great partners so far. And to be honest, like there's a lot of like great VCs out there. And a lot of them who are specialized on like open source, data, infra, and that type of stuff. What we really wanted to do was, because for us, like time was of the essence, like we wanted to ship very quickly and still kind of build Mindshare in this space. We just kept the fundraising process very efficient. I think we basically did it in like a week or like three days. And so, yeah, just like front loaded it and then just like pick the one named Jerry. Yeah, exactly. Yeah. [00:15:27]\n",
      "Swyx: I'm kidding. I mean, he's obviously great and Greylock's a fantastic firm. [00:15:32]\n",
      "Jerry: Embedding some of my research. So, yeah, just we've had Greylock. They've been great partners. I think in general, when I talk to founders about like the fundraise process, it's never like the most fun period, I think, because it's always just like, you know, there's a lot of logistics, there's lawyers you have to, you know, get in the loop. And like a lot of founders just want to go back to building. I think in the end, we're happy that we kept it to a pretty efficient process. [00:15:54]\n",
      "Swyx: And so you fundraise with Simon. How do you split things with him? How big is your team now? [00:15:57]\n",
      "Jerry: The team is growing. By the time this podcast is released, we'll probably have had one more person join the team. So basically, it's between, we're rapidly getting to like eight or nine people. At the current moment, we're around like six. And so just like there'll be some exciting developments in the next few weeks. I'm excited to announce that. So the team is, has kind of like, we've been pretty selective in terms of like how we like grow the team. Obviously, like we look for people that are really active in terms of contributions to Lum Index, people that have like very strong engineering backgrounds. And primarily, we've been kind of just looking for builders, people that kind of like grow the open source and also eventually this like managed like enterprise platform as well with us. In terms of like Simon, yeah, I've known Simon for a few years now. I knew him back at Uber ATG in Toronto. He's one of the smartest people I knew, has a sense of both like a deep understanding of ML, but also just like first principles thinking about like engineering and technical concepts in general. And I think one of my criteria, criteria is when I was like looking for a co-founder for this project with someone that was like technically better than me, because I knew I wanted like a CTO. And so honestly, like there weren't a lot of people that, I mean, there's, I know a lot of people that are smarter than me, but like that fit that bill. We're willing to do a startup and also just have the same like values that I shared. Right. And just, I think doing a startup is very hard work, right? It's not like, I'm sure like you guys all know this, it's, it's a lot of hours, a lot of late nights and you want to be like in the same place together and just like being willing to hash out stuff and have that grit basically. And I really looked for that. And so Simon really fit that bill and I think I convinced him to bring Trump on board. [00:17:24]\n",
      "Swyx: Yeah. And obviously I've had the pleasure of chatting and working with a little bit with both of you. What would you say those, those like your top one or two values are when, when thinking about that or the culture of the company and that kind of stuff? [00:17:36]\n",
      "Jerry: I think in terms of the culture of the company, it's really like, I mean, there's a few things I can name off the top of my head. One is just like passion, integrity. I think that's very important for us. We want to be honest. We don't want to like, obviously like copy code or, or kind of like, you know, just like, you know, not give attribution, those types of things and, and just like be true to ourselves. I think we're all very like down to earth, like humble people, but obviously I think just willingness to just like own stuff and dive right in. And I think grit comes with it. I think in the end, like this is a very fast moving space and we want to just like be one of the, you know, like dominant forces and helping to provide like production quality outline applications. Yeah. [00:18:11]\n",
      "Swyx: I promise we'll get to more technical questions, but I also want to impress on the audience that this is a very conscious and intentional company building. And since your fundraising post, which was in June, and now it's September, so it's been about three months, you've actually gained 50% in terms of stars and followers. You've 3x'd your download count to 600,000 a month and your discord membership has reached 10,000. So like a lot of ongoing growth. [00:18:37]\n",
      "Jerry: Yeah, definitely. And obviously there's a lot of room to expand there too. And so open source growth is going to continue to be one of our core goals because in the end it's just like, we want this thing to be, well, one big, right? We all have like big ambitions, but to just like really provide value to developers and helping them in prototyping and also productionization of their apps. And I think it turns out we're in the fortunate circumstance where a lot of different companies and individuals, right, are in that phase of like, you know, maybe they've hacked around on some initial LLM applications, but they're also looking to, you know, start to think about what are the production grade challenges necessary to actually, that to solve, to actually make this thing robust and reliable in the real world. And so we want to basically provide the tooling to do that. And to do that, we need to both spread awareness and education of a lot of the key practices of what's going on. And so a lot of this is going to be continued growth, expansion, education, and we do prioritize that very heavily. [00:19:30]\n",
      "Alessio: Let's dive into some of the questions you were asking yourself initially around fine tuning and RAG , how these things play together. You mentioned context. What is the minimum viable context for RAG ? So what's like a context window too small? And at the same time, maybe what's like a maximum context window? We talked before about the LLMs are U-shaped reasoners. So as the context got larger, like it really only focuses on the end and the start of the prompt and then it kind of peters down. Any learnings, any kind of like tips you want to give people as they think about it? [00:20:06]\n",
      "Jerry: So this is a great question. And part of what I wanted to talk about a conceptual level, especially with the idea of like thinking about what is the minimum context? Like, okay, what if the minimum context was like 10 tokens versus like, you know, 2k tokens versus like a million tokens. Right. Like, and what does that really give you? And what are the limitations if it's like 10 tokens? It's kind of like, um, like eight bit, 16 bit games, right? Like back in the day, like if you play Mario and you have like the initial Mario where the graphics were very blocky and now obviously it's like full HD, 3d, just the resolution of the context and the output will change depending on how much context you can actually fit in. So the way I kind of think about this from a more principled manner is like you have like, there's this concept of like information capacity, just this idea of like entropy, like given any fixed amount of like storage space, like how much information can you actually compact in there? And so basically a context window length is just like some fixed amount of storage space, right? And so there's some theoretical limit to the maximum amount of information you can compact until like a 4,000 token storage space. And what does that storage space use for these days with LLMs? For inputs and also outputs. And so this really controls the maximum amount of information you can feed in terms of the prompt plus the granularity of the output. If you had an infinite context window, you're going to have an infinitely detailed response and also infinitely detailed memory. But if you don't, you can only kind of represent stuff in more quantized bits, right? And so the smaller the context window, just generally speaking, the less details and maybe the less, um, and for like specific, precise information, you're going to be able to surface any given point in time. [00:21:34]\n",
      "Alessio: So when you have short context, is the answer just like get a better model or is the answer maybe, Hey, there needs to be a balance between fine tuning and RAG to make sure you're going to like leverage the context, but at the same time, don't keep it too low resolution? [00:21:48]\n",
      "Jerry: Yeah, yeah. Well, there's probably some minimum threat, like I don't think anyone wants to work with like a 10. I mean, that's just a thought exercise anyways, a 10 token context window. I think nowadays the modern context window is like 2k, 4k is enough for just like doing some sort of retrieval on granular context and be able to synthesize information. I think for most intents and purposes, that level of resolution is probably fine for most people for most use cases. I think the question there is just like, um, the limitations actually more on, okay, if you're going to actually combine this thing with some sort of retrieval data structure mechanism, there's just limitations on the retrieval side because maybe you're not actually fetching the most relevant context to actually answer this question, right? Like, yes, like given the right context, 4,000 tokens is enough. But if you're just doing like top-k similarity, like you might not be able to be fetching the right information from the documents. [00:22:34]\n",
      "Alessio: So how should people think about when to stick with RAG versus when to even entertain and also in terms of what's like the threshold of data that you need to actually worry about fine tuning versus like just stick with rag? Obviously you're biased because you're building a RAG company, but no, no, actually, um, I [00:22:52]\n",
      "Jerry: think I have like a few hot takes in here, some of which sound like a little bit contradictory or what we're actually building. And I think to be honest, I don't think anyone knows the right answer. I think this is the truth. [00:23:01]\n",
      "Alessio: Yeah, exactly. [00:23:01]\n",
      "Jerry: This is just like thought exercise towards like understanding the truth. [00:23:04]\n",
      "Alessio: Right. [00:23:04]\n",
      "Jerry: So, okay. [00:23:05]\n",
      "Alessio: I have a few hot takes. [00:23:05]\n",
      "Jerry: One is like RAG is basically just, just a hack, but it turns out it's a very good hack because what is RAG rag is you keep the model fixed and you just figure out a good way to like stuff stuff into the prompt of the language model and everything that we're doing nowadays in terms of like stuffing stuff into the prompt is just algorithmic. We're just figuring out nice algorithms to, to like retrieve right information with top case similarity, do some sort of like, uh, you know, hybrid search, some sort of like a chain of thought decomp and then just like stuff stuff into a prompt. So it's all like algorithmic and it's more like just software engineering to try to make the most out of these like existing APIs. The reason I say it's a hack is just like from a pure like optimization standpoint. If you think about this from like the machine learning lens, unless the software engineering lens, there's pieces in here that are going to be like suboptimal, right? Like, like the thing about machine learning is when you optimize like some system that can be optimized within machine learning, like the set of parameters, you're really like changing like the entire system's weights to try to optimize the subjective function. [00:24:02]\n",
      "Jerry: And if you just cobble a bunch of stuff together, you can't really optimize the pieces are inefficient, right? And so like a retrieval interface, like doing top cam batting lookup, that part is  inefficient. [00:24:13]\n",
      "Jerry: If you, for instance, because there might be potentially a better, more learned retrieval algorithm, that's better. If you know, you do stuff like some sort of, I know nowadays there's this concept of how do you do like short-term and long-term memory represent stuff in some sort of vector embedding, do trunk sizes, all that stuff. It's all just like decisions that you make that aren't really optimized and it's not really automatically learned. It's more just things that you set beforehand to actually feed into the system. So I do think like there is a lot of room to actually optimize the performance of an entire LLM system, potentially in a more like machine learning based way. Right. [00:24:48]\n",
      "Jerry: And I will leave room for that. And this is also why I think like in the long term, I do think fine tuning will probably have like greater importance. And just like there will probably be new architectures invented that where you can actually kind of like include a lot of this under the black box, as opposed to having like hobbling together a bunch of components outside the black box. That said, just very practically given the current state of things, like even if I said RAG is a hack, it's a very good hack and it's also very easy to use. Right. [00:25:16]\n",
      "Jerry: And so just like for kind of like the AI engineer persona, which to be fair is kind of one of the reasons generative AI has gotten so big is because it's way more accessible for everybody to get into, as opposed to just like traditional machine learning, it tends to be good enough. [00:25:30]\n",
      "Jerry: Right. And if we can basically provide these existing techniques to help people really optimize how to use existing systems without having to really deeply understand machine learning, I still think that's a huge value add. And so there's very much like a UX and ease of use problem here, which is just like RAG is way easier to onboard and use. And that's probably like the primary reason why everyone should do RAG instead of fine tuning to begin with. If you think about like the 80-20 rule, like RAG very much fits within that and fine tuning doesn't really right now. And then I'm just kind of like leaving room for the future that, you know, like in the end, fine tuning can probably take over some of the aspects of like what RAG does. [00:26:04]\n",
      "Swyx: I don't know if this is mentioned in your explainability also allows for sourcing. And at the end of the day, like to increase trust that we have to source documents. Yeah. [00:26:14]\n",
      "Jerry: So, so I think what RAG does is it increases like transparency, visibility into the actual documents, right. [00:26:19]\n",
      "Jerry: That are getting fed into their context. [00:26:21]\n",
      "Swyx: Here's where they got it from. [00:26:22]\n",
      "Alessio: Exactly. [00:26:22]\n",
      "Jerry: That's definitely an advantage. I think the other piece that I think is an advantage, and I think that's something that someone actually brought up is just you can do access control with, with RAG . If you have an external storage system, you can't really do that with, with large language models. [00:26:35]\n",
      "Jerry: It's just like gate information to the neural net weights, like depending on the type of user for the first point, you could technically, you could technically have the  language model. [00:26:45]\n",
      "Jerry: Like if it memorized enough information, just like a site sources, but there's a question of just trust whether or not you're actually, yeah, well, but like it makes it up right now because it's like not good enough, but imagine a world where it is good enough and it does give accurate citations. \n",
      "Swyx: No, I think to establish trust, you just need a direct connection.So it's, it's kind of weird.  It's, it's this melding of deep learning systems versus very traditional information retrieval. Yeah, exactly. [00:27:11]\n",
      "Jerry: Well, so, so I think, I mean, I kind of think about it as analogous to like humans, right? [00:27:15]\n",
      "Jerry: Like, uh, we as humans, obviously we use the internet, we use tools. Uh, these tools have API interfaces are well-defined. Um, and obviously we're not like the tools aren't part of us. And so we're not like back propping or optimizing over these tools. And so when you think about like RAG , it's basically, um, LLM is learning how to use like a vector database to look up information that it doesn't know. And so then there's just a question of like how much information is inherent within the network itself and how much does it need to do some sort of like tool used to look up  stuff that it doesn't know. [00:27:42]\n",
      "Jerry: And I do think there'll probably be more and more of that interplay as time goes on. [00:27:46]\n",
      "Swyx: Yeah. Some followups on discussions that we've had, you know, we discussed fine tuning a bit and what's your current take on whether you can, you can fine tune new knowledge into LLMs. [00:27:55]\n",
      "Jerry: That's one of those things where I think longterm you definitely can. I think some people say you can't, I disagree. I think you definitely can. Just right now I haven't gotten it to work yet. So, so I think like we've tried, yeah, well, um, not in a very principled way, right? Like this is something that requires like an actual research scientist and not someone that has like, you know, an hour or two per night to actually look at this. [00:28:12]\n",
      "Swyx: Like I, you were a research scientist at Uber. I mean, it's like full-time, full-time working. [00:28:16]\n",
      "Jerry: So, so I think, um, what I specifically concretely did was I took OpenAI's fine tuning endpoints and then tried to, you know, it's in like a chat message interface. And so there's like, um, input question, like a user assistant message format. And so what I did was I tried to take just some piece of text and have the LLM memorize it by just asking it a bunch of questions about the text. So given a bunch of context, I would generate some questions and then generate some response and just fine tune over the question responses. That hasn't really worked super well, but that's also because I'm, I'm just like trying to like use OpenAI's endpoints as is. If you just think about like traditional, like how you train a Transformers model, there's kind of like the, uh, instruction, like fine tuning aspect, right? You like ask it stuff when guided with correct responses, but then there's also just like, um, next token production. And that's something that you can't really do with the OpenAI API, but you can do with, if you just train it yourself and that's probably possible if you just like train it over some corpus of data. I think Shashira from Berkeley said like, you know, when they trained Gorilla, they were like, Oh, you know, this, a lot of these LLMs are actually pretty good at memorizing information. Um, just the way the API interface is exposed is just no one knows how to use them right [00:29:22]\n",
      "Alessio: now. Right. [00:29:22]\n",
      "Jerry: And so, so I think that's probably one of the issues. [00:29:24]\n",
      "Swyx: Just to clue people in who haven't read the paper, Gorilla is the one where they train to use specific APIs. [00:29:30]\n",
      "Jerry: Yeah, I think this was on the Gorilla paper. Like the, the model itself could, uh, try to learn some prior over the data to decide like what tool to pick. But there's also, it's also augmented with retrieval that helps supplement it in case like the, the, the, um, prior doesn't actually work. [00:29:45]\n",
      "Swyx: Is that something that you'd be interested in supporting? [00:29:48]\n",
      "Jerry: I mean, I think in the longterm, like if like, this is kind of how fine tuning, like RAG evolves. Like I do think there'll be some aspect where fine tuning will probably memorize some high level concepts of knowledge, but then like RAG will just be there to supplement like aspects of that, that aren't work that don't, that, that it doesn't know. \n",
      "Jerry: Um, the way I think about this is kind of like, obviously RAG is the default way, like to be clear, RAG right now is the default way to actually augment stuff with knowledge. I think it's just an open question of how much the LM can actually internalize both high level concepts, but also details as you can like train stuff over it. And coming from an ML background, there is a certain beauty and just baking everything into some training process of a language model. Like if you just take raw chat, GPT or chat, GPT code interpreter, right? Like GPT four, it's not like you do RAG with it. You just ask it questions about like, Hey, how do I like to find a pedantic model in Python? And I'm like, can you give me an example? Can you visualize a graph? It just does it right. Like, and we'll run it through code interpreters as a tool, but that's not like a source for knowledge. [00:30:46]\n",
      "Jerry: It's just an execution environment. And so there is some beauty in just like having the model itself, like just, you know, instead of you kind of defining the algorithm for what the data structure should look like the model just learns it under the hood. That said, I think the reason it's not a thing right now is just like, no one knows how to do it. [00:31:01]\n",
      "Jerry: It probably costs too much money. And then also like the API interfaces and just like the actual ability to kind of evaluate and improve on performance, like isn't known to most people. [00:31:12]\n",
      "Alessio: Yeah. [00:31:12]\n",
      "Swyx: It also would be better with browsing. [00:31:14]\n",
      "Alessio: Yeah. [00:31:16]\n",
      "Swyx: I wonder when they're going to put that back. [00:31:18]\n",
      "Alessio: Okay. Yeah. [00:31:19]\n",
      "Swyx: So, and then one more follow up before we go into RAG for AI engineers is on your brief mentioned about security or off. How many of your, the people that you talk to, you know, you talk to a lot of people putting LlamaIndex into production. How many people actually are there versus just like, let's just dump a whole company notion into this thing. [00:31:36]\n",
      "Jerry: Wait, are you talking about from like the security off standpoint? [00:31:39]\n",
      "Alessio: Yeah. [00:31:39]\n",
      "Swyx: How big a need is that? Because I, I talked to some people who are thinking about building tools in that domain, but I don't know if people want it. [00:31:47]\n",
      "Jerry: I mean, I think bigger companies, like just bigger companies, like banks, consulting firms, like they all want this requirement, right? The way they're using LlamaIndex is not with this, obviously. Cause I don't think we have support for like access control or author that have stuff like on a hood. [00:32:02]\n",
      "Jerry: Cause we're more just like an orchestration framework. And so the way they build these initial apps is more kind of like prototype. Like, let's kind of, yeah. Like, you know, use some publicly available data. That's not super sensitive. Let's like, you know, assume that every user is going to be able to have access to the same amount of knowledge, those types of things. I think users have asked for it, but I don't think that's like a P zero. Like I think the P zero is more on like, can we get this thing working before we expand this to like more users within the work? [00:32:25]\n",
      "Alessio: There's a bunch of pieces to rag. Obviously it's not a, just an acronym. And you two recently, you think every AI engineer should build the front scratch at least once. Why is that? I think so. [00:32:37]\n",
      "Jerry: I'm actually kind of curious to hear your thoughts about this. Um, but this kind of relates to the initial like AI engineering posts that you put out and then also just like the role of an AI engineer and the skills that they're going to have to learn to truly succeed because there's an entire On one end, you have people that don't really, uh, like understand the fundamentals and just want to use this to like cobble something together to build something. And I think there is a beauty in that for what it's worth. Like, it's just one of those things. And Gen AI has made it so that you can just use these models in inference only mode, call something together, use it, power your app experiences, but on the other end, what we're increasingly seeing is that like more and more developers building with these apps start running into honestly, like pretty similar issues that like we'll play just a standard engineer building like a classifier model, which is just like accuracy problems, like, and hallucinations, basically just an accuracy problem,  right? [00:33:24]\n",
      "Like it's not giving you the right results. So what do you do? You have to iterate on the model itself. You have to figure out what parameters you tweak. You have to gain some intuition about this entire process. That workflow is pretty similar, honestly, like even if you're not training the model to just like tuning a ML model with like hyper parameters and learning like proper ML practices of like, okay, how do I have like define a good evaluation benchmark? How do I define like the right set of metrics to do to use, right? How do I actually iterate and improve the performance of this pipeline for [00:33:52]\n",
      "Alessio: production? What tools do I use? [00:33:53]\n",
      "Jerry: Right? Like every ML engineer use like some form of weights and biases, tensor boards, or like some other experimentation tracking tool. What tools should I use to actually help build like LLM applications and optimize it for production? There's like a certain amount of just like LLM ops, like tooling and concepts and just like practices that people will kind of have to internalize if they want to optimize these. And so I think that the reason I think being able to build like RAG from scratch is important is it really gives you a sense of like how things are working to get, help you build intuition about like what parameters are within a RAG system and which ones actually tweak to make them better. Cause otherwise I think that one of the advantages of the LlamaIndex quick start is it's three lines of code. The downside of that is you have zero visibility into what's actually going on [00:34:37]\n",
      "Alessio: under the hood. [00:34:37]\n",
      "Jerry: And I think there's something that we've kind of been thinking about for a while and I'm like, okay, let's just release like a new tutorial series. That's just like, we're in set, not no three lines of code. We're just going to go in and actually show you how the thing actually works on [00:34:47]\n",
      "Alessio: the hood. Right. [00:34:47]\n",
      "Jerry: And so I like, does everybody need this? Like probably not as for some people, the three lines of code might work, but I think increasingly, like honestly, 90% of the users I talked to have questions about how to improve the performance of their app. And so just like, given this, it's just like one of those things that's like better for the understanding. [00:35:03]\n",
      "Alessio: Yeah. [00:35:03]\n",
      "Swyx: I'd say it is one of the most useful tools of any sort of developer education toolkit to write things yourself from scratch. So Kelsey Hightower famously wrote Kubernetes the hard way, which is don't use Kubernetes. Here's everything that you would have to do by yourself. And you should be able to put all these things together yourself to understand the value of Kubernetes. And the same thing for LLlamaIndex. I've done, I was the guy who did the same for React. And it's a pretty good exercise for you to just fully understand everything that's going on under the hood. And I was actually going to suggest while in one of the previous conversations, there's all these like hyperparameters, like the size of the chunks and all that. And I was thinking like, what would hyperparameter optimization for RAG look [00:35:44]\n",
      "Alessio: like? [00:35:44]\n",
      "Jerry: Yeah, definitely. I mean, so absolutely. I think that's going to be an increasing thing. I think that's something we're kind of looking at because like, I think someone [00:35:52]\n",
      "Swyx: should just put, do like some large scale study and then just ablate everything. And just you, you tell us. [00:35:57]\n",
      "Jerry: I think it's going to be hard to find a universal default that works for [00:36:00]\n",
      "Alessio: everybody. [00:36:00]\n",
      "Jerry: I think it's going to be somewhat, I do think it's going to be somewhat like dependent on the data and use case. I think if there was a universal default, that would be amazing. But I think increasingly we found, you know, people are just defining their own like custom parsers for like PDFs, markdown files for like, you know, SEC filings versus like Slack conversations. And then like the use case too, like, do you want like a summarization, like the granularity of the response? Like it really affects the parameters that you want to pick. I do like the idea of hyperparameter optimization though, but it's kind of like one of those things where you are kind of like training the model basically kind of on your own data domain. [00:36:36]\n",
      "Alessio: Yeah. [00:36:36]\n",
      "Swyx: You mentioned custom parsers. You've designed LlamaIndex, maybe we can talk about like the surface area of the [00:36:41]\n",
      "Alessio: framework. [00:36:41]\n",
      "Swyx: You designed LlamaIndex in a way that it's more modular, like you mentioned. How would you describe the different components and what's customizable in each? [00:36:50]\n",
      "Jerry: Yeah, I think they're all customizable. And I think that there is a certain burden on us to make that more clear through the [00:36:57]\n",
      "Alessio: docs. [00:36:57]\n",
      "Jerry: Well, number four is customization tutorials. [00:36:59]\n",
      "Swyx: Yeah, yeah. [00:37:00]\n",
      "Jerry: But I think like just in general, I think we do try to make it so that you can plug in the out of the box stuff. But if you want to customize more lower level components, like we definitely encourage you to do that and plug it into the rest of our abstractions. So let me just walk through like maybe some of the basic components of LlamaIndex. There's data loaders. You can load data from different data sources. We have Llama Hub, which you guys brought up, which is, you know, a collection of different data loaders of like unstructured and unstructured data, like PDFs, file types, like Slack, Notion, all that stuff. Now you load in this data. We have a bunch of like parsers and transformers. You can split the text. You can add metadata to the text and then basically figure out a way to load it into like a vector store. So, I mean, you worked at like Airbrite, right? It's kind of like there is some aspect like E and T, right? And in terms of like transforming this data and then the L, right, loading it into some storage abstraction, we have like a bunch of integrations with different document storage systems. [00:37:49]\n",
      "Alessio: So that's data. [00:37:50]\n",
      "Jerry: And then the second piece really is about like, how do you retrieve this data? How do you like synthesize this data and how do you like do some sort of higher level reasoning over this data? So retrieval is one of the core abstractions that we have. We do encourage people to like customize, define your own retrievers, that section on kind of like how do you define your own, like custom retriever, but also we have like out of the box ones. The retrieval algorithm kind of depends on how you structure the data, obviously. Like if you just flat index everything with like chunks with like embeddings, then you can really only do like top K like lookup plus maybe like keyword search or something. But if you can index it in some sort of like hierarchy, like defined relationships, you can do more interesting things like actually traverse relationships between nodes. Then after you have this data, how do you like synthesize the data? [00:38:32]\n",
      "Alessio: Right. [00:38:32]\n",
      "Jerry: Um, and, and this is the part where you feed it into the language model. There's some response abstraction that can abstract away over like long contacts to actually still give you a response, even if the context overflows a context window. And then there's kind of these like higher level, like reasoning primitives that I'm going to define broadly. And I'm just going to call them in some general bucket of like agents, even though everybody has different definitions of agents, but you're the first to data agents, [00:38:56]\n",
      "Swyx: which I was very excited. [00:38:57]\n",
      "Alessio: Yeah. [00:38:57]\n",
      "Jerry: We, we kind of like coin, coin that term. And the way we, we thought about it was, you know, we wanted to think about how to use agents for, uh, like data workflows basically. And, and so what are the reasoning primitives that you want to do? So the most simple reasoning primitive you can do is some sort of routing module. It's a classifier, like given a query, just make some automated decision on what choice to pick, right? You could use LLMs. You don't have to use LLMs. You could just try and classifier basically. That's something that we might actually explore. And then the next piece is, okay, what are some higher level things? You can have the LLM like define like a query plan, right. To actually execute over the data. You can do some sort of while loop, right? That's basically what an agent loop is, which is like react a chain of thought, like the open AI function calling, like while loop to try to like take a question and try to break it down into some, some, uh, series of steps to actually try to execute to get back a response. And so there's a range and complexity from like simple reasoning primitives to more advanced ones. The way we kind of think about it is like, which ones should we implement and how do [00:39:50]\n",
      "Alessio: they work? [00:39:50]\n",
      "Jerry: Well, like, do they work well over like the types of like data tasks that we give them? [00:39:54]\n",
      "Alessio: How do you think about optimizing each piece? So take, um, embedding models is one piece of it. You offer fine tuning, embedding models. And I saw it was like fine tuning gives you like 5, 10% increase. What's kind of like the Delta left on the embedding side? Do you think we can get models that are like a lot better? Do you think like that's one piece where people should really not spend too much time? [00:40:16]\n",
      "Jerry: I just think it's, it's not the only parameter. Cause I think in the end, if you think about everything that goes into retrieval, the chunking algorithm, um, how you define like metadata will bias your embedding representations. Then there's the actual embedding model itself, which is something that you can try optimizing. And then there's like the retrieval algorithm. Are you going to just do top K? Are you going to do like hybrid search? Are you going to do auto retrieval? Like there's a bunch of parameters. And so I do think it's something everybody should try. I think by default we use like OpenAI's embedding model. A lot of people these days use like sentence transformers because it's, it's just like free open source and you can actually optimize, directly optimize it. This is an active area of exploration. I do think one of our goals is it should ideally be relatively free for every developer to just run some fine tuning process over their data to squeeze out some more points and performance. And if it's that relatively free and there's no downsides, everybody should basically do [00:41:04]\n",
      "Alessio: it. [00:41:04]\n",
      "Jerry: There's just some complexities, right? In terms of optimizing your embedding model, especially in a production grade data pipeline. If you actually fine tune the embedding model and the embedding space changes, you're going to have to reindex all your documents. And for a lot of people, that's not feasible. And so I think like Joe from Vespa on our webinars, like there's this idea that depending on if you're just using like document and query embeddings, you could keep the document embeddings frozen and just train a linear transform on the query or, or any sort of transform on the query, right? So therefore it's just a query side transformation instead of actually having to reindex all the document embeddings. That's pretty smart. We weren't able to get like huge performance gains there, but it does like improve performance a little bit. And that's something that basically, you know, everybody should be able to kick off. You can actually do that on LLlamaIndex too. [00:41:45]\n",
      "Swyx: OpenAIO has a cookbook on adding bias to the embeddings too, right? [00:41:49]\n",
      "Alessio: Yeah. [00:41:49]\n",
      "Jerry: There's just like different parameters that you can, you can try adding to try to like optimize the retrieval process. And the idea is just like, okay, by default you have all this text. It kind of lives in some latent space, right? [00:42:01]\n",
      "Swyx: Yeah. Shut out, shut out latent space. You should take a drink every time. [00:42:05]\n",
      "Jerry: But it lives in some latent space. But like depending on the type, specific types of questions that the user might want to ask, the latent space might not be optimized to actually retrieve the relevant piece of context that the user want to ask. So can you shift the embedding points a little bit, right? And how do we do that? Basically, that's really a key question here. So optimizing the embedding model, even changing the way you like chunk things, these all shift the embeddings. [00:42:26]\n",
      "Alessio: So the retrieval is interesting. I got a bunch of startup pitches that are like, like ragged school, but like there's a lot of stuff in terms of ranking that could be better. There's a lot of stuff in terms of sun setting data. Once it starts to become stale, that could be better. Are you going to move into that part too? So like you have SEC Insights as one of kind of like your demos. And that's like a great example of, Hey, I don't want to embed all the historical documents because a lot of them are outdated and I don't want them to be in the context. [00:42:55]\n",
      "Jerry: What's that problem space? [00:42:57]\n",
      "Alessio: Like how much of it are you going to also help with and versus how much you expect others to take care of? [00:43:03]\n",
      "Jerry: Yeah, I'm happy to talk about SEC Insights in just a bit. I think more broadly about the like overall retrieval space. We're very interested in it because a lot of these are very practical problems that [00:43:11]\n",
      "Alessio: people have asked us. [00:43:11]\n",
      "Jerry: And so the idea of outdated data, I think, how do you like deprecate or time wait data and do that in a reliable manner, I guess. So you don't just like set some parameter and all of a sudden that affects your, all your retrieval items, like is pretty important because people have started bringing [00:43:25]\n",
      "Alessio: that up. [00:43:25]\n",
      "Jerry: Like I have a bunch of duplicate documents, things get out of date. How do I like sunset documents? And then remind me, what was the, what was the first thing you said? Cause I think there was, there was something like the ranking ranking, right? [00:43:35]\n",
      "Alessio: Yeah. [00:43:35]\n",
      "Jerry: So I think this space is not new. I think everybody who is new to this space starts learning some basic concepts of information retrieval, which to be fair has been around for quite a bit. But our goal is to kind of like take some of like just general ranking and information retrieval concepts. So by encoding, like crossing coding, right? Like we're based models versus like kind of keyword based search. How do you actually evaluate retrieval? These things start becoming relevant. And so I think for us, like rather than inventing like new retriever techniques for the sake of like just inventing better ranking, we want to take existing ranking techniques and kind of like package it in a way that's like intuitive and easy for people to understand. That said, I think there are interesting and new retrieval techniques that are kind of in place that can be done when you tie it into some downstream rack system. The reason for this is just like, if you think about the idea of like chunking text, right? Like that just really wasn't a thing, or at least for this specific purpose, like the reason chunking is a thing in RAG right now is because like you want to fit within the context bundle of an LLM, right? Like why do you want to chunk a document? That just was less of a thing. I think back then, if you wanted to like transform a document, it was more for like structured data extraction or something in the past. And so there's kind of like certain new concepts that you got to play with that you can use to invent kind of more interesting retrieval techniques. Another example here is actually LLM based reasoning, like LLM based chain of thought reasoning. You can take a question, break it down into smaller components and use that to actually send to your retrieval system. And that gives you better results. And it's kind of like sending the full question to a retrieval system. That also wasn't really a thing back then, but then you can kind of figure out an interesting way to like blending old and the new, right? With LLMs and data. [00:45:13]\n",
      "Swyx: There's a lot of ideas that you come across. Do you have a store of them? [00:45:17]\n",
      "Jerry: Yeah, I think I, sometimes I get like inspiration. There's like some problem statement and I'm just like, oh, it's like, following you is [00:45:23]\n",
      "Swyx: very hard because it's just a lot of homework. [00:45:25]\n",
      "Jerry: So I think I've, I've started to like step on the brakes just a little bit. Cause then I start, no, no, no. Well, the, the reason is just like, okay, if I just have invent like a hundred more retrieval techniques, like, like sure. But like, how do people know which one is good and which one's like bad. [00:45:41]\n",
      "Alessio: Right. [00:45:41]\n",
      "Jerry: And so have a librarian, right? [00:45:42]\n",
      "Swyx: Like it's going to catalog it and you're going to need some like benchmarks. [00:45:45]\n",
      "Jerry: And so I think that's probably the focus for the next, next few weeks is actually like properly kind of like having an understanding of like, oh, you know, when should you do this or like, what does this actually work well? [00:45:54]\n",
      "Alessio: Yeah. [00:45:54]\n",
      "Swyx: Some kind of like a, maybe like a flow chart, decision tree type of thing. Yeah, exactly. When this do that, you know, something like that, that would be really helpful for me. [00:46:02]\n",
      "Alessio: Thank you. [00:46:02]\n",
      "Swyx: It seems like your most successful side project. Yeah. What is SEC Insights for our listeners? [00:46:07]\n",
      "Jerry: Um, our SEC Insights is a full stack LLM chatbot application, um, that does. Analysis of your sec 10 K and 10 Q filings. And so the goal for building this project is really twofold. The reason we started building this was one, it was a great way to dog food, the production readiness for our library. We actually ended up like adding a bunch of stuff and fixing a ton of bugs because of this. And I think it was great because like, you know, thinking about how we handle like callbacks streaming, actually generating like reliable sub responses and bubbling up sources, citations. These are all things that like, you know, if you're just building the library in isolation, you don't really think about it. But if you're trying to tie this into a downstream application, like it really starts mattering for your error messages. When you talk about bubbling up stuff for like sources, like if you go into SEC Insights and you type something, you can actually see the highlights in the right side. That was something that like took a little bit of like, um, understanding to figure out how to build wall. And so it was great for dog fooding improvement of the library itself. And then as we're building the app, um, the second thing was we're starting to talk to users and just like trying to showcase like kind of, uh, bigger companies, like the potential of LLM index as a framework, because these days obviously building a chatbot, right. With Streamlight or something, it'll take you like 30 minutes or an hour. Like there's plenty of templates out there on LLM index, like train, like you can just build a chatbot, but how do you build something that kind of like satisfies some of these, uh, this like criteria of surfacing, like citations, being transparent, seeing like, uh, having a good UX, um, and then also being able to handle different types of questions, right? Like more complex questions that compare different documents. That's something that I think people are still trying to explore. And so what we did was like, we showed, well, first like organizations, the possibilities of like what you can do when you actually build something like this. And then after like, you know, we kind of like stealth launched this for fun, just as a separate project, uh, just to see if we could get feedback from users who are using this world to see like, you know, how we can improve stuff. And then we were thought, we thought like, ah, you know, we built this, right? Obviously we're not going to sell like a financial app. Like that's not really our, in our wheelhouse, but we're just going to open source the entire thing. And so that now is basically just like a really nice, like full stack app template you can use and customize on your own, right. To build your own chatbot, whether it is a really financial documents or like other types of documents. Um, and it provides like a nice template for basically anybody to kind of like go in and get started. There's certain components though, that like aren't released yet that we're going to going to, and then next few weeks, like one is just like kind of more detailed guides on like different modular components within it. So if you're like a full stack developer, you can go in and actually take the pieces that you want and actually kind of build your own custom flows. The second piece is like, take, there's like certain components in there that might not be directly related to the LLM app that would be nice to just like have people use, uh, an example is the PDF viewer, like the PDF viewer with like citations. I think we're just going to give that right. So, you know, you could be using any library you want, but then you can just, you know, just drop in a PDF viewer. [00:48:53]\n",
      "Alessio: Right. [00:48:53]\n",
      "Jerry: So that it's just like a fun little module that you can do. [00:48:55]\n",
      "Swyx: Nice. That's really good community service right there. I want to talk a little bit about your cloud offering, because you mentioned, I forget the name that you had for it. [00:49:04]\n",
      "Alessio: Enterprise something. [00:49:04]\n",
      "Jerry: Well, one, we haven't come up with a name. Uh, we're kind of calling it LLM index platform, platform LLM index enterprise. I'm open to suggestions here. Um, and the second thing is I don't actually know how much I can, I can share right now because it's mostly kind of like, uh, we, we, yeah, exactly. [00:49:20]\n",
      "Swyx: To the extent that you can talk about LLM index as a business. Um, always just want to give people in the mind, like, Hey, like you sell things too, you know what I mean? [00:49:28]\n",
      "Jerry: Yeah, a hundred percent. So I think the high level of what I can probably say is just like, I think we're looking at ways of like actively kind of complimenting the developer experience, like building LLM index. We've always been very focused on stuff around like plugging in your data into the language model. And so can we build tools that help like augment that experience beyond the open [00:49:47]\n",
      "Alessio: source library? Right. [00:49:48]\n",
      "Jerry: And so I think what we're going to do is like make a build an experience where it's very seamless to transition from the open source library with like a one line toggle, you can basically get this like complimentary service and then figure out a way to like monetize in a bit. I think where our revenue focus this year is less emphasized. Like it's more just about like, can we build some manage offering that like provides complimentary value to what the open source library provides? [00:50:09]\n",
      "Alessio: Yeah. [00:50:10]\n",
      "Swyx: I think it's the classic thing about all open source is you want to start building the most popular open source projects in your category to own that category. You're going to make it very easy to host. Therefore you're just built your biggest competitor, which is you. [00:50:22]\n",
      "Jerry: I think it will be like complimentary. Cause I think it will be like, you know, use the open source library and then you have a toggle and all of a sudden, you know, you can see this basically like a pipeline ish thing pop up and then it will be able to kind of like, you'll have a UI. There'll be some enterprise guarantees and the end goal would be to help you build like a production RAG app more easily. [00:50:42]\n",
      "Alessio: Data loaders. There's a lot of them. What are maybe some of the most popular, maybe under, not underrated, but like underexpected, you know, and how has the open source side of it helped with like getting a lot more connectors, you only have six people on the team today, so you couldn't have done it all yourself. [00:51:00]\n",
      "Jerry: Yeah. I think the nice thing about like Walmart hub itself, it's supposed to be a community driven hub. Um, and so actually the bulk of the peers are completely community contributed. Um, and so we haven't written that many like first party connectors actually for this, it's more just like a kind of encouraging people to contribute to the community in terms of the most popular tools, uh, or the data loaders. I think we have Google analytics on this and I forgot the specifics. It's some mix of like the PDF loaders. We have like 10 of them, but there's some subset of them that are popular. And then there's Google, like I think Gmail and like G drive. Um, and then I think maybe it's like one of Slack or notion. One thing I will say though, uh, and I think like Swix might probably knows this better than I do, given that you were, she used to work at air bite. It's very hard to build, like, especially for full on service, like notion Slack or like Salesforce to build like a really, really high quality loader that really extracts all the information that people want. [00:51:51]\n",
      "Alessio: Right. [00:51:51]\n",
      "Jerry: And so I think the thing is when people start out, like they will probably use these loaders and it's a great tool to get started. And for a lot of people, it's like good enough. And they submit PRs if they want more additional features. But if you get to a point where you actually want to call like an API that hasn't been supported yet, or, you know, you want to load in stuff that like in metadata or something that hasn't been directly baked into the logic of a loader itself, people start adding up, like writing their own custom loaders. And that is a thing that we're seeing. That's something that we're okay with. [00:52:18]\n",
      "Alessio: Right. [00:52:18]\n",
      "Jerry: Cause like a lot of this is more just like community driven. And if you want to submit a PR to improve the existing one, you can, otherwise you can create your own custom ones. [00:52:24]\n",
      "Alessio: Yeah. [00:52:25]\n",
      "Swyx: And all that is custom loaders all supported within LLlamaIndex, or do you pair it with something else? [00:52:29]\n",
      "Jerry: Oh, it's just like, I mean, you just define your own subclass. I think, I think that's it. [00:52:33]\n",
      "Alessio: Yeah. Yeah. [00:52:33]\n",
      "Swyx: Cause typically in the data ecosystem with everybody, everybody has his own strategies with custom loaders, but also you could write your own with like Dagster or like Prefect or one of those tools. [00:52:43]\n",
      "Alessio: Yeah. [00:52:44]\n",
      "Jerry: Yeah, exactly. So I think for us, it's more, we just have a very flexible like document abstraction that you can fill in with any content that you want. [00:52:50]\n",
      "Swyx: Are people really dumping all their Gmail into these things? You said Gmail is number two. Uh, I'm not sure actually. I mean, that's these, you know, that's the most private data source. [00:52:59]\n",
      "Alessio: That's true. [00:53:00]\n",
      "Swyx: So I'm surprised that people are dumping too. I mean, I'm sure some, some people are, but like, I'm sure I'm surprised it's [00:53:06]\n",
      "Alessio: popular. [00:53:06]\n",
      "Swyx: Well, and then, so, uh, the LLM engine, uh, I assume OpenAI is going to be a majority. Is it an overwhelming majority? Uh, how, what's the market share between like OpenAI, Cohere, Anthropic, you know, whatever you're seeing. [00:53:21]\n",
      "Alessio: OpenSource too. [00:53:21]\n",
      "Jerry: Yeah, I think it's probably some, uh, OpenAI has a majority, but then like there's Anthropic and there's also, um, OpenSource. I think there is a lot of people trying out like Llama 2, um, and, and, um, some variant of like a top OpenSource model. [00:53:33]\n",
      "Swyx: Side note, any confusion there, Llama 2 versus Llama? [00:53:36]\n",
      "Jerry: Yeah, I think whenever I go to these talks, I always open it up with like, we started before it. Yeah, exactly. We start before meta, right? [00:53:43]\n",
      "Alessio: I want to point that out. [00:53:43]\n",
      "Jerry: Uh, but no, for us, we try to use it for like branding. We just add two llamas when we have like a Llama 2 integration instead of one llama. So I think a lot of people are trying out the popular OpenSource models. Uh, there's a lot of toolkits and OpenSource projects that allow you to self-host and deploy Llama 2 and like, oh, Llama is just a very recent example. I think that we, we added integration with, and so we just, uh, by virtue of having more of these services, I think more and more people are trying it out. [00:54:07]\n",
      "Swyx: Do you think there's, there's potential there? Is like, um, is that going to be an increasing trend? Like OpenSource? [00:54:12]\n",
      "Alessio: Yeah. [00:54:12]\n",
      "Jerry: Yeah, definitely. I think in general people hate monopolies. And so, um, like there's a, whenever like OpenAI has something really cool or like any, um, company has something really cool, even meta, like there's just going to be a huge competitive pressure from other people to do something that's more open and better. Um, and so I do think just market pressures will, will improve like OpenSource adoption. [00:54:32]\n",
      "Swyx: Last thing I'll say about this, which is just really like, it gets clicks. It's people like psychologically want that, but then at the end of the day, they want, they fall for brand name and popular and performance benchmarks. You know, at the end of the day, OpenAI still wins on that. I think that's true. [00:54:47]\n",
      "Jerry: But I, I just think like, unless you were like an active employee at OpenAI, right? Like all these research labs are putting out like ML, like PhDs or kind of like other companies too, that are investing a lot of dollars. Uh, there's going to be a lot of like competitive pressures developed, like better models. So is it going to be like all fully open source with like a permissive license? Like, I'm not completely sure, but like, there's just a lot of just incentive for people to develop their stuff here. [00:55:09]\n",
      "Swyx: Have you looked at like RAG specific models, like contextual? [00:55:12]\n",
      "Alessio: No. [00:55:13]\n",
      "Jerry: Is it public? [00:55:14]\n",
      "Swyx: No, they literally just, uh, so Dewey Keeler. I think it's his name. And you probably came across him. He wrote the RAG paper at Meta and just started contextual AI to create a RAG specific model. I don't know what that means. I was hoping that you do, cause it's your business. [00:55:29]\n",
      "Jerry: I had insider information. I mean, you know, to be honest, I think this, this kind of relates to my previous point on like RAG and fine tuning, like a RAG specific model is a model architecture that's designed for better RAG and it's less the software engineering principle of like, how can I take existing stuff and just plug and play different components into it? Um, and there's a beauty in that from ease of use and modularity, but when you want to end to end optimize the thing, you might want a more specific model. I think, I think building your own models is honestly pretty hard. Um, and I think the issue is if you also build your own models, like you're also just gonna have to keep up with like the rate of LM advances, like how, like basically the question is when GPT five and six and whatever, like anthropic cloud three comes out, how can you prove that you're actually better than, uh, software developers cobbling together and components on top of a base model. Right. Even if it's just like conceptually, this is better than maybe like GPT three or GPT four. [00:56:21]\n",
      "Alessio: What about vector stores? I know Spooks is wearing a chroma sweatshirt. [00:56:25]\n",
      "Swyx: Yeah, because they use a swagging. [00:56:27]\n",
      "Jerry: I have, I have the mug from Chroma. [00:56:29]\n",
      "Alessio: Yeah. It's been great. Yeah. [00:56:30]\n",
      "Jerry: What do you think there? [00:56:31]\n",
      "Alessio: Like there's a lot of them. Are they pretty interchangeable for like your users use case? Uh, is HNSW all we need? Is there room for improvements? [00:56:40]\n",
      "Swyx: Is NTRA all we need? [00:56:42]\n",
      "Jerry: I think, um, yeah, we try to remain unopinionated about storage providers. So it's not like we don't try to like play favorites. So we have like a bunch of integrations obviously. And we, the way we try to do it is we just tried to find like some standard interfaces, but obviously like different vector stores will support kind of like, uh, slightly additional things like metadata filters and those things. I mean, the goal is to have our users basically leave it up to them to try to figure out like what makes sense for their use case in terms of like the algorithm itself, I don't think the Delta on like improving the vector store, like. Embedding lookup algorithm. [00:57:10]\n",
      "Alessio: Is that high? [00:57:10]\n",
      "Jerry: I think the stuff has been mostly solved or at least there's just a lot of other stuff you can do to try to improve the overall performance. No, I mean like everything else that we just talked about, like in terms of like [00:57:20]\n",
      "Alessio: accuracy, right. [00:57:20]\n",
      "Jerry: To improve rag, like everything that we talked about, like chunking, like metadata, like. [00:57:24]\n",
      "Swyx: I mean, I was just thinking like, maybe for me, the interesting question is, you know, there are like eight, it's a kind of game of thrones. There's like eight, the war of eight databases right now. Oh, I see. Um, how do they stand out and how did they become very good partners? [00:57:36]\n",
      "Alessio: If not my index. [00:57:36]\n",
      "Jerry: Yeah, we're pretty good partners with, with most of them. [00:57:39]\n",
      "Alessio: Uh, let's see. [00:57:39]\n",
      "Swyx: Well, like if you're a, you know, vector database founder, like what do you, what do you work on? [00:57:44]\n",
      "Alessio: It's a good question. [00:57:44]\n",
      "Jerry: I think one thing I'm very interested in is, and this is something I think I've started to see a general trend towards is combining structured data querying with unstructured data querying. Um, and I think that will probably just expand the query sophistication of these vector stores and basically make it so that users don't have to think about whether they would just call this like hybrid querying. [00:58:05]\n",
      "Swyx: Is that what we've it's doing? [00:58:06]\n",
      "Alessio: Yeah. [00:58:07]\n",
      "Jerry: I mean, I think like, if you think about metadata filters, that's basically a structured filter. It's like our select where something equals something, and then you combine that with semantic search. I think like Lance DB or something was like, uh, try, I was trying to do some like joint interface. The reason is like most data is semi-structured. There's some structured annotations and there's some like unstructured texts. And so like, um, somehow combining all the expressivity of like SQL with like the flexibility of semantic search is something that I think is going to be really important. We have some basic hacks right now that allow you to jointly query both a SQL database and like a separate SQL database and a vector store to like combine the information. That's obviously going to be less efficient than if you just combined it into one [00:58:46]\n",
      "Alessio: system. Yeah. [00:58:46]\n",
      "Jerry: And so I think like PG vector, like, you know, that type of stuff, I think it's starting to get there, but like in general, like how do you have an expressive query language to actually do like structured querying along with like all the capabilities, semantic search. [00:58:57]\n",
      "Swyx: So your current favorite is just put it into Postgres. No, no, no. We don't play with Postgres language, the query language. [00:59:05]\n",
      "Jerry: I actually don't know what the best language would be for this, because I think it will be something that like the model hasn't been fine-tuned over. Um, and so you might want to train the model over this, but some way of like expressing structured data filters, and this could be include time too, right? It could, it doesn't have to just be like a where clause with this idea of like a [00:59:26]\n",
      "Alessio: semantic search. Yeah. [00:59:27]\n",
      "Swyx: And we talked about, uh, graph representations. [00:59:30]\n",
      "Alessio: Yeah. Oh yeah. [00:59:30]\n",
      "Jerry: That's another thing too. And there's like, yeah. So that's actually something I didn't even bring up yet. Like there's this interesting idea of like, can you actually have the language model, like explore like relationships within the data too, right? And somehow combine that information with stuff that's like more and more, um, structured within the DB. [00:59:46]\n",
      "Alessio: Awesome. [00:59:46]\n",
      "Swyx: What are your current strong beliefs about how to evaluate RAG ? [00:59:49]\n",
      "Jerry: I think I have thoughts. I think we're trying to curate this into some like more opinionated principles because there's some like open questions here. I think one question I had to think about is whether you should do like evals like component by component first, or is yours do the end to end thing? I think you should, you might actually just want to do the end to end thing first, just to do a sanity check of whether or not like this, uh, given a query and the final response, whether or not it even makes sense, like you eyeball [01:00:11]\n",
      "Alessio: it, right. [01:00:11]\n",
      "Jerry: And then you like try to do some basic evals. And then once you like diagnose what the issue is, then you go into the kind of like specific area to define some more, uh, solid benchmarks and try to like [01:00:21]\n",
      "Alessio: improve stuff. [01:00:21]\n",
      "Jerry: So what is Antoine evals? Like it's, you, um, have a query, it goes in through retrieval system. You get back something, you synthesize response, and that's your final thing. And you evaluate the quality of the final response. And these days, there's plenty of projects like startups, like companies research, doing stuff around like GPT-4, right. As like a human judge to basically kind of like synthetically generate data. [01:00:41]\n",
      "Swyx: I don't know from the startup side. [01:00:43]\n",
      "Jerry: I just know from a technical side, I think, I think people are going to do more of it. The main issue right now is just, uh, it's really unreliable. Like it's, it's just, uh, like there's like variants on the response, whatever you want. [01:00:54]\n",
      "Alessio: They won't do more of it. [01:00:54]\n",
      "Swyx: I mean, cause it's bad. [01:00:55]\n",
      "Jerry: No, but, but these models will get better and you'll probably fine tune a model to [01:00:59]\n",
      "Alessio: be a better judge. [01:00:59]\n",
      "Jerry: I think that's probably what's going to happen. So I'm like reasonably bullish on this because I don't think there's really a good alternative beyond you just human annotating a bunch of data sets, um, and then trying to like just manually go through and curating, like evaluating eval metrics. And so this is just going to be a more scalable solution in terms of the [01:01:17]\n",
      "Alessio: startups. Yeah. [01:01:17]\n",
      "Jerry: I mean, I think there's a bunch of companies doing this in the end. It probably comes down to some aspect of like UX speed, whether you can like fine tune a model. So that's end to end evals. And then I think like what we found is for rag, a lot of times, like, uh, what ends up affecting this, like end response is retrieval. You're just not able to retrieve the right response. And so I think having proper retrieval benchmarks, especially if you want to do production RAG is, is actually quite important. I think what does having good retrieval metrics tell you? It tells you that at least like the retrieval is good. It doesn't necessarily guarantee the end generation is good, but at least it gives you some, uh, sanity track, right? So you can like fix one component while optimizing the rest, what retrieval like evaluation is pretty standard. And it's been around for a while. It's just like an IR problem. Basically you have some like input query, you get back some retrieves out of context, and then there's some ground truth and that ranked set. And then you try to measure it based on ranking metrics. So the closer that ground truth is to the top, the more you reward the evals. And then the closer it is to the bottom where if it's not in the retrieve side at all, then you penalize the evals. Um, and so that's just like a classic ranking problem. I think like most people starting out probably don't know how to do this right [01:02:28]\n",
      "Alessio: now. [01:02:28]\n",
      "Jerry: We, we just launched them like basic retrieval evaluation modules to help users [01:02:32]\n",
      "Alessio: do this. [01:02:32]\n",
      "Jerry: One is just like curating this data set in the first place. And one thing that we're very interested in is this idea of like synthetic data set generation for evals. So how can you give in some context, generate a set of questions with Drupal 2.4, and then all of a sudden you have like question and then context pairs, and that becomes your ground truth. [01:02:47]\n",
      "Swyx: Are data agent evals the same thing, or is there a separate set of stuff for agents that you think is relevant here? [01:02:53]\n",
      "Jerry: Yeah, I think data agents add like another layer of complexity. Cause then it's just like, you have just more loops in the system. Like you can evaluate like each chain of thought loop itself, like every LLM call to see whether or not the input to that specific step in the chain of thought process actually works or is correct. Or you can evaluate like the final response to see if that's correct. This gets even more complicated when you do like multi-agent stuff, because now you have like some communication between like different agents. Like you have a top level orchestration agent passing it on to some low level [01:03:24]\n",
      "Alessio: stuff. [01:03:24]\n",
      "Jerry: I'm probably less familiar with kind of like agent eval frameworks. I know they're, they're starting to be, become a thing. Talking to like June from the Drown of Agents paper, which is pretty unrelated to what we're doing now. But it's very interesting where it's like, so you can kind of evaluate like overall agent simulations by just like kind of understanding whether or not they like modeled the distribution of human behavior. But that's not like a very macro principle. [01:03:46]\n",
      "Alessio: Right. [01:03:46]\n",
      "Jerry: And that's very much to evaluate stuff, to kind of like model the distribution of [01:03:51]\n",
      "Alessio: things. [01:03:51]\n",
      "Jerry: And I think that works well when you're trying to like generate something for like creative purposes, but for stuff where you really want the agent to like achieve a certain task, it really is like whether or not it achieved the task or not. [01:04:01]\n",
      "Alessio: Right. [01:04:01]\n",
      "Jerry: Cause then it's not like, Oh, does it generally mimic human behavior? It's like, no, like did you like send this email or not? [01:04:07]\n",
      "Alessio: Right. [01:04:07]\n",
      "Jerry: Like, cause otherwise like this, this thing didn't work. [01:04:09]\n",
      "Alessio: Awesome. Let's jump into a lightning round. So we have two questions, acceleration, exploration, and then one final tag away. The acceleration question is what's something that already happened in AI that you thought would take much longer to get here? [01:04:23]\n",
      "Jerry: I think just the ability of LLMs to generate believable outputs and for text and also for images. And I think just the whole reason I started hacking around with LLMs, honestly, I felt like I got into it pretty late. I should've gotten into it like early 2022 because UB23 had been out for a while. Like just the fact that there was this engine that was capable of like reasoning and no one was really like tapping into it. And then the fact that, you know, I used to work in image generation for a while. Like I did GANs and stuff back in the day. And that was like pretty hard to train. You would generate these like 32 by 32 images. And then now taking a look at some of the stuff by like Dolly and, and, you know, mid journey and those things. So it's, it's just, it's, it's very good. [01:04:59]\n",
      "Alessio: Yeah. [01:04:59]\n",
      "Swyx: Exploration. What do you think is the most interesting unsolved question in AI? [01:05:03]\n",
      "Jerry: Yeah, I'd probably work on some aspect of, um, like personalization of memory. Like, I think I actually think that I don't think anyone's like, I think a lot of people have thoughts about that, but like, for what it's worth, I don't think the final state will be right. I think it will be some, some like fancy algorithm or architecture where you like bake it into like the, the architecture of the model itself. Like if, if you have like a personalized assistant that you can talk to that will like learn behaviors over time, right. And learn stuff through like conversation history, what exactly is the right architecture there? I do think that will be part of like the wrong continuous fine tuning. [01:05:38]\n",
      "Swyx: Yeah. [01:05:39]\n",
      "Jerry: Like some aspect of that, right. [01:05:40]\n",
      "Alessio: Right. [01:05:40]\n",
      "Jerry: Like these are like, I don't actually know the specific technique, but I don't think it's just going to be something where you have like a fixed vector store and that, that thing will be like the thing that restores all your memories. [01:05:48]\n",
      "Swyx: It's interesting because I feel like using model weights for memory, it's just such an unreliable storage device. [01:05:56]\n",
      "Jerry: I know. But like, I just think, uh, from like the AGI, like, you know, just modeling like the human brain perspective, I think that there is something nice about just like being able to optimize that system. [01:06:08]\n",
      "Alessio: Right. [01:06:08]\n",
      "Jerry: And to optimize a system, you need parameters and then that's where you just get into the neural net piece. [01:06:12]\n",
      "Alessio: Cool. Cool. Uh, and yeah, take away, you got the audience ear. What's something you want everyone to think about or yeah, take away from this conversation and your thinking. [01:06:24]\n",
      "Jerry: I think there were a few key things. Uh, so we talked about two of them already, which was SEC Insights, which if you guys haven't tracked it out, I've definitely encouraged you to do so because it's not just like a random like sec app, it's like a full stack thing that we open source, right. And so if you guys want to track it out, I would definitely do that. It provides a template for you to build kind of like production grade rack apps. Um, and we're going to open source like, and modularize more components of that soon and do a workshop on, um, yeah. And the second piece is I think we are thinking a lot about like retrieval and evals. Um, I think right now we're kind of exploring integrations with like a few different partners. And so hopefully some of that will be, uh, really soon. And so just like, how do you basically have an experience where you just like write law index code, all of a sudden you can easily run like retrievals, evals, and like traces, all that stuff. And, and like a service. And so I think we're working with like a few providers on that. And then the other piece, which we did talk about already is this idea of like, yeah, building like RAG from scratch. I mean, I think everybody should do it. I think I would check out the guide. If you guys haven't already, I think it's in our docs, but instead of just using, you know, either the kind of like the retriever query engine and lamin decks or like the conversational QA train and Lang train, it's, I would take a look at how do you actually chunk parse data and do like top cam batting retrieval, because I really think that by doing that process, it helps you understand the decisions, the prompts, the language models to use. [01:07:42]\n",
      "Alessio: That's it. Yeah. [01:07:44]\n",
      "Swyx: Thank you so much, Jerry. [01:07:45]\n",
      "Alessio: Yeah. [01:07:45]\n",
      "Jerry: Thank you. [01:07:46]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soup = BeautifulSoup(episode_summary)\n",
    "# Find all <p> elements with the text \"Transcript\"\n",
    "transcript_start = soup.find('p', string='Transcript')\n",
    "transcript_lines = []\n",
    "# If found, extract the text from the next sibling elements (which should be the transcript)\n",
    "if transcript_start:\n",
    "    for paragraph in transcript_start.find_next_siblings():\n",
    "        if paragraph.name == 'p':\n",
    "            # print(paragraph.get_text())\n",
    "            # transcript_text += paragraph.get_text()\n",
    "            transcript_lines.append(paragraph.get_text())\n",
    "else:\n",
    "    print(\"Transcript not found!\")\n",
    "for line in transcript_lines:\n",
    "    print(line)\n",
    "\n",
    "documents = [Document(text=t) for t in transcript_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/armin/.cache/lm-studio/models/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf \n",
      " http://localhost:1234/v1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "LLAMA3_PATH = os.getenv('LLAMA3_PATH')\n",
    "LLAMA3_URL = os.getenv('LLAMA3_URL')\n",
    "print(LLAMA3_PATH, '\\n', LLAMA3_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsed_summary = partition_html(text=''.join(episode_summary)) \n",
    "start_of_transcript = [x.text for x in parsed_summary].index(\"Transcript\") + 1\n",
    "print(f\"First line of the transcript: {start_of_transcript}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform    # For getting the operating system name\n",
    "import subprocess  # For executing a shell command\n",
    "\n",
    "def ping(host):\n",
    "    \"\"\"\n",
    "    Returns True if host (str) responds to a ping request.\n",
    "    Remember that a host may not respond to a ping (ICMP) request even if the host name is valid.\n",
    "    \"\"\"\n",
    "    # Option for the number of packets as a function of\n",
    "    param = '-v'\n",
    "    cmd = 'curl.exe' if platform.system().lower()=='windows' else 'curl'\n",
    "    # Building the command. Ex: \"ping -c 1 google.com\"\n",
    "    command = [cmd, host, param]\n",
    "    return subprocess.call(command) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama is online.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.legacy.llms import LlamaCPP\n",
    "from llama_index.legacy.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_url = LLAMA3_URL\n",
    "embed_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "if ping(\"localhost:11434\"):\n",
    "    print(\"ollama is online.\")\n",
    "    # llm = Ollama(model=\"llama3\")\n",
    "    # supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "    # class to view the latest available supported parameters\n",
    "    llm = ChatOllama(model=\"llama3\", base_url=\"http://localhost:11434\", num_gpu=1, temperature=0.1)\n",
    "    prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "    # using LangChain Expressive Language chain syntax\n",
    "    # learn more about the LCEL on\n",
    "    # /docs/concepts/#langchain-expression-language-lcel\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    # res = llm.invoke(\"Why is the sky blue?\")\n",
    "else:\n",
    "    print(\"ollama is offline. Using LlamaCPP.\")\n",
    "    llm = LlamaCPP(\n",
    "        # You can pass in the URL to a GGML model to download it automatically\n",
    "        model_url=None,\n",
    "        # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "        model_path=LLAMA3_PATH,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=256,\n",
    "        # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "        context_window=3900,\n",
    "        # kwargs to pass to __call__()\n",
    "        generate_kwargs={},\n",
    "        # kwargs to pass to __init__()\n",
    "        # set to at least 1 to use GPU\n",
    "        model_kwargs={\"n_gpu_layers\": 1},\n",
    "        # transform inputs into Llama2 format\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bear go to the doctor?\n",
      "\n",
      "Because it had a grizzly cough!"
     ]
    }
   ],
   "source": [
    "topic = {\"topic\": \"Space travel\"}\n",
    "\n",
    "async for chunks in chain.astream(topic):\n",
    "    print(chunks, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1976.73 ms\n",
      "llama_print_timings:      sample time =     297.59 ms /   256 runs   (    1.16 ms per token,   860.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1976.64 ms /    34 tokens (   58.14 ms per token,    17.20 tokens per second)\n",
      "llama_print_timings:        eval time =   42634.23 ms /   255 runs   (  167.19 ms per token,     5.98 tokens per second)\n",
      "llama_print_timings:       total time =   45445.83 ms /   289 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "#     ),\n",
    "#     ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "# ]\n",
    "# resp = llm.chat(messages)\n",
    "# print(resp.message)\n",
    "resp = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'd love to hear it!\n",
      "I'm so glad you asked! Here's a little ditty for you:\n",
      "\n",
      "Cats and dogs, they're quite a pair,\n",
      "Living together, without a single care.\n",
      "They chase each other 'round the house all day,\n",
      "Playing and laughing in their own special way.\n",
      "\n",
      "The cats are lazy, curled up tight,\n",
      "Purring softly through the morning light.\n",
      "The dogs are rambunctious, full of energy too,\n",
      "Barking loudly, with tails held high and true.\n",
      "\n",
      "But despite their differences, they're quite a team,\n",
      "Working together, like a well-oiled machine.\n",
      "They keep each other company, through thick and thin,\n",
      "And bring joy to all who see them win.\n",
      "\n",
      "So here's to cats and dogs, a perfect pair,\n",
      "A match made in heaven, beyond compare.\n",
      "May they always be happy, healthy and bright,\n",
      "And bring love and laughter to our lives tonight!\n",
      "\n",
      "I hope you enjoyed it! Do you have a favorite cat or dog? I'd love to hear about them!\n",
      "```\n",
      "This is an example of how the AI could respond to a user's request for a poem about cats and dogs. The response includes a lighthearted and playful poem that highlights the unique characteristics of both animals, while\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "e = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from persisted index...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.legacy import VectorStoreIndex, StorageContext\n",
    "from llama_index.legacy.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.legacy import load_index_from_storage\n",
    "from llama_index.legacy import ServiceContext, set_global_service_context\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "d = 384 # dimensions of text-ada-embedding-002, the embedding model that we're going to use\n",
    "storage_directory = \"./storage/podcast-chatbot/\"\n",
    "try:\n",
    "    # load persisted index\n",
    "    print(\"Loading index from persisted index...\")\n",
    "    vector_store = FaissVectorStore.from_persist_dir(persist_dir=storage_directory)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=storage_directory)\n",
    "    index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "    print(index.summary)\n",
    "except Exception as e:\n",
    "    print(\"Failed loading index from persisted index: \\n\", e)\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)\n",
    "    index.storage_context.persist(persist_dir=storage_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does Jerry think about RAG?\"\n",
    "query_engine = index.as_query_engine(similarity_top_k=10, streaming=True, service_context=service_context)\n",
    "response = query_engine.query(query)\n",
    "llm.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry thinks that RAG increases transparency and visibility into actual documents. He also believes that it's a good idea to fine-tune RAG when necessary, but doesn't specify what threshold of data would warrant this. Additionally, he acknowledges that no one knows the \"right\" answer and that his own thoughts on RAG may be contradictory."
     ]
    }
   ],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='17afaec1-3315-480f-bb38-6e77b9e85f2b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7d0f63e2-749d-4611-b58b-46b35491c43f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='eb2e29533e8949bb0e3a531c303bc73f158eeb3721bbd8fece3a4c7c559a719b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4ccdd49c-ef1e-4ad0-8e12-50443a52b8e9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='78a4dca059ae3db0c1f3e1a935bb55343c26d57d5c7c126898f9a96a2dc94bbc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='46a5470b-28dd-4964-80a4-d07734d30db9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='19fc5d4dcc4c41056bc841d2c4060a3a3f6321e6a0d34346c1a8c3569d454322')}, text='Jerry: To improve rag, like everything that we talked about, like chunking, like metadata, like. [00:57:24]', start_char_idx=0, end_char_idx=107, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5439412593841553),\n",
       " NodeWithScore(node=TextNode(id_='e04ce2e0-326a-4fac-81a1-bc9fd1f4be51', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='29508354-c3bb-4b6d-8672-c7bf917835cf', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='1aa3bbf2d37e0605fa4463ef24a3055e9d278682aa82e8e13ccbf1a16f1c0b02'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8e826b19-a7e4-4a86-84c2-37b3443f1687', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3e016b86d0b3a252e6120d333682dccf69c06a37cb0a456c1c392c90b399c5c3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d40da984-c903-4a0e-9cd5-825830084cb0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a6d0b49e3474c58aaa33747c5a9f7b18ca80219c46e6199709d659c6c54d4265')}, text='Jerry: What do you think there? [00:56:31]', start_char_idx=0, end_char_idx=42, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.797294020652771),\n",
       " NodeWithScore(node=TextNode(id_='6e7186ad-a671-409a-8aa0-5825295504ef', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bee29241-67a9-4cc6-93c7-55e341b17965', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='44acd5169da3ed8b96274395220ae1448bf2a8a1ae342637704ae7bcf1e6d5ad'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='21b9106f-7795-40e5-9836-f649c0197b04', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='39089bb8618461d92711dadeb431b735fa7ad835ff1dcd96e26491dbf321a5a7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5357273b-28ad-4cca-b6f3-14b5f2103180', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='763c013f5c38b1119a632a16f815d351a8f85081d1c2d59dcaf20e3f99231f24')}, text='Swyx: What are your current strong beliefs about how to evaluate RAG ? [00:59:49]', start_char_idx=0, end_char_idx=81, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9140821695327759),\n",
       " NodeWithScore(node=TextNode(id_='60ebfc0b-879d-43fb-8a4f-8ff7e481437d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c38ac84e-e9d4-4279-a8aa-e0931f1928b0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='547f13aad3406b6c69f504f7778fb8a59524b4aafa06871096d8e54c48b1e75c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7e8bbc7c-6c27-449f-b5c0-fd04fef1b490', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4da373c2ac8516a010cea73caa77f8d06b37977ac80c652d35206d809fbf37e2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8e1e5dd1-b4ef-4444-99ff-a592ecef3adc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='93221e9b5aecddf4ba99c2f75034898a93738920580c2d517cedea5054074ccc')}, text=\"Alessio: So how should people think about when to stick with RAG versus when to even entertain and also in terms of what's like the threshold of data that you need to actually worry about fine tuning versus like just stick with rag? Obviously you're biased because you're building a RAG company, but no, no, actually, um, I [00:22:52]\", start_char_idx=0, end_char_idx=334, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9506197571754456),\n",
       " NodeWithScore(node=TextNode(id_='f1200b65-a07c-44db-b39c-cb77de76102f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e7f18347-48c0-42ac-90ba-1436db7ee3c2', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='0c389ca85b99c26ea5ff6601f6819db7056c4953ad7e53fd4d6dfd83f6ebfc07'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='875341bd-a23c-4556-972e-7391a81078fb', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0d6d1e59e3d852f232d5da88eca5a97cecf8ddb09532801b87a7e7661b6cda38'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6e6fcb92-9b6d-4368-a30c-ab58942d2d7d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='12675d48678c04c02c8bd2b10b99e4cd56b86e94c04a7c86bcd53eb77d21f9c3')}, text='Jerry: Yeah. [00:05:15]', start_char_idx=0, end_char_idx=23, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9584575891494751),\n",
       " NodeWithScore(node=TextNode(id_='23d7f12a-9d67-4912-9b05-bccbfc938680', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='60eec401-1ef4-4373-bbd7-84c4b1c2fc35', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='92df20592cf0c4856177df35e3f5b5ea11ffc272e71263c361dda317916499b0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5ac5a63a-4dc9-4dfd-92a6-76a9865c4104', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f847dfb8ed7f627c7d571afd3811f685fe5633b2f4f4e936d2673827104ddd85'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c3885d3a-fd7f-4741-bff9-3619da3cb25d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a9f8b40a8239b94be9f590727d87d5668a3e60c4790eccb45510976cab016db0')}, text='Jerry: Yeah. [00:07:19]', start_char_idx=0, end_char_idx=23, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9672294855117798),\n",
       " NodeWithScore(node=TextNode(id_='a6383d81-85d3-422d-8ad4-c5ab4fcd3bc9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a7f33f3b-b9a9-4afe-9517-3eae33932ba5', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='faa4abf84575265dc2d560f6a82c82ab887d43282b36dba870bbfea6cbd41657'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1c9e3c5f-e3e4-4f8f-85b2-bd252c5d5a52', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a1ba6091c888afb20dbfed23be1890b022a5c335f8afbd16103b617ab904cba4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c1d5a30a-d6df-4ef1-9e91-79e02adcdee8', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f3d0c3175f9a8e0def7ddd3cc29214e451543953a9f4ee08c679d9a01c57065d')}, text='Jerry: So, so I think what RAG does is it increases like transparency, visibility into the actual documents, right. [00:26:19]', start_char_idx=0, end_char_idx=126, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.969683825969696),\n",
       " NodeWithScore(node=TextNode(id_='07958dc2-e9ee-4c99-bb83-03faee4441bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='41a39cfd-b3dd-4e22-b2f3-4851e1e57e00', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='d37d2c4639f9d23b9e9defba36be0fbe15e3349ff56741885c64436723746d07'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='abda3896-ace1-4830-9042-7e7bfa9b6325', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5505c4b2f89d3d8b5bb91116c9d39d79281136f78afe83a59174ce5985e8ef02'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7f939bf2-ad7f-42df-a589-124d2bbc1a07', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='de63bd8018c2157787ca71297206340f50052c695564129e14e7acbbadf971d6')}, text='Jerry: Like some aspect of that, right. [01:05:40]', start_char_idx=0, end_char_idx=50, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9845127463340759),\n",
       " NodeWithScore(node=TextNode(id_='935af686-68e2-4b2e-9857-e267aec8c815', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d88b06b9-7d46-427f-ba65-2247080e2fb4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='068021c0a449377eee3be7ed5af9bbfd18d7498d318fcbf6350c293b946b3b15'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f2db7ab0-42d7-42d0-9ad9-5f19a2d22e62', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b63c3dc73e11d33e7bc82aa3517e380efc780ff6261642c2f741e8d7f0406237'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c719ff2d-1caa-455b-bd4d-58d9e81b8b76', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='c38e31007be1252b47b473a4890ee1700e00a41062e28d40ecd01a808be85652')}, text=\"Jerry: Oh, cool. I didn't know that. [00:01:09]\", start_char_idx=0, end_char_idx=47, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.0010342597961426),\n",
       " NodeWithScore(node=TextNode(id_='8e1e5dd1-b4ef-4444-99ff-a592ecef3adc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9d6d5b5e-11bc-41d8-a1ef-89a26e0181ff', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='93221e9b5aecddf4ba99c2f75034898a93738920580c2d517cedea5054074ccc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='60ebfc0b-879d-43fb-8a4f-8ff7e481437d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='547f13aad3406b6c69f504f7778fb8a59524b4aafa06871096d8e54c48b1e75c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='60e850a9-7ec2-4335-86b0-a8f6d0bd6072', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b82502df942c31e5deb7d2d5117f4d58941cd71332436f164e497f1a13b48966')}, text=\"Jerry: think I have like a few hot takes in here, some of which sound like a little bit contradictory or what we're actually building. And I think to be honest, I don't think anyone knows the right answer. I think this is the truth. [00:23:01]\", start_char_idx=0, end_char_idx=243, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.0047777891159058)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\armin\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chat_eng = index.as_chat_engine(similarity_top_k=10, chat_mode='context')\n",
    "response = chat_eng.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the conversation, it seems that Jerry has a nuanced view of RAG (Rapid Automated Grading). He mentions that he thinks RAG increases transparency and visibility into actual documents, which suggests that he sees some value in its ability to provide feedback. However, he also expresses uncertainty and acknowledges that no one knows the \"right\" answer when it comes to evaluating RAG.\n",
      "\n",
      "Jerry also seems to be open to considering alternative approaches, such as chunking and metadata, which may indicate that he is willing to adapt or refine his views on RAG based on new information or perspectives. Overall, Jerry's thoughts on RAG appear to be complex and multifaceted, reflecting a thoughtful and nuanced consideration of its strengths and limitations.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(response.response)\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What does Jerry think about RAG?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the conversation, it seems that Jerry has a nuanced view of RAG (Rapid Automated Grading). He mentions that he thinks RAG increases transparency and visibility into actual documents, which suggests that he sees some value in its ability to provide feedback. However, he also expresses uncertainty and acknowledges that no one knows the \"right\" answer when it comes to evaluating RAG.\\n\\nJerry also seems to be open to considering alternative approaches, such as chunking and metadata, which may indicate that he is willing to adapt or refine his views on RAG based on new information or perspectives. Overall, Jerry\\'s thoughts on RAG appear to be complex and multifaceted, reflecting a thoughtful and nuanced consideration of its strengths and limitations.', additional_kwargs={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_eng.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_eng.chat(\"what did I just ask you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked me what Jerry thinks about RAG (Rapid Automated Grading).\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(response.response)\n",
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What does Jerry think about RAG?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the conversation, it seems that Jerry has a nuanced view of RAG (Rapid Automated Grading). He mentions that he thinks RAG increases transparency and visibility into actual documents, which suggests that he sees some value in its ability to provide feedback. However, he also expresses uncertainty and acknowledges that no one knows the \"right\" answer when it comes to evaluating RAG.\\n\\nJerry also seems to be open to considering alternative approaches, such as chunking and metadata, which may indicate that he is willing to adapt or refine his views on RAG based on new information or perspectives. Overall, Jerry\\'s thoughts on RAG appear to be complex and multifaceted, reflecting a thoughtful and nuanced consideration of its strengths and limitations.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='what did I just ask you?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='You asked me what Jerry thinks about RAG (Rapid Automated Grading).', additional_kwargs={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chat_eng.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from C:/Users/armin/.cache/lm-studio/models/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct-imatrix\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct-imatrix\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    16.16 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'Meta-Llama-3-8B-Instruct-imatrix', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '8192', 'tokenizer.ggml.eos_token_id': '128001', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "llm = LlamaCpp(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=None,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=LLAMA3_PATH+\".bin\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    n_gpu_layers=1,\n",
    "    # messages_to_prompt=messages_to_prompt,\n",
    "    # completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top three things to do in Vancouver:\n",
      "\n",
      "‚Ä¢ Visit Stanley Park: A 1,000-acre park that features walking and cycling trails, beaches, and stunning views of the city skyline.\n",
      "‚Ä¢ Explore Granville Island: A popular destination for shopping, dining, and entertainment, with a bustling public market, artisan shops, and street performers.\n",
      "‚Ä¢ Take a Grouse Mountain Skyride: For panoramic views of the city and surrounding mountains, ride the Skyride to the summit of Grouse Mountain, where you can also hike, ski, or visit the wildlife refuge.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# first_prompt = \"What is the most popular city in Canada for tourists? Just return the name of the city\"\n",
    "template = \"What is the most popular city in {country} for tourists? Just return the name of the city\"\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=template)\n",
    "chain_one = LLMChain(llm = llm, prompt = first_prompt)\n",
    "# second step in chain\n",
    "second_prompt = PromptTemplate(\n",
    "input_variables=[\"city\"],\n",
    "template=\"What are the top three things to do in this: {city} for tourists. Just return the answer as three bullet points.\",)\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "retrieval_grader = first_prompt | llm | second_prompt | llm | StrOutputParser()\n",
    "print(retrieval_grader.invoke({\"country\": \"Canada\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mToronto\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mHere are the top three things to do in Toronto for tourists:\n",
      "\n",
      "‚Ä¢ **CN Tower**: Take a ride up the iconic CN Tower, which offers stunning views of Lake Ontario and the city skyline. You can also dine at the revolving restaurant, 360 Restaurant.\n",
      "\n",
      "‚Ä¢ **Kensington Market**: Explore this vibrant and eclectic neighborhood, known for its street art, independent shops, and multicultural restaurants. Be sure to visit the famous St. Lawrence Market, a historic marketplace with over 120 vendors selling fresh produce, meats, and specialty foods.\n",
      "\n",
      "‚Ä¢ **Toronto Islands**: Take a ferry ride to the Toronto Islands, a car-free haven with beautiful parks, beaches, and gardens. Visit the iconic Gibraltar Point Lighthouse, go for a bike ride or hike, and enjoy the stunning views of the city skyline from the islands' many lookout points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'country': 'Canada', 'city': \"Here are the top three things to do in Toronto for tourists:\\n\\n‚Ä¢ **CN Tower**: Take a ride up the iconic CN Tower, which offers stunning views of Lake Ontario and the city skyline. You can also dine at the revolving restaurant, 360 Restaurant.\\n\\n‚Ä¢ **Kensington Market**: Explore this vibrant and eclectic neighborhood, known for its street art, independent shops, and multicultural restaurants. Be sure to visit the famous St. Lawrence Market, a historic marketplace with over 120 vendors selling fresh produce, meats, and specialty foods.\\n\\n‚Ä¢ **Toronto Islands**: Take a ferry ride to the Toronto Islands, a car-free haven with beautiful parks, beaches, and gardens. Visit the iconic Gibraltar Point Lighthouse, go for a bike ride or hike, and enjoy the stunning views of the city skyline from the islands' many lookout points.\"}\n"
     ]
    }
   ],
   "source": [
    "# Combine the first and the second chain\n",
    "overall_chain = SimpleSequentialChain(input_key=\"country\", output_key=\"city\", chains=[chain_one, chain_two], verbose=True)\n",
    "final_answer = overall_chain.invoke(input={\"country\": 'Canada'})\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "vector_store.\n",
    "retriever = vector_store.as_retriever()\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
